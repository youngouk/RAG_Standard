# RAG_Standard ì„±ëŠ¥ ìµœì í™” ë¶„ì„ ë³´ê³ ì„œ

**ë¶„ì„ ì¼ì**: 2026-01-08
**ë¶„ì„ ëŒ€ìƒ**: v3.3.0 (Perfect State)
**ë¶„ì„ì**: Performance Optimization Expert

---

## ğŸ“Š Executive Summary

RAG_StandardëŠ” ì´ë¯¸ ë†’ì€ ìˆ˜ì¤€ì˜ ì•„í‚¤í…ì²˜ ì„¤ê³„ë¥¼ ê°–ì¶”ê³  ìˆìœ¼ë‚˜, ì„±ëŠ¥ ë³‘ëª©ì ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë³¸ ë¶„ì„ì—ì„œëŠ” **5ê°œì˜ í•µì‹¬ ë³‘ëª©ì **ì„ ì‹ë³„í•˜ê³ , ê°ê°ì— ëŒ€í•´ **êµ¬ì²´ì ì¸ ê°œì„  ë°©ì•ˆ**ê³¼ **ì˜ˆìƒ íš¨ê³¼**ë¥¼ ì œì‹œí•©ë‹ˆë‹¤.

### í•µì‹¬ ë°œê²¬ì‚¬í•­

| ë³‘ëª© ì§€ì  | í˜„ì¬ ì¶”ì • ì„±ëŠ¥ | ê°œì„  í›„ ì˜ˆìƒ | ìš°ì„ ìˆœìœ„ |
|----------|---------------|-------------|---------|
| 1. ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ë ˆì´í„´ì‹œ | 800-1200ms | 300-500ms | ğŸ”´ High |
| 2. LLM í˜¸ì¶œ ìµœì í™” | 2000-4000ms | 1000-2000ms | ğŸ”´ High |
| 3. ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ | 200-400ms | 50-150ms | ğŸŸ¡ Medium |
| 4. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ | ì¤‘ê°„ | ë‚®ìŒ | ğŸŸ¢ Low |
| 5. ë™ì‹œì„± ì²˜ë¦¬ | ì œí•œì  | ë†’ìŒ | ğŸŸ¡ Medium |

**ì „ì²´ ì˜ˆìƒ ê°œì„ ìœ¨**: 40-60% ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•

---

## ğŸ¯ ë³‘ëª©ì  #1: ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ ë ˆì´í„´ì‹œ

### ğŸ“ ìœ„ì¹˜
- `app/modules/core/retrieval/orchestrator.py` (L322-L532)
- `app/api/services/rag_pipeline.py` (L508-L599)

### ğŸ” ë¬¸ì œì 

#### 1.1 ìˆœì°¨ì  ì›Œí¬í”Œë¡œìš° (ê°€ì¥ ì‹¬ê°)
```python
# orchestrator.py L372-L478
# ìºì‹œ í™•ì¸ â†’ ì¿¼ë¦¬ í™•ì¥ â†’ ê²€ìƒ‰ â†’ ë¦¬ë­í‚¹ â†’ ìºì‹œ ì €ì¥ (ìˆœì°¨ ì‹¤í–‰)
if self.cache:
    cached_results = await self.cache.get(cache_key)  # ğŸŒ 20-50ms

if self.query_expansion:
    expanded_query_obj = await self.query_expansion.expand(query)  # ğŸŒ 100-300ms (LLM í˜¸ì¶œ!)

if effective_use_graph and self._hybrid_strategy:
    hybrid_result = await self._hybrid_strategy.search(...)  # ğŸŒ 400-600ms
else:
    search_results = await self.retriever.search(...)  # ğŸŒ 300-500ms

if rerank_enabled and self.reranker:
    reranked_results = await self.reranker.rerank(...)  # ğŸŒ 200-400ms (Jina ColBERT v2)
```

**ì¶”ì • ì´ ì‹œê°„**: 800-1200ms (ëª¨ë“  ë‹¨ê³„ í•©ì‚°)

#### 1.2 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì˜¤ë²„í—¤ë“œ
```python
# orchestrator.py L431-L446
# ë²¡í„° ê²€ìƒ‰ + ê·¸ë˜í”„ ê²€ìƒ‰ + RRF ë³‘í•© (ìˆœì°¨ ì‹¤í–‰)
hybrid_result = await self._hybrid_strategy.search(
    query=query,
    top_k=top_k * 2,  # âš ï¸ í•„ìš”í•œ ì–‘ì˜ 2ë°°ë¥¼ ê²€ìƒ‰ (ë¹„íš¨ìœ¨)
)
```

#### 1.3 Multi-Query RRF ë¹„íš¨ìœ¨
```python
# orchestrator.py L776-L866
# 5ê°œ ì¿¼ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ê²€ìƒ‰í•˜ì§€ë§Œ, ê° ì¿¼ë¦¬ë§ˆë‹¤ top_k*2ê°œì”© ê²€ìƒ‰
search_top_k = top_k * 2  # âš ï¸ 8*2 = 16ê°œ Ã— 5ì¿¼ë¦¬ = 80ê°œ ë¬¸ì„œ ê²€ìƒ‰
search_tasks = [self.retriever.search(q, search_top_k, filters) for q in queries]
```

### âœ… ê°œì„  ë°©ì•ˆ

#### ê°œì„ ì•ˆ 1-A: ë³‘ë ¬ ì›Œí¬í”Œë¡œìš° íŒŒì´í”„ë¼ì¸ (High Impact)
```python
# ì œì•ˆ: ë…ë¦½ì ì¸ ì‘ì—…ì„ ë³‘ë ¬í™”
async def search_and_rerank_parallel(self, query: str, top_k: int = 15, ...):
    """ë³‘ë ¬ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸"""

    # Phase 1: ìºì‹œ í™•ì¸ (ë¹ ë¦„)
    if self.cache:
        cache_key = self.cache.generate_cache_key(query, top_k, filters)
        cached = await self.cache.get(cache_key)
        if cached:
            return cached

    # Phase 2: ì¿¼ë¦¬ í™•ì¥ + ê²€ìƒ‰ ë³‘ë ¬ ì‹¤í–‰ (í•µì‹¬ ê°œì„ !)
    tasks = []

    # Task 1: ì¿¼ë¦¬ í™•ì¥ (LLM í˜¸ì¶œ)
    if self.query_expansion:
        tasks.append(self.query_expansion.expand(query))
    else:
        tasks.append(asyncio.create_task(asyncio.sleep(0)))  # No-op

    # Task 2: ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ (ì¦‰ì‹œ ì‹œì‘)
    tasks.append(self.retriever.search(query, top_k * 2, filters))

    # ë³‘ë ¬ ì‹¤í–‰
    expansion_result, base_search_result = await asyncio.gather(*tasks)

    # Phase 3: í™•ì¥ ì¿¼ë¦¬ë¡œ ì¶”ê°€ ê²€ìƒ‰ (ì„ íƒì )
    if expansion_result and expansion_result.all_queries:
        expanded_queries = expansion_result.all_queries[1:]  # ì²« ì¿¼ë¦¬ëŠ” ì´ë¯¸ ê²€ìƒ‰ë¨
        if expanded_queries:
            additional_results = await self._search_and_merge(
                expanded_queries, top_k, filters
            )
            # RRF ë³‘í•©
            search_results = self._rrf_merge([base_search_result, additional_results], ...)
        else:
            search_results = base_search_result
    else:
        search_results = base_search_result

    # Phase 4: ë¦¬ë­í‚¹ (ìµœì¢… ë‹¨ê³„)
    if rerank_enabled and self.reranker:
        final_results = await self.reranker.rerank(query, search_results, top_k)
    else:
        final_results = search_results[:top_k]

    # Phase 5: ìºì‹œ ì €ì¥ (ë°±ê·¸ë¼ìš´ë“œ)
    if self.cache:
        asyncio.create_task(self.cache.set(cache_key, final_results))

    return final_results
```

**ì˜ˆìƒ íš¨ê³¼**:
- ì¿¼ë¦¬ í™•ì¥(100-300ms) + ê²€ìƒ‰(300-500ms) ë³‘ë ¬í™” â†’ **400-500ms ì ˆì•½**
- ì´ ë ˆì´í„´ì‹œ: 800-1200ms â†’ **400-700ms** (33-50% ê°œì„ )

**êµ¬í˜„ ë³µì¡ë„**: â­â­â­ Medium (ê¸°ì¡´ ì½”ë“œ ì¬êµ¬ì„±)

---

#### ê°œì„ ì•ˆ 1-B: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìµœì í™” (Medium Impact)
```python
# ì œì•ˆ: ë²¡í„° + ê·¸ë˜í”„ ê²€ìƒ‰ ë³‘ë ¬í™”
class VectorGraphHybridSearch:
    async def search(self, query: str, top_k: int):
        """ë²¡í„° + ê·¸ë˜í”„ ë³‘ë ¬ ê²€ìƒ‰"""
        # ê¸°ì¡´: ìˆœì°¨ ì‹¤í–‰
        # vector_results = await self.retriever.search(...)  # 300ms
        # graph_results = await self.graph_store.search(...)  # 200ms
        # merged = self._rrf_merge(vector_results, graph_results)  # 50ms
        # ì´ 550ms

        # ê°œì„ : ë³‘ë ¬ ì‹¤í–‰
        vector_task = self.retriever.search(query, top_k, filters)
        graph_task = self.graph_store.search(query, top_k)

        vector_results, graph_results = await asyncio.gather(
            vector_task, graph_task
        )

        # RRF ë³‘í•© (50ms)
        merged = self._rrf_merge(vector_results, graph_results, top_k)
        return merged
        # ì´ 350ms (200ms ì ˆì•½)
```

**ì˜ˆìƒ íš¨ê³¼**: 550ms â†’ 350ms (36% ê°œì„ )
**êµ¬í˜„ ë³µì¡ë„**: â­â­ Low

---

#### ê°œì„ ì•ˆ 1-C: Multi-Query ì ì‘í˜• í¬ê¸° ì¡°ì • (Low Impact)
```python
# ì œì•ˆ: ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¼ top_k ë°°ìˆ˜ ì¡°ì •
async def _search_and_merge(
    self,
    queries: list[str],
    top_k: int,
    filters: dict[str, Any] | None = None,
    adaptive: bool = True,  # ğŸ†• ì ì‘í˜• ëª¨ë“œ
):
    # ê¸°ì¡´: ë¬´ì¡°ê±´ top_k * 2
    # search_top_k = top_k * 2  # 8*2 = 16ê°œ

    # ê°œì„ : ì¿¼ë¦¬ ë³µì¡ë„ì— ë”°ë¼ ì¡°ì •
    if adaptive:
        if len(queries) <= 2:
            search_top_k = top_k * 2  # ë³µì¡í•œ ê²½ìš°: ì—¬ìœ ë¶„ ë§ì´
        elif len(queries) <= 4:
            search_top_k = int(top_k * 1.5)  # ì¤‘ê°„: ì•½ê°„ ì—¬ìœ 
        else:
            search_top_k = top_k  # ê°„ë‹¨í•œ ê²½ìš°: ì •í™•í•œ ìˆ˜ë§Œ
    else:
        search_top_k = top_k * 2

    search_tasks = [
        self.retriever.search(q, search_top_k, filters) for q in queries
    ]
    # ...
```

**ì˜ˆìƒ íš¨ê³¼**: ê²€ìƒ‰ ë¬¸ì„œ ìˆ˜ 40-60% ê°ì†Œ â†’ DB ì¿¼ë¦¬ ì‹œê°„ 15-25% ì ˆì•½
**êµ¬í˜„ ë³µì¡ë„**: â­ Very Low

---

#### ê°œì„ ì•ˆ 1-D: ìºì‹œ ì›Œë°ì—… (Warm-up Cache)
```python
# ì œì•ˆ: ìì£¼ ë¬»ëŠ” ì§ˆë¬¸(FAQ) ì‚¬ì „ ìºì‹±
class CacheWarmer:
    """ìºì‹œ ì›Œë°ì—… ì„œë¹„ìŠ¤"""

    def __init__(self, orchestrator: RetrievalOrchestrator):
        self.orchestrator = orchestrator
        self.faq_queries = []  # FAQ ë¦¬ìŠ¤íŠ¸ (configì—ì„œ ë¡œë“œ)

    async def warmup(self):
        """ì„œë²„ ì‹œì‘ ì‹œ FAQ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ì „ ìºì‹±"""
        logger.info(f"ğŸ”¥ ìºì‹œ ì›Œë°ì—… ì‹œì‘: {len(self.faq_queries)}ê°œ ì¿¼ë¦¬")

        tasks = [
            self.orchestrator.search_and_rerank(query, top_k=8)
            for query in self.faq_queries
        ]

        await asyncio.gather(*tasks, return_exceptions=True)

        logger.info("âœ… ìºì‹œ ì›Œë°ì—… ì™„ë£Œ")
```

**ì˜ˆìƒ íš¨ê³¼**: FAQ ì§ˆë¬¸ì— ëŒ€í•´ **ì¦‰ì‹œ ì‘ë‹µ** (800ms â†’ 20-50ms, 95% ê°œì„ )
**êµ¬í˜„ ë³µì¡ë„**: â­ Very Low

---

### ğŸ“ˆ ê°œì„ ì•ˆ 1 ì¢…í•© íš¨ê³¼

| ê°œì„ ì•ˆ | ì˜ˆìƒ ì‹œê°„ ì ˆì•½ | ìš°ì„ ìˆœìœ„ | ë³µì¡ë„ |
|-------|---------------|---------|-------|
| 1-A. ë³‘ë ¬ ì›Œí¬í”Œë¡œìš° | 400-500ms | ğŸ”´ High | â­â­â­ |
| 1-B. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë³‘ë ¬í™” | 200ms | ğŸŸ¡ Medium | â­â­ |
| 1-C. ì ì‘í˜• top_k | 50-100ms | ğŸŸ¢ Low | â­ |
| 1-D. ìºì‹œ ì›Œë°ì—… | FAQ ì§ˆë¬¸ 750ms | ğŸŸ¡ Medium | â­ |

**ì „ì²´ ì¡°í•© ì‹œ ì˜ˆìƒ íš¨ê³¼**: 800-1200ms â†’ **300-500ms** (58-75% ê°œì„ )

---

## ğŸ¯ ë³‘ëª©ì  #2: LLM í˜¸ì¶œ ìµœì í™”

### ğŸ“ ìœ„ì¹˜
- `app/modules/core/generation/generator.py` (L198-L408)
- `app/api/services/rag_pipeline.py` (L594-L615)

### ğŸ” ë¬¸ì œì 

#### 2.1 ë¶ˆí•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ì „ì†¡
```python
# generator.py L439-L462
def _build_context(self, context_documents: list[Any]) -> str:
    """ì»¨í…ìŠ¤íŠ¸ í…ìŠ¤íŠ¸ êµ¬ì„±"""
    context_parts = []
    for i, doc in enumerate(context_documents[:5]):  # Top-5ë§Œ ì‚¬ìš©
        # âš ï¸ ë¦¬ë­í‚¹ í›„ 8ê°œ ë¬¸ì„œ ì¤‘ 5ê°œë§Œ ì‚¬ìš© (3ê°œëŠ” ë‚­ë¹„)
        # âš ï¸ ì „ì²´ chunk í…ìŠ¤íŠ¸ë¥¼ LLMì— ì „ì†¡ (ìš”ì•½ ì—†ìŒ)
        content = doc.content if hasattr(doc, "content") else doc.page_content
        context_parts.append(f"[ë¬¸ì„œ {i+1}]\n{content}\n")

    return "\n".join(context_parts)
```

**ë¬¸ì œì **:
1. ë¦¬ë­í‚¹ í›„ 8ê°œ ë¬¸ì„œë¥¼ ë°›ì§€ë§Œ, **5ê°œë§Œ ì‚¬ìš©** (37.5% ë‚­ë¹„)
2. chunkê°€ í‰ê·  1400ìì¸ë°, **ì „ì²´ë¥¼ LLMì— ì „ì†¡** (ìš”ì•½ ì—†ìŒ)
3. ì»¨í…ìŠ¤íŠ¸ ì´ í¬ê¸°: **5 Ã— 1400ì = 7000ì** (í† í° ì•½ 3500ê°œ)

**ì¶”ì • ë¹„ìš©**:
- Input Tokens: 3500 tokens (ì»¨í…ìŠ¤íŠ¸) + 500 tokens (í”„ë¡¬í”„íŠ¸) = **4000 tokens**
- ëª¨ë¸: `google/gemini-2.5-flash` ($0.075/1M input tokens)
- ë‹¨ì¼ ìš”ì²­ ë¹„ìš©: $0.0003
- ë ˆì´í„´ì‹œ: **2000-4000ms** (í† í° ìˆ˜ì— ë¹„ë¡€)

#### 2.2 í”„ë¡¬í”„íŠ¸ ë¹„íš¨ìœ¨
```python
# generator.py L464-L533
async def _build_prompt(self, query: str, context_text: str, options: dict):
    """í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
    # âš ï¸ ë¶ˆí•„ìš”í•˜ê²Œ ê¸´ í”„ë¡¬í”„íŠ¸
    user_parts = [
        "<conversation_history>",
        escape_xml(session_context),  # í‰ê·  500ì
        "</conversation_history>\n",
        "<reference_documents>",
        escape_xml(context_text),  # 7000ì
        "</reference_documents>\n",
        "<user_question>",
        escape_xml(query),  # 50-200ì
        "</user_question>\n",
        # âš ï¸ ë¶ˆí•„ìš”í•œ XML íƒœê·¸ ì˜¤ë²„í—¤ë“œ (í† í° ë‚­ë¹„)
    ]
```

**ë¬¸ì œì **:
1. XML íƒœê·¸ ì‚¬ìš© â†’ **í† í° 5-10% ë‚­ë¹„**
2. ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ê°€ í•­ìƒ í¬í•¨ë¨ (í•„ìš” ì—†ëŠ” ê²½ìš°ë„ ìˆìŒ)
3. í”„ë¡¬í”„íŠ¸ ìµœì í™” ì—†ìŒ (ì••ì¶•, ìš”ì•½ ë“±)

#### 2.3 Fallback ëª¨ë¸ ì²´ì¸ ìˆœì°¨ ì‹¤í–‰
```python
# generator.py L259-L286
for model in models_to_try:
    try:
        result = await self._generate_with_model(model, ...)
        return result
    except Exception as e:
        logger.warning(f"âŒ ëª¨ë¸ {model} ì‹¤íŒ¨: {e}")
        last_error = e
        continue  # ğŸŒ ë‹¤ìŒ ëª¨ë¸ ì‹œë„ (ìˆœì°¨)
```

**ë¬¸ì œì **: ì²« ë²ˆì§¸ ëª¨ë¸ ì‹¤íŒ¨ ì‹œ **ì¶”ê°€ 2-4ì´ˆ ëŒ€ê¸°** (fallback ëª¨ë¸ í˜¸ì¶œ)

### âœ… ê°œì„  ë°©ì•ˆ

#### ê°œì„ ì•ˆ 2-A: ì»¨í…ìŠ¤íŠ¸ ì••ì¶• (High Impact)
```python
# ì œì•ˆ: ì¤‘ìš” ë¬¸ì¥ë§Œ ì¶”ì¶œí•˜ì—¬ ì»¨í…ìŠ¤íŠ¸ í¬ê¸° 50% ì ˆê°
class ContextCompressor:
    """ì»¨í…ìŠ¤íŠ¸ ì••ì¶• ì„œë¹„ìŠ¤"""

    def __init__(self, target_ratio: float = 0.5):
        self.target_ratio = target_ratio  # ëª©í‘œ ì••ì¶•ë¥ 

    def compress(self, documents: list[Any], query: str) -> str:
        """ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ ê¸°ë°˜ ì••ì¶•"""
        compressed_parts = []

        for i, doc in enumerate(documents[:5]):
            content = doc.content if hasattr(doc, "content") else doc.page_content

            # ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ (TF-IDF ë˜ëŠ” LLM ê¸°ë°˜)
            important_sentences = self._extract_important_sentences(
                content, query, target_ratio=self.target_ratio
            )

            compressed_parts.append(f"[ë¬¸ì„œ {i+1}]\n{important_sentences}\n")

        return "\n".join(compressed_parts)

    def _extract_important_sentences(
        self, text: str, query: str, target_ratio: float
    ) -> str:
        """TF-IDF ê¸°ë°˜ ì¤‘ìš” ë¬¸ì¥ ì¶”ì¶œ"""
        from sklearn.feature_extraction.text import TfidfVectorizer
        import numpy as np

        # ë¬¸ì¥ ë¶„ë¦¬
        sentences = text.split(". ")
        if len(sentences) <= 3:
            return text  # ì§§ì€ ë¬¸ì„œëŠ” ì••ì¶• ì•ˆ í•¨

        # TF-IDF ê³„ì‚°
        vectorizer = TfidfVectorizer()
        tfidf_matrix = vectorizer.fit_transform([query] + sentences)

        # ì¿¼ë¦¬ì™€ ê° ë¬¸ì¥ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        query_vec = tfidf_matrix[0:1]
        sentence_vecs = tfidf_matrix[1:]
        similarities = (sentence_vecs * query_vec.T).toarray().flatten()

        # ìƒìœ„ N% ë¬¸ì¥ ì„ íƒ
        n_keep = max(3, int(len(sentences) * target_ratio))
        top_indices = np.argsort(similarities)[-n_keep:]
        top_indices = sorted(top_indices)  # ì›ë˜ ìˆœì„œ ìœ ì§€

        important_sentences = [sentences[i] for i in top_indices]
        return ". ".join(important_sentences) + "."
```

**ì˜ˆìƒ íš¨ê³¼**:
- ì»¨í…ìŠ¤íŠ¸ í¬ê¸°: 7000ì â†’ **3500ì** (50% ì ˆê°)
- Input Tokens: 4000 â†’ **2000** (50% ì ˆê°)
- ë ˆì´í„´ì‹œ: 2000-4000ms â†’ **1000-2000ms** (50% ê°œì„ )
- ë¹„ìš©: $0.0003 â†’ **$0.00015** (50% ì ˆê°)

**êµ¬í˜„ ë³µì¡ë„**: â­â­â­ Medium (TF-IDF êµ¬í˜„ í•„ìš”)

---

#### ê°œì„ ì•ˆ 2-B: í”„ë¡¬í”„íŠ¸ ìµœì í™” (Medium Impact)
```python
# ì œì•ˆ: ë¶ˆí•„ìš”í•œ XML íƒœê·¸ ì œê±°, ê°„ê²°í•œ í”„ë¡¬í”„íŠ¸
async def _build_prompt_optimized(
    self, query: str, context_text: str, options: dict
) -> tuple[str, str]:
    """ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸"""
    # System í”„ë¡¬í”„íŠ¸ (ê°„ê²°)
    system_content = """í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸. ì œê³µëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ ì œê³µ."""

    # User í”„ë¡¬í”„íŠ¸ (êµ¬ì¡°í™”, íƒœê·¸ ìµœì†Œí™”)
    user_parts = []

    # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ (í•„ìš”í•œ ê²½ìš°ë§Œ í¬í•¨)
    if options.get("session_context"):
        user_parts.append(f"ì´ì „ ëŒ€í™”:\n{options['session_context']}\n")

    # ì°¸ê³  ë¬¸ì„œ (ì••ì¶•ëœ ì»¨í…ìŠ¤íŠ¸)
    user_parts.append(f"ì°¸ê³  ë¬¸ì„œ:\n{context_text}\n")

    # ì§ˆë¬¸
    user_parts.append(f"ì§ˆë¬¸: {query}\n")

    # ë‹µë³€ ì§€ì‹œ
    user_parts.append("ìœ„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.")

    user_content = "\n".join(user_parts)

    return system_content, user_content
```

**ì˜ˆìƒ íš¨ê³¼**:
- í”„ë¡¬í”„íŠ¸ í† í°: 500 â†’ **300** (40% ì ˆê°)
- ì´ í† í°: 4000 â†’ **2300** (ê°œì„ ì•ˆ 2-Aì™€ ì¡°í•© ì‹œ)

**êµ¬í˜„ ë³µì¡ë„**: â­ Very Low

---

#### ê°œì„ ì•ˆ 2-C: Request Batching (Advanced)
```python
# ì œì•ˆ: ë‹¤ì¤‘ ìš”ì²­ì„ ë°°ì¹˜ë¡œ ë¬¶ì–´ ì²˜ë¦¬
class BatchedGenerationModule:
    """ë°°ì¹˜ ìƒì„± ëª¨ë“ˆ"""

    def __init__(self, base_module: GenerationModule, batch_size: int = 5):
        self.base_module = base_module
        self.batch_size = batch_size
        self.pending_requests: list[dict] = []
        self.batch_timer = None

    async def generate_answer_batched(
        self, query: str, context_documents: list[Any], options: dict
    ) -> GenerationResult:
        """ë°°ì¹˜ ìƒì„± (ì—¬ëŸ¬ ìš”ì²­ ë¬¶ì–´ì„œ ì²˜ë¦¬)"""
        # ìš”ì²­ì„ íì— ì¶”ê°€
        request_future = asyncio.Future()
        self.pending_requests.append({
            "query": query,
            "context_documents": context_documents,
            "options": options,
            "future": request_future,
        })

        # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ë˜ëŠ” íƒ€ì„ì•„ì›ƒ ì‹œ ì¼ê´„ ì²˜ë¦¬
        if len(self.pending_requests) >= self.batch_size:
            await self._process_batch()
        else:
            # íƒ€ì´ë¨¸ ì‹œì‘ (100ms ëŒ€ê¸°)
            if not self.batch_timer:
                self.batch_timer = asyncio.create_task(self._wait_and_process())

        return await request_future

    async def _wait_and_process(self):
        """íƒ€ì´ë¨¸ ëŒ€ê¸° í›„ ë°°ì¹˜ ì²˜ë¦¬"""
        await asyncio.sleep(0.1)  # 100ms ëŒ€ê¸°
        await self._process_batch()

    async def _process_batch(self):
        """ë°°ì¹˜ ì¼ê´„ ì²˜ë¦¬"""
        if not self.pending_requests:
            return

        batch = self.pending_requests[:self.batch_size]
        self.pending_requests = self.pending_requests[self.batch_size:]
        self.batch_timer = None

        # OpenRouter Batch API í˜¸ì¶œ (ì•„ì§ ì§€ì› ì•ˆ ë¨, í–¥í›„ ëŒ€ë¹„)
        # í˜„ì¬ëŠ” ë³‘ë ¬ ì²˜ë¦¬ë¡œ ëŒ€ì²´
        tasks = [
            self.base_module.generate_answer(
                req["query"], req["context_documents"], req["options"]
            )
            for req in batch
        ]

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ë°°í¬
        for req, result in zip(batch, results):
            if isinstance(result, Exception):
                req["future"].set_exception(result)
            else:
                req["future"].set_result(result)
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë™ì‹œ ìš”ì²­ 5ê°œ ì‹œ: 5 Ã— 2000ms = 10000ms â†’ **2500ms** (75% ê°œì„ )
- ìŠ¤ë£¨í’‹(Throughput): 0.5 req/s â†’ **2 req/s** (4ë°° ê°œì„ )

**êµ¬í˜„ ë³µì¡ë„**: â­â­â­â­ High (ë³µì¡í•œ ë°°ì¹˜ ë¡œì§)

---

#### ê°œì„ ì•ˆ 2-D: Streaming Response (UX ê°œì„ )
```python
# ì œì•ˆ: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µìœ¼ë¡œ ì²´ê° ë ˆì´í„´ì‹œ ê°ì†Œ
async def generate_answer_stream(
    self, query: str, context_documents: list[Any], options: dict
) -> AsyncIterator[str]:
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (ì²­í¬ ë‹¨ìœ„)"""
    # OpenAI SDK stream=True ì˜µì…˜ ì‚¬ìš©
    response = await asyncio.to_thread(
        self.client.chat.completions.create,
        model=model,
        messages=messages,
        stream=True,  # ğŸ†• ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
        **api_params,
    )

    async for chunk in response:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content
```

**ì˜ˆìƒ íš¨ê³¼**:
- ì²« í† í° ì‘ë‹µ(TTFT): **200-500ms** (ì „ì²´ ì‘ë‹µ 2000ms ëŒ€ë¹„ 75-90% ê°œì„ )
- ì‚¬ìš©ì ì²´ê° ë ˆì´í„´ì‹œ: **ëŒ€í­ ê°ì†Œ** (UX í¬ê²Œ ê°œì„ )

**êµ¬í˜„ ë³µì¡ë„**: â­â­ Low (OpenAI SDK ê¸°ëŠ¥)

---

### ğŸ“ˆ ê°œì„ ì•ˆ 2 ì¢…í•© íš¨ê³¼

| ê°œì„ ì•ˆ | ì˜ˆìƒ ì‹œê°„/ë¹„ìš© ì ˆì•½ | ìš°ì„ ìˆœìœ„ | ë³µì¡ë„ |
|-------|-------------------|---------|-------|
| 2-A. ì»¨í…ìŠ¤íŠ¸ ì••ì¶• | 1000-2000ms, 50% ë¹„ìš© | ğŸ”´ High | â­â­â­ |
| 2-B. í”„ë¡¬í”„íŠ¸ ìµœì í™” | 200-400ms, 20% ë¹„ìš© | ğŸŸ¡ Medium | â­ |
| 2-C. Request Batching | 75% ìŠ¤ë£¨í’‹ ê°œì„  | ğŸŸ¢ Low | â­â­â­â­ |
| 2-D. Streaming Response | TTFT 75-90% ê°œì„  | ğŸŸ¡ Medium | â­â­ |

**ì „ì²´ ì¡°í•© ì‹œ ì˜ˆìƒ íš¨ê³¼**: 2000-4000ms â†’ **1000-2000ms** (50% ê°œì„ )

---

## ğŸ¯ ë³‘ëª©ì  #3: ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ìµœì í™”

### ğŸ“ ìœ„ì¹˜
- `app/modules/core/retrieval/retrievers/weaviate_retriever.py`
- `app/modules/core/retrieval/retrievers/mongodb_retriever.py`

### ğŸ” ë¬¸ì œì 

#### 3.1 Weaviate í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë¹„íš¨ìœ¨
```python
# weaviate_retriever.py (ì¶”ì • ì½”ë“œ)
async def search(self, query: str, top_k: int, filters: dict | None = None):
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Dense + Sparse BM25)"""
    # âš ï¸ Denseì™€ Sparse ê²€ìƒ‰ì´ ìˆœì°¨ ì‹¤í–‰ë  ê°€ëŠ¥ì„±
    # âš ï¸ top_k = 8ì¸ë°, ì‹¤ì œë¡œëŠ” 16ê°œ ê²€ìƒ‰ í›„ ìƒìœ„ 8ê°œ ì„ íƒ (ì˜¤ë²„í—¤ë“œ)

    response = self.client.query.get(
        class_name="Documents",
        properties=["content", "metadata", ...],
    ).with_hybrid(
        query=query,
        alpha=0.6,  # ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜
    ).with_limit(top_k).do()

    # ì¶”ì • ì‹œê°„: 300-500ms
```

**ë¬¸ì œì **:
1. Weaviateì˜ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë‚´ë¶€ êµ¬í˜„ íŒŒì•… í•„ìš” (ë³‘ë ¬ vs ìˆœì°¨)
2. `with_limit(top_k)`ì´ íš¨ìœ¨ì ìœ¼ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸ í•„ìš”
3. í•„í„° ì ìš© ì‹œ ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ì„±

#### 3.2 MongoDB ì§‘ê³„ íŒŒì´í”„ë¼ì¸ ë¹„íš¨ìœ¨
```python
# mongodb_retriever.py (ì¶”ì • ì½”ë“œ)
async def search(self, query: str, top_k: int, filters: dict | None = None):
    """MongoDB Atlas ë²¡í„° ê²€ìƒ‰"""
    pipeline = [
        {
            "$vectorSearch": {
                "index": "vector_index",
                "queryVector": query_embedding,
                "path": "embedding",
                "numCandidates": top_k * 10,  # âš ï¸ ì˜¤ë²„ìƒ˜í”Œë§ (ë¹„íš¨ìœ¨)
                "limit": top_k,
            }
        },
        # âš ï¸ ì¶”ê°€ ì§‘ê³„ ë‹¨ê³„ (ë©”íƒ€ë°ì´í„° ë³€í™˜ ë“±)
        {"$project": {...}},
        {"$addFields": {...}},
    ]

    results = await self.collection.aggregate(pipeline).to_list(length=top_k)
    # ì¶”ì • ì‹œê°„: 200-400ms
```

**ë¬¸ì œì **:
1. `numCandidates = top_k * 10` â†’ **80ê°œ í›„ë³´ ê²€ìƒ‰** (ì˜¤ë²„í—¤ë“œ)
2. ì§‘ê³„ íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ê°€ ë§ì„ìˆ˜ë¡ ëŠë¦¼
3. ì¸ë±ìŠ¤ ìµœì í™” í™•ì¸ í•„ìš”

#### 3.3 ìºì‹œ ë¯¸ìŠ¤ ì‹œ ì¤‘ë³µ ì¿¼ë¦¬
```python
# orchestrator.py L372-L394
if self.cache:
    cache_key = self.cache.generate_cache_key(query, top_k, filters)
    cached_results = await self.cache.get(cache_key)

    if cached_results:
        return cached_results  # âœ… ìºì‹œ íˆíŠ¸ (20-50ms)

# âŒ ìºì‹œ ë¯¸ìŠ¤ â†’ ì „ì²´ ê²€ìƒ‰ (300-500ms)
search_results = await self.retriever.search(query, top_k, filters)
```

**ë¬¸ì œì **: ìºì‹œ íˆíŠ¸ìœ¨ì´ ë‚®ìœ¼ë©´ **ë§¤ë²ˆ ì „ì²´ DB ì¿¼ë¦¬** ì‹¤í–‰

### âœ… ê°œì„  ë°©ì•ˆ

#### ê°œì„ ì•ˆ 3-A: ì¸ë±ìŠ¤ ìµœì í™” (High Impact)
```yaml
# ì œì•ˆ: Weaviate ì¸ë±ìŠ¤ ì„¤ì • ìµœì í™”
# weaviate.yaml
vector_index_config:
  ef_construction: 128  # ê¸°ë³¸ê°’: 64 (ê²€ìƒ‰ ì •í™•ë„ vs ì†ë„ trade-off)
  max_connections: 64   # ê¸°ë³¸ê°’: 32 (ë©”ëª¨ë¦¬ vs ì†ë„ trade-off)

# MongoDB Atlas ë²¡í„° ì¸ë±ìŠ¤ ìµœì í™”
# mongodb.yaml
vector_search:
  numCandidates: "top_k * 5"  # 10 â†’ 5ë¡œ ê°ì†Œ (40ê°œ í›„ë³´)
  similarity: "cosine"
```

**ì˜ˆìƒ íš¨ê³¼**:
- Weaviate ê²€ìƒ‰: 300-500ms â†’ **200-350ms** (30% ê°œì„ )
- MongoDB ê²€ìƒ‰: 200-400ms â†’ **150-300ms** (25% ê°œì„ )

**êµ¬í˜„ ë³µì¡ë„**: â­â­ Low (ì„¤ì • ë³€ê²½)

---

#### ê°œì„ ì•ˆ 3-B: ì—°ê²° í’€ ìµœì í™” (Medium Impact)
```python
# ì œì•ˆ: DB ì—°ê²° í’€ í¬ê¸° ì¦ê°€
# weaviate_retriever.py
class WeaviateRetriever:
    def __init__(self, config: dict):
        self.client = weaviate.Client(
            url=config["url"],
            timeout_config=(5, 30),  # (connect, read) timeout
            startup_period=10,
            additional_config=weaviate.AdditionalConfig(
                connection_config=weaviate.ConnectionConfig(
                    session_pool_connections=20,  # ğŸ†• ê¸°ë³¸ê°’: 10
                    session_pool_maxsize=50,      # ğŸ†• ê¸°ë³¸ê°’: 20
                )
            )
        )
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ì‹œ **ì—°ê²° ëŒ€ê¸° ì‹œê°„ ê°ì†Œ** (50-100ms ì ˆì•½)
- ìŠ¤ë£¨í’‹: 1.5ë°° ê°œì„ 

**êµ¬í˜„ ë³µì¡ë„**: â­ Very Low

---

#### ê°œì„ ì•ˆ 3-C: ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ (Semantic Cache)
```python
# ì œì•ˆ: Semantic Cache ìœ ì‚¬ë„ ì„ê³„ê°’ ì¡°ì •
# cache.yaml
semantic:
  similarity_threshold: 0.92  # í˜„ì¬ê°’
  # â†’ 0.88ë¡œ ì™„í™” (íˆíŠ¸ìœ¨ í–¥ìƒ, ì •í™•ë„ ì•½ê°„ ê°ì†Œ)
  similarity_threshold: 0.88
```

**ì˜ˆìƒ íš¨ê³¼**:
- ìºì‹œ íˆíŠ¸ìœ¨: 30% â†’ **50%** (ìœ ì‚¬ ì¿¼ë¦¬ ì¦ê°€)
- í‰ê·  ì‘ë‹µ ì‹œê°„: 500ms â†’ **350ms** (íˆíŠ¸ ì‹œ 20-50ms)

**êµ¬í˜„ ë³µì¡ë„**: â­ Very Low (ì„¤ì • ë³€ê²½)

---

#### ê°œì„ ì•ˆ 3-D: Lazy Loading (Advanced)
```python
# ì œì•ˆ: ë©”íƒ€ë°ì´í„° ì§€ì—° ë¡œë”©
async def search(self, query: str, top_k: int, filters: dict | None = None):
    """ê²€ìƒ‰ (ë©”íƒ€ë°ì´í„° ìµœì†Œí™”)"""
    # Phase 1: IDì™€ scoreë§Œ ê²€ìƒ‰ (ë¹ ë¦„)
    response = self.client.query.get(
        class_name="Documents",
        properties=["_id", "score"],  # ğŸ†• ìµœì†Œ í•„ë“œ
    ).with_hybrid(query=query, alpha=0.6).with_limit(top_k).do()

    # Phase 2: ë¦¬ë­í‚¹ í›„ ìƒìœ„ 5ê°œë§Œ ë©”íƒ€ë°ì´í„° ë¡œë”©
    top_ids = [doc["_id"] for doc in response["data"]["Get"]["Documents"][:5]]

    detailed_docs = await self._load_full_documents(top_ids)
    return detailed_docs
```

**ì˜ˆìƒ íš¨ê³¼**:
- ê²€ìƒ‰ ì‹œê°„: 300ms â†’ **200ms** (33% ê°œì„ )
- ë„¤íŠ¸ì›Œí¬ ì „ì†¡ëŸ‰: 50% ê°ì†Œ

**êµ¬í˜„ ë³µì¡ë„**: â­â­â­ Medium

---

### ğŸ“ˆ ê°œì„ ì•ˆ 3 ì¢…í•© íš¨ê³¼

| ê°œì„ ì•ˆ | ì˜ˆìƒ ì‹œê°„ ì ˆì•½ | ìš°ì„ ìˆœìœ„ | ë³µì¡ë„ |
|-------|---------------|---------|-------|
| 3-A. ì¸ë±ìŠ¤ ìµœì í™” | 50-150ms | ğŸŸ¡ Medium | â­â­ |
| 3-B. ì—°ê²° í’€ ì¦ê°€ | 50-100ms | ğŸŸ¢ Low | â­ |
| 3-C. ìºì‹œ íˆíŠ¸ìœ¨ í–¥ìƒ | 150ms (í‰ê· ) | ğŸŸ¡ Medium | â­ |
| 3-D. Lazy Loading | 100ms | ğŸŸ¢ Low | â­â­â­ |

**ì „ì²´ ì¡°í•© ì‹œ ì˜ˆìƒ íš¨ê³¼**: 200-400ms â†’ **50-150ms** (62-75% ê°œì„ )

---

## ğŸ¯ ë³‘ëª©ì  #4: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„

### ğŸ“ ìœ„ì¹˜
- `app/modules/core/retrieval/orchestrator.py`
- `app/api/services/rag_pipeline.py`
- `app/modules/core/retrieval/cache/memory_cache.py`

### ğŸ” ë¬¸ì œì 

#### 4.1 ì¤‘ë³µ ë¬¸ì„œ ì €ì¥
```python
# orchestrator.py L776-L866
async def _search_and_merge(self, queries: list[str], top_k: int, ...):
    """Multi-Query RRF"""
    # 5ê°œ ì¿¼ë¦¬ Ã— 16ê°œ ê²°ê³¼ = 80ê°œ ë¬¸ì„œ ë©”ëª¨ë¦¬ ì ì¬
    search_top_k = top_k * 2  # 16ê°œ
    search_tasks = [self.retriever.search(q, search_top_k, filters) for q in queries]

    results_per_query = await asyncio.gather(*search_tasks)
    # ë©”ëª¨ë¦¬: 80ê°œ Ã— 1400ì Ã— 5ì¿¼ë¦¬ = ì•½ 560KB

    # RRF ë³‘í•© í›„ì—ë„ ì›ë³¸ 80ê°œ ê°ì²´ ìœ ì§€ë¨ (GC ì „ê¹Œì§€)
```

**ë¬¸ì œì **:
1. ìµœì¢… 8ê°œë§Œ í•„ìš”í•˜ì§€ë§Œ, **80ê°œë¥¼ ë©”ëª¨ë¦¬ì— ì ì¬** (10ë°° ì˜¤ë²„í—¤ë“œ)
2. RRF ë³‘í•© ì‹œ ì¤‘ë³µ ë¬¸ì„œ ì œê±° ì•ˆ ë¨ (ë¬¸ì„œ ID ì¤‘ë³µ ê°€ëŠ¥)
3. Python ê°ì²´ ì˜¤ë²„í—¤ë“œ (ê° Document ê°ì²´ë§ˆë‹¤ ì•½ 1KB)

**ì¶”ì • ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰**: **560KB ~ 1MB** (ë‹¨ì¼ ìš”ì²­ ê¸°ì¤€)

#### 4.2 ìºì‹œ ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ìœ„í—˜
```python
# cache/memory_cache.py
class MemoryCacheManager:
    def __init__(self, maxsize: int = 1000, ttl: int = 3600):
        self.cache: dict[str, CacheEntry] = {}  # âš ï¸ ë¬´ì œí•œ ì¦ê°€ ê°€ëŠ¥
        self.maxsize = maxsize
        self.ttl = ttl
```

**ë¬¸ì œì **:
1. `maxsize=1000` â†’ **ìµœëŒ€ 1000ê°œ ì¿¼ë¦¬** Ã— 8ê°œ ë¬¸ì„œ Ã— 1400ì = **11.2MB**
2. TTL ë§Œë£Œ ì²´í¬ê°€ ë¹„ë™ê¸°ë¡œ ì´ë£¨ì–´ì§€ì§€ ì•Šìœ¼ë©´ **ë©”ëª¨ë¦¬ ëˆ„ìˆ˜** ê°€ëŠ¥
3. LRU ì •ì±…ì´ ì œëŒ€ë¡œ êµ¬í˜„ë˜ì§€ ì•Šìœ¼ë©´ ì˜¤ë˜ëœ í•­ëª© ì œê±° ì•ˆ ë¨

### âœ… ê°œì„  ë°©ì•ˆ

#### ê°œì„ ì•ˆ 4-A: ë¬¸ì„œ ì¤‘ë³µ ì œê±° (Medium Impact)
```python
# ì œì•ˆ: RRF ë³‘í•© ì‹œ ì¤‘ë³µ ë¬¸ì„œ ì¦‰ì‹œ ì œê±°
def _rrf_merge(
    self,
    results_per_query: list[list[SearchResult]],
    queries: list[str],
    weights: list[float],
    top_k: int,
):
    """RRF ë³‘í•© (ì¤‘ë³µ ì œê±° ìµœì í™”)"""
    doc_scores: dict[str, float] = {}
    doc_objects: dict[str, SearchResult] = {}

    for query_idx, results in enumerate(results_per_query):
        weight = weights[query_idx]

        for rank, result in enumerate(results):
            doc_id = self._get_doc_id(result)

            if not doc_id:
                continue

            # âœ… ì¤‘ë³µ ì²´í¬: ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì„œëŠ” ìŠ¤í‚µ
            if doc_id in doc_objects:
                # ì ìˆ˜ë§Œ ëˆ„ì , ê°ì²´ëŠ” ì¬ì‚¬ìš©
                rrf_score = weight / (rrf_k + rank)
                doc_scores[doc_id] += rrf_score
                continue  # ğŸ†• ë©”ëª¨ë¦¬ ì ˆì•½

            # ìƒˆë¡œìš´ ë¬¸ì„œë§Œ ì €ì¥
            rrf_score = weight / (rrf_k + rank)
            doc_scores[doc_id] = rrf_score
            doc_objects[doc_id] = result

    # ìƒìœ„ top_kê°œë§Œ ë°˜í™˜ (ë‚˜ë¨¸ì§€ëŠ” GC)
    sorted_doc_ids = sorted(
        doc_scores.keys(), key=lambda doc_id: doc_scores[doc_id], reverse=True
    )

    return [doc_objects[doc_id] for doc_id in sorted_doc_ids[:top_k]]
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 560KB â†’ **280KB** (50% ì ˆê°)
- GC ì••ë ¥ ê°ì†Œ â†’ **CPU ì‚¬ìš©ëŸ‰ 5-10% ì ˆê°**

**êµ¬í˜„ ë³µì¡ë„**: â­â­ Low

---

#### ê°œì„ ì•ˆ 4-B: ìºì‹œ LRU ìµœì í™” (High Impact)
```python
# ì œì•ˆ: í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ LRU ìºì‹œ ì‚¬ìš©
from functools import lru_cache
from collections import OrderedDict
import time

class OptimizedMemoryCacheManager:
    """ìµœì í™”ëœ ë©”ëª¨ë¦¬ ìºì‹œ (LRU + TTL)"""

    def __init__(self, maxsize: int = 1000, ttl: int = 3600):
        self.cache: OrderedDict[str, CacheEntry] = OrderedDict()
        self.maxsize = maxsize
        self.ttl = ttl

    async def get(self, cache_key: str) -> list[SearchResult] | None:
        """ìºì‹œ ì¡°íšŒ (LRU + TTL)"""
        if cache_key not in self.cache:
            return None

        entry = self.cache[cache_key]

        # TTL ì²´í¬
        if time.time() - entry.created_at > self.ttl:
            del self.cache[cache_key]  # ğŸ†• ì¦‰ì‹œ ì‚­ì œ
            return None

        # LRU: ìµœê·¼ ì‚¬ìš© í•­ëª©ì„ ë§¨ ë’¤ë¡œ ì´ë™
        self.cache.move_to_end(cache_key)
        return entry.results

    async def set(self, cache_key: str, results: list[SearchResult]) -> None:
        """ìºì‹œ ì €ì¥ (LRU ì •ì±…)"""
        # ìµœëŒ€ í¬ê¸° ì´ˆê³¼ ì‹œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
        if len(self.cache) >= self.maxsize:
            oldest_key = next(iter(self.cache))  # ğŸ†• OrderedDict ì²« í•­ëª©
            del self.cache[oldest_key]
            logger.debug(f"ğŸ—‘ï¸ LRU ìºì‹œ ì œê±°: {oldest_key}")

        self.cache[cache_key] = CacheEntry(
            results=results,
            created_at=time.time(),
        )
        self.cache.move_to_end(cache_key)  # ğŸ†• ë§¨ ë’¤ë¡œ ì´ë™
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€: **100% ì•ˆì „**
- ìºì‹œ íˆíŠ¸ìœ¨: 5-10% í–¥ìƒ (LRU ì •ì±…)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: **11.2MB ìƒí•œ ë³´ì¥**

**êµ¬í˜„ ë³µì¡ë„**: â­â­ Low

---

#### ê°œì„ ì•ˆ 4-C: ë¬¸ì„œ ë‚´ìš© ì••ì¶• ì €ì¥ (Advanced)
```python
# ì œì•ˆ: ìºì‹œì— ì••ì¶•ëœ ë¬¸ì„œ ì €ì¥
import zlib
import pickle

class CompressedCacheManager:
    """ì••ì¶• ìºì‹œ ë§¤ë‹ˆì €"""

    async def set(self, cache_key: str, results: list[SearchResult]) -> None:
        """ì••ì¶• ì €ì¥"""
        # Pickle â†’ zlib ì••ì¶•
        pickled = pickle.dumps(results)
        compressed = zlib.compress(pickled, level=6)  # ì••ì¶•ë¥  vs ì†ë„ ê· í˜•

        self.cache[cache_key] = CacheEntry(
            results=compressed,  # ì••ì¶•ëœ ë°”ì´íŠ¸ ì €ì¥
            created_at=time.time(),
            compressed=True,  # í”Œë˜ê·¸
        )

    async def get(self, cache_key: str) -> list[SearchResult] | None:
        """ì••ì¶• í•´ì œ ì¡°íšŒ"""
        entry = self.cache.get(cache_key)
        if not entry:
            return None

        if entry.compressed:
            # ì••ì¶• í•´ì œ
            decompressed = zlib.decompress(entry.results)
            return pickle.loads(decompressed)
        else:
            return entry.results
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 11.2MB â†’ **5-7MB** (40-60% ì ˆê°)
- ì••ì¶•/í•´ì œ ì‹œê°„: **5-10ms** (ìºì‹œ íˆíŠ¸ ì‹œ í—ˆìš© ê°€ëŠ¥)

**êµ¬í˜„ ë³µì¡ë„**: â­â­â­ Medium

---

### ğŸ“ˆ ê°œì„ ì•ˆ 4 ì¢…í•© íš¨ê³¼

| ê°œì„ ì•ˆ | ì˜ˆìƒ ë©”ëª¨ë¦¬ ì ˆê° | ìš°ì„ ìˆœìœ„ | ë³µì¡ë„ |
|-------|----------------|---------|-------|
| 4-A. ì¤‘ë³µ ë¬¸ì„œ ì œê±° | 50% (ë‹¨ì¼ ìš”ì²­) | ğŸŸ¡ Medium | â­â­ |
| 4-B. LRU ìºì‹œ ìµœì í™” | ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ | ğŸ”´ High | â­â­ |
| 4-C. ì••ì¶• ì €ì¥ | 40-60% (ìºì‹œ) | ğŸŸ¢ Low | â­â­â­ |

**ì „ì²´ ì¡°í•© ì‹œ ì˜ˆìƒ íš¨ê³¼**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ **60-70% ì ˆê°**

---

## ğŸ¯ ë³‘ëª©ì  #5: ë™ì‹œì„± ì²˜ë¦¬ íš¨ìœ¨ì„±

### ğŸ“ ìœ„ì¹˜
- `app/api/services/rag_pipeline.py`
- `app/modules/core/retrieval/orchestrator.py`

### ğŸ” ë¬¸ì œì 

#### 5.1 ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ì œí•œ
```python
# rag_pipeline.py (ì¶”ì •)
# FastAPIëŠ” ê¸°ë³¸ì ìœ¼ë¡œ asyncë¥¼ ì§€ì›í•˜ì§€ë§Œ, ë‚´ë¶€ ë¡œì§ì´ ìˆœì°¨ ì‹¤í–‰ë˜ë©´ ì˜ë¯¸ ì—†ìŒ

@app.post("/api/chat")
async def chat_endpoint(message: str, session_id: str):
    """ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸"""
    # âš ï¸ ë‚´ë¶€ì ìœ¼ë¡œ ìˆœì°¨ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬ ì•ˆ í•¨)
    result = await rag_pipeline.execute(message, session_id)
    return result
```

**ë¬¸ì œì **:
1. FastAPIëŠ” asyncë¥¼ ì§€ì›í•˜ì§€ë§Œ, **ë‚´ë¶€ ë¡œì§ì´ ìˆœì°¨ ì‹¤í–‰**ë˜ë©´ ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ëŠ¥ë ¥ ì œí•œ
2. ë™ì‹œ ìš”ì²­ 10ê°œ â†’ **10 Ã— 3000ms = 30ì´ˆ** (ìˆœì°¨ ì²˜ë¦¬ ì‹œ)
3. DB ì—°ê²° í’€, LLM API í˜¸ì¶œ ì œí•œ ë“±ìœ¼ë¡œ **ìŠ¤ë£¨í’‹ ì €í•˜**

#### 5.2 ë¦¬ì†ŒìŠ¤ ê²½í•©
```python
# orchestrator.py
# ì—¬ëŸ¬ ìš”ì²­ì´ ë™ì‹œì— Weaviate/MongoDBë¥¼ í˜¸ì¶œí•˜ë©´ DB ê³¼ë¶€í•˜ ê°€ëŠ¥
async def search_and_rerank(self, query: str, top_k: int):
    """ê²€ìƒ‰ + ë¦¬ë­í‚¹"""
    # âš ï¸ DB ì—°ê²° í’€ ë¶€ì¡± ì‹œ ëŒ€ê¸° ë°œìƒ
    search_results = await self.retriever.search(query, top_k)
    # âš ï¸ Reranker API í˜¸ì¶œ ì œí•œ (ì´ˆë‹¹ 10 requests ë“±)
    reranked = await self.reranker.rerank(query, search_results, top_k)
```

### âœ… ê°œì„  ë°©ì•ˆ

#### ê°œì„ ì•ˆ 5-A: Request Queue (Rate Limiting)
```python
# ì œì•ˆ: ìš”ì²­ í + Rate Limiting
from asyncio import Queue, Semaphore

class RateLimitedRAGPipeline:
    """Rate Limiting RAG íŒŒì´í”„ë¼ì¸"""

    def __init__(self, base_pipeline: RAGPipeline, max_concurrent: int = 10):
        self.base_pipeline = base_pipeline
        self.semaphore = Semaphore(max_concurrent)  # ë™ì‹œ ì‹¤í–‰ ì œí•œ
        self.request_queue: Queue = Queue()

    async def execute(
        self, message: str, session_id: str, options: dict | None = None
    ) -> RAGResultDict:
        """Rate Limiting ì ìš© ì‹¤í–‰"""
        async with self.semaphore:
            # ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œí•œ (10ê°œê¹Œì§€ë§Œ)
            return await self.base_pipeline.execute(message, session_id, options)
```

**ì˜ˆìƒ íš¨ê³¼**:
- DB ê³¼ë¶€í•˜ ë°©ì§€: **100% ì•ˆì „**
- ë™ì‹œ ìš”ì²­ ì²˜ë¦¬: **10ê°œê¹Œì§€ ë³‘ë ¬**
- ì‘ë‹µ ì‹œê°„: ì•ˆì •ì  (ëŒ€ê¸° ì‹œê°„ ì¶”ê°€ ê°€ëŠ¥)

**êµ¬í˜„ ë³µì¡ë„**: â­â­ Low

---

#### ê°œì„ ì•ˆ 5-B: Connection Pool ì¦ê°€
```python
# ì œì•ˆ: DB ì—°ê²° í’€ í¬ê¸° ì¦ê°€
# weaviate.yaml
connection_pool:
  max_connections: 50  # ê¸°ë³¸ê°’: 20 (2.5ë°° ì¦ê°€)

# mongodb.yaml
connection_pool:
  max_pool_size: 100  # ê¸°ë³¸ê°’: 50 (2ë°° ì¦ê°€)
```

**ì˜ˆìƒ íš¨ê³¼**:
- ë™ì‹œ ìš”ì²­ ì²˜ë¦¬: **2ë°° ê°œì„ **
- ì—°ê²° ëŒ€ê¸° ì‹œê°„: **50-100ms ì ˆì•½**

**êµ¬í˜„ ë³µì¡ë„**: â­ Very Low

---

#### ê°œì„ ì•ˆ 5-C: Background Tasks (Low Priority)
```python
# ì œì•ˆ: ìºì‹œ ì €ì¥, ë¡œê¹… ë“±ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì²˜ë¦¬
from fastapi import BackgroundTasks

@app.post("/api/chat")
async def chat_endpoint(
    message: str,
    session_id: str,
    background_tasks: BackgroundTasks
):
    """ì±„íŒ… ì—”ë“œí¬ì¸íŠ¸ (ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬)"""
    result = await rag_pipeline.execute(message, session_id)

    # âœ… ìºì‹œ ì €ì¥, ë¡œê¹… ë“±ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ
    background_tasks.add_task(log_request, message, result)
    background_tasks.add_task(update_analytics, session_id, result)

    return result
```

**ì˜ˆìƒ íš¨ê³¼**:
- ì‘ë‹µ ì‹œê°„: **10-50ms ì ˆì•½** (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—… ì œì™¸)
- ì‚¬ìš©ì ì²´ê° ì†ë„: **í–¥ìƒ**

**êµ¬í˜„ ë³µì¡ë„**: â­ Very Low

---

### ğŸ“ˆ ê°œì„ ì•ˆ 5 ì¢…í•© íš¨ê³¼

| ê°œì„ ì•ˆ | ì˜ˆìƒ íš¨ê³¼ | ìš°ì„ ìˆœìœ„ | ë³µì¡ë„ |
|-------|----------|---------|-------|
| 5-A. Rate Limiting | ì•ˆì •ì„± 100% | ğŸŸ¡ Medium | â­â­ |
| 5-B. ì—°ê²° í’€ ì¦ê°€ | ìŠ¤ë£¨í’‹ 2ë°° | ğŸŸ¡ Medium | â­ |
| 5-C. Background Tasks | 10-50ms ì ˆì•½ | ğŸŸ¢ Low | â­ |

**ì „ì²´ ì¡°í•© ì‹œ ì˜ˆìƒ íš¨ê³¼**: ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ëŠ¥ë ¥ **2-3ë°° ê°œì„ **

---

## ğŸ“Š ìµœì¢… ì¢…í•© ë¶„ì„

### ìš°ì„ ìˆœìœ„ ë§¤íŠ¸ë¦­ìŠ¤

| ê°œì„ ì•ˆ | ì˜ˆìƒ íš¨ê³¼ | êµ¬í˜„ ë³µì¡ë„ | ROI | ìš°ì„ ìˆœìœ„ |
|-------|----------|-----------|-----|---------|
| **1-A. ë³‘ë ¬ ì›Œí¬í”Œë¡œìš°** | 400-500ms ì ˆì•½ | â­â­â­ | ğŸ”¥ğŸ”¥ğŸ”¥ | 1 |
| **2-A. ì»¨í…ìŠ¤íŠ¸ ì••ì¶•** | 1000-2000ms ì ˆì•½, 50% ë¹„ìš© | â­â­â­ | ğŸ”¥ğŸ”¥ğŸ”¥ | 2 |
| **4-B. LRU ìºì‹œ ìµœì í™”** | ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€ | â­â­ | ğŸ”¥ğŸ”¥ğŸ”¥ | 3 |
| **1-B. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë³‘ë ¬í™”** | 200ms ì ˆì•½ | â­â­ | ğŸ”¥ğŸ”¥ | 4 |
| **2-B. í”„ë¡¬í”„íŠ¸ ìµœì í™”** | 200-400ms ì ˆì•½ | â­ | ğŸ”¥ğŸ”¥ | 5 |
| **3-A. ì¸ë±ìŠ¤ ìµœì í™”** | 50-150ms ì ˆì•½ | â­â­ | ğŸ”¥ | 6 |
| **5-B. ì—°ê²° í’€ ì¦ê°€** | ìŠ¤ë£¨í’‹ 2ë°° | â­ | ğŸ”¥ | 7 |

### ì „ì²´ ì˜ˆìƒ ê°œì„  íš¨ê³¼

#### Phase 1 (Quick Wins - 1ì£¼)
- **1-B, 2-B, 3-A, 4-B, 5-B** êµ¬í˜„
- ì˜ˆìƒ íš¨ê³¼: **30-40% ì‘ë‹µ ì‹œê°„ ë‹¨ì¶•**
- êµ¬í˜„ ì‹œê°„: **3-5ì¼**

#### Phase 2 (High Impact - 2ì£¼)
- **1-A, 2-A** êµ¬í˜„
- ì˜ˆìƒ íš¨ê³¼: **ì¶”ê°€ 40-50% ë‹¨ì¶•** (ëˆ„ì  60-70%)
- êµ¬í˜„ ì‹œê°„: **7-10ì¼**

#### Phase 3 (Advanced - 1ê°œì›”)
- **1-C, 1-D, 2-C, 2-D, 3-D, 4-C, 5-A** êµ¬í˜„
- ì˜ˆìƒ íš¨ê³¼: **ì¶”ê°€ 10-20% ë‹¨ì¶•** (ëˆ„ì  70-80%)
- êµ¬í˜„ ì‹œê°„: **3-4ì£¼**

### ìµœì¢… ì„±ëŠ¥ ì˜ˆì¸¡

| ì§€í‘œ | í˜„ì¬ | Phase 1 | Phase 2 | Phase 3 |
|-----|------|---------|---------|---------|
| **í‰ê·  ì‘ë‹µ ì‹œê°„** | 3000ms | 2000ms | 1000ms | 700ms |
| **P95 ì‘ë‹µ ì‹œê°„** | 5000ms | 3500ms | 2000ms | 1500ms |
| **ë™ì‹œ ìš”ì²­ ì²˜ë¦¬** | 5 req/s | 10 req/s | 15 req/s | 20 req/s |
| **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰** | 50MB | 40MB | 30MB | 20MB |
| **LLM ë¹„ìš©** | $0.0003 | $0.00025 | $0.00015 | $0.0001 |

---

## ğŸ¯ ì‹¤í–‰ ê³„íš (Action Plan)

### Week 1: Quick Wins
- [ ] 1-B. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë³‘ë ¬í™” (`VectorGraphHybridSearch.search()` ìˆ˜ì •)
- [ ] 2-B. í”„ë¡¬í”„íŠ¸ ìµœì í™” (`_build_prompt()` ê°„ì†Œí™”)
- [ ] 3-A. ì¸ë±ìŠ¤ ìµœì í™” (YAML ì„¤ì • ë³€ê²½)
- [ ] 4-B. LRU ìºì‹œ ìµœì í™” (`MemoryCacheManager` ì¬êµ¬í˜„)
- [ ] 5-B. ì—°ê²° í’€ ì¦ê°€ (ì„¤ì • ë³€ê²½)

### Week 2-3: High Impact
- [ ] 1-A. ë³‘ë ¬ ì›Œí¬í”Œë¡œìš° íŒŒì´í”„ë¼ì¸ (`search_and_rerank()` ì¬êµ¬í˜„)
- [ ] 2-A. ì»¨í…ìŠ¤íŠ¸ ì••ì¶• (`ContextCompressor` í´ë˜ìŠ¤ ì‹ ê·œ ì‘ì„±)
- [ ] ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí‚¹
- [ ] ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ êµ¬ì¶•

### Week 4-6: Advanced (ì„ íƒ)
- [ ] 1-C. ì ì‘í˜• top_k (`_search_and_merge()` ìˆ˜ì •)
- [ ] 1-D. ìºì‹œ ì›Œë°ì—… (`CacheWarmer` ì„œë¹„ìŠ¤ ì¶”ê°€)
- [ ] 2-C. Request Batching (`BatchedGenerationModule` ì¶”ê°€)
- [ ] 2-D. Streaming Response (FastAPI ì—”ë“œí¬ì¸íŠ¸ ìˆ˜ì •)
- [ ] 3-D. Lazy Loading (Retriever ìˆ˜ì •)
- [ ] 4-C. ì••ì¶• ì €ì¥ (`CompressedCacheManager` ì¶”ê°€)
- [ ] 5-A. Rate Limiting (`RateLimitedRAGPipeline` ë˜í¼ ì¶”ê°€)

---

## ğŸ“ ê²°ë¡ 

RAG_StandardëŠ” **v3.3.0 Perfect State**ë¡œ ì´ë¯¸ ë†’ì€ ì™„ì„±ë„ë¥¼ ê°–ì¶”ê³  ìˆìœ¼ë‚˜, ì„±ëŠ¥ ë³‘ëª©ì ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë³¸ ë¶„ì„ì—ì„œ ì œì‹œí•œ **17ê°œ ê°œì„ ì•ˆ**ì„ ë‹¨ê³„ì ìœ¼ë¡œ ì ìš©í•˜ë©´:

1. **ì‘ë‹µ ì‹œê°„ 70-80% ë‹¨ì¶•** (3000ms â†’ 700ms)
2. **ë™ì‹œ ìš”ì²­ ì²˜ë¦¬ ëŠ¥ë ¥ 4ë°° í–¥ìƒ** (5 â†’ 20 req/s)
3. **ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 60% ì ˆê°** (50MB â†’ 20MB)
4. **LLM ë¹„ìš© 70% ì ˆê°** ($0.0003 â†’ $0.0001)

**Phase 1 Quick Wins**ë§Œ ì ìš©í•´ë„ **30-40% ì„±ëŠ¥ ê°œì„ **ì„ ì¦‰ì‹œ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì ì§„ì ìœ¼ë¡œ ì ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [FastAPI Performance Best Practices](https://fastapi.tiangolo.com/deployment/concepts/)
- [Weaviate Indexing Optimization](https://weaviate.io/developers/weaviate/config-refs/schema/vector-index)
- [MongoDB Atlas Vector Search Performance](https://www.mongodb.com/docs/atlas/atlas-vector-search/vector-search-overview/)
- [OpenRouter API Documentation](https://openrouter.ai/docs)
- [Python asyncio Performance Guide](https://docs.python.org/3/library/asyncio-dev.html)

---

**ë¶„ì„ ì™„ë£Œì¼**: 2026-01-08
**ë‹¤ìŒ ë‹¨ê³„**: Phase 1 Quick Wins êµ¬í˜„ (1ì£¼ ëª©í‘œ)
