#!/usr/bin/env python
"""
CI/CD ìë™ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸

Golden Datasetì„ ëŒ€ìƒìœ¼ë¡œ ë°°ì¹˜ í‰ê°€ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
CI/CD íŒŒì´í”„ë¼ì¸ì—ì„œ í’ˆì§ˆ ê²Œì´íŠ¸ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    # ê¸°ë³¸ ì‹¤í–‰ (internal í‰ê°€ê¸°)
    python scripts/run_eval.py

    # Ragas í‰ê°€ê¸° ì‚¬ìš©
    python scripts/run_eval.py --provider ragas

    # íŠ¹ì • ë°ì´í„°ì…‹ ì§€ì •
    python scripts/run_eval.py --dataset data/eval_set.json

    # ìµœì†Œ í’ˆì§ˆ ì„ê³„ê°’ ì„¤ì •
    python scripts/run_eval.py --threshold 0.8

    # pytestì™€ í•¨ê»˜ ì‚¬ìš© (ë§ˆì»¤ í•„í„°ë§)
    pytest -m eval

ì¢…ë£Œ ì½”ë“œ:
    0: í‰ê°€ í†µê³¼ (í‰ê·  ì ìˆ˜ >= threshold)
    1: í‰ê°€ ì‹¤íŒ¨ (í‰ê·  ì ìˆ˜ < threshold)
    2: ì‹¤í–‰ ì˜¤ë¥˜
"""

import argparse
import asyncio
import json
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ PYTHONPATHì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


async def load_dataset(dataset_path: str) -> list[dict]:
    """
    Golden Dataset ë¡œë“œ

    Args:
        dataset_path: ë°ì´í„°ì…‹ íŒŒì¼ ê²½ë¡œ (JSON)

    Returns:
        í‰ê°€ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
    """
    path = Path(dataset_path)
    if not path.exists():
        print(f"âš ï¸  ë°ì´í„°ì…‹ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {dataset_path}")
        print("    ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ
        return [
            {
                "query": "í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ì…ë‹ˆë‹¤",
                "answer": "ì´ê²ƒì€ í…ŒìŠ¤íŠ¸ ë‹µë³€ì…ë‹ˆë‹¤.",
                "context": "í…ŒìŠ¤íŠ¸ ì»¨í…ìŠ¤íŠ¸ ë¬¸ì„œì…ë‹ˆë‹¤.",
                "reference": None,
            }
        ]

    with open(path, encoding="utf-8") as f:
        data = json.load(f)

    # í˜•ì‹ ê²€ì¦
    if isinstance(data, list):
        return data
    elif isinstance(data, dict) and "samples" in data:
        return data["samples"]
    else:
        raise ValueError(f"ì˜ëª»ëœ ë°ì´í„°ì…‹ í˜•ì‹: {dataset_path}")


async def run_evaluation(
    samples: list[dict],
    provider: str = "internal",
    threshold: float = 0.7,
) -> dict:
    """
    ë°°ì¹˜ í‰ê°€ ì‹¤í–‰

    Args:
        samples: í‰ê°€ ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸
        provider: í‰ê°€ê¸° ì¢…ë¥˜ ("internal" ë˜ëŠ” "ragas")
        threshold: ìµœì†Œ í’ˆì§ˆ ì„ê³„ê°’

    Returns:
        í‰ê°€ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    from app.modules.core.evaluation import EvaluatorFactory

    config = {
        "evaluation": {
            "enabled": True,
            "provider": provider,
        }
    }

    evaluator = EvaluatorFactory.create(config)

    print(f"\nğŸ“Š ë°°ì¹˜ í‰ê°€ ì‹œì‘ ({provider} í‰ê°€ê¸°)")
    print(f"   ìƒ˜í”Œ ìˆ˜: {len(samples)}")
    print(f"   ì„ê³„ê°’: {threshold}")
    print("-" * 50)

    # í‰ê°€ ì‹¤í–‰
    results = await evaluator.batch_evaluate(samples)

    # í†µê³„ ê³„ì‚°
    if results:
        avg_faithfulness = sum(r.faithfulness for r in results) / len(results)
        avg_relevance = sum(r.relevance for r in results) / len(results)
        avg_overall = sum(r.overall for r in results) / len(results)
        min_overall = min(r.overall for r in results)
        max_overall = max(r.overall for r in results)
    else:
        avg_faithfulness = avg_relevance = avg_overall = 0.0
        min_overall = max_overall = 0.0

    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“ˆ í‰ê°€ ê²°ê³¼:")
    print(f"   í‰ê·  ì¶©ì‹¤ë„ (Faithfulness): {avg_faithfulness:.4f}")
    print(f"   í‰ê·  ê´€ë ¨ì„± (Relevance):    {avg_relevance:.4f}")
    print(f"   í‰ê·  ì¢…í•© (Overall):        {avg_overall:.4f}")
    print(f"   ìµœì†Œ ì ìˆ˜: {min_overall:.4f}, ìµœëŒ€ ì ìˆ˜: {max_overall:.4f}")
    print("-" * 50)

    # ê°œë³„ ê²°ê³¼ ì¶œë ¥
    if len(samples) <= 10:
        print("\nğŸ“ ê°œë³„ ê²°ê³¼:")
        for i, (sample, result) in enumerate(zip(samples, results, strict=True)):
            status = "âœ…" if result.overall >= threshold else "âŒ"
            print(f"   {i+1}. {status} {sample.get('query', 'N/A')[:30]}...")
            print(f"      â†’ ì¶©ì‹¤ë„: {result.faithfulness:.2f}, ê´€ë ¨ì„±: {result.relevance:.2f}, ì¢…í•©: {result.overall:.2f}")

    # í†µê³¼ ì—¬ë¶€ íŒì •
    passed = avg_overall >= threshold
    failed_count = sum(1 for r in results if r.overall < threshold)

    print("\n" + "=" * 50)
    if passed:
        print(f"âœ… í‰ê°€ í†µê³¼! (í‰ê· : {avg_overall:.4f} >= {threshold})")
    else:
        print(f"âŒ í‰ê°€ ì‹¤íŒ¨! (í‰ê· : {avg_overall:.4f} < {threshold})")
        print(f"   ì‹¤íŒ¨í•œ ìƒ˜í”Œ: {failed_count}/{len(results)}")
    print("=" * 50)

    return {
        "passed": passed,
        "avg_faithfulness": avg_faithfulness,
        "avg_relevance": avg_relevance,
        "avg_overall": avg_overall,
        "min_overall": min_overall,
        "max_overall": max_overall,
        "sample_count": len(results),
        "failed_count": failed_count,
    }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="RAG ì‹œìŠ¤í…œ í’ˆì§ˆ í‰ê°€ ìŠ¤í¬ë¦½íŠ¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python scripts/run_eval.py
  python scripts/run_eval.py --provider ragas --threshold 0.8
  python scripts/run_eval.py --dataset data/golden_dataset.json
        """,
    )
    parser.add_argument(
        "--provider",
        type=str,
        default="internal",
        choices=["internal", "ragas"],
        help="í‰ê°€ê¸° ì¢…ë¥˜ (ê¸°ë³¸ê°’: internal)",
    )
    parser.add_argument(
        "--dataset",
        type=str,
        default="data/eval_set.json",
        help="Golden Dataset ê²½ë¡œ (ê¸°ë³¸ê°’: data/eval_set.json)",
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.7,
        help="ìµœì†Œ í’ˆì§ˆ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.7)",
    )
    parser.add_argument(
        "--output",
        type=str,
        default=None,
        help="ê²°ê³¼ ì €ì¥ íŒŒì¼ ê²½ë¡œ (JSON)",
    )

    args = parser.parse_args()

    try:
        # ë°ì´í„°ì…‹ ë¡œë“œ
        samples = asyncio.run(load_dataset(args.dataset))

        # í‰ê°€ ì‹¤í–‰
        result = asyncio.run(
            run_evaluation(
                samples=samples,
                provider=args.provider,
                threshold=args.threshold,
            )
        )

        # ê²°ê³¼ ì €ì¥
        if args.output:
            output_path = Path(args.output)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with open(output_path, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)
            print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ë¨: {args.output}")

        # ì¢…ë£Œ ì½”ë“œ
        sys.exit(0 if result["passed"] else 1)

    except Exception as e:
        print(f"\nâŒ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(2)


if __name__ == "__main__":
    main()
