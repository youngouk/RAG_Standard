"""
Upload API endpoints
íŒŒì¼ ì—…ë¡œë“œ ë° ë¬¸ì„œ ì²˜ë¦¬ API ì—”ë“œí¬ì¸íŠ¸
"""

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Any
from uuid import uuid4

from fastapi import APIRouter, BackgroundTasks, Depends, File, Form, HTTPException, UploadFile
from pydantic import BaseModel, Field

from ..lib.auth import get_api_key
from ..lib.logger import get_logger
from ..modules.core.privacy.masker import DEFAULT_WHITELIST, PrivacyMasker

logger = get_logger(__name__)

# PII ë§ˆìŠ¤í‚¹ì„ ìœ„í•œ ì¸ìŠ¤í„´ìŠ¤ (privacy.enabled ì²´í¬ í›„ ì´ˆê¸°í™”)
# DEFAULT_WHITELIST ì‚¬ìš© (ì˜¤íƒ ë°©ì§€: ì´ëª¨ë‹˜, í—¬í¼ë‹˜, ë‹´ë‹¹ ë“±)
# Note: DI Container ì™¸ë¶€ì—ì„œ ì‚¬ìš©í•˜ë¯€ë¡œ ê¸°ë³¸ í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ì§ì ‘ ì§€ì •
_privacy_masker: PrivacyMasker | None = PrivacyMasker(whitelist=list(DEFAULT_WHITELIST))
# âœ… H4 ë³´ì•ˆ íŒ¨ì¹˜: Upload API ì¸ì¦ ì¶”ê°€
# íŒŒì¼ ì—…ë¡œë“œ/ì‚­ì œëŠ” ì‹œìŠ¤í…œ ë³€ê²½ì´ë¯€ë¡œ ì¸ì¦ í•„ìš”
router = APIRouter(tags=["Upload"], dependencies=[Depends(get_api_key)])
modules: dict[str, Any] = {}
config: dict[str, Any] = {}


def set_dependencies(app_modules: dict[str, Any], app_config: dict[str, Any]):
    """ì˜ì¡´ì„± ì£¼ìž…"""
    global modules, config, _privacy_masker
    modules = app_modules
    config = app_config

    # privacy.enabled: false â†’ PII ë§ˆìŠ¤í‚¹ ë¹„í™œì„±í™”
    privacy_config = config.get("privacy", {})
    if not privacy_config.get("enabled", True):
        _privacy_masker = None
        logger.info("ðŸ”“ Upload API: PII ë§ˆìŠ¤í‚¹ ë¹„í™œì„±í™”ë¨ (privacy.enabled: false)")


JOBS_FILE = Path("/app/uploads/jobs.json")


def load_upload_jobs() -> dict[str, dict[str, Any]]:
    """ì—…ë¡œë“œ ìž‘ì—… ìƒíƒœë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ"""
    try:
        if JOBS_FILE.exists():
            with open(JOBS_FILE, encoding="utf-8") as f:
                loaded_data = json.load(f)
                return dict(loaded_data) if isinstance(loaded_data, dict) else {}
    except Exception as e:
        logger.warning(f"Failed to load jobs file: {e}")
    return {}


def save_upload_jobs(jobs: dict[str, dict[str, Any]]):
    """ì—…ë¡œë“œ ìž‘ì—… ìƒíƒœë¥¼ íŒŒì¼ì— ì €ìž¥"""
    try:
        JOBS_FILE.parent.mkdir(parents=True, exist_ok=True)
        with open(JOBS_FILE, "w", encoding="utf-8") as f:
            json.dump(jobs, f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"Failed to save jobs file: {e}")


upload_jobs: dict[str, dict[str, Any]] = load_upload_jobs()


class DocumentInfo(BaseModel):
    """ë¬¸ì„œ ì •ë³´ ëª¨ë¸"""

    id: str
    filename: str
    file_type: str
    file_size: int
    upload_date: str
    status: str
    chunk_count: int | None = None
    processing_time: float | None = None
    error_message: str | None = None


class UploadResponse(BaseModel):
    """ì—…ë¡œë“œ ì‘ë‹µ ëª¨ë¸"""

    job_id: str
    message: str
    filename: str
    file_size: int
    estimated_processing_time: float
    timestamp: str


class JobStatusResponse(BaseModel):
    """ìž‘ì—… ìƒíƒœ ì‘ë‹µ ëª¨ë¸"""

    job_id: str
    status: str
    progress: float
    message: str
    filename: str
    chunk_count: int | None = None
    processing_time: float | None = None
    error_message: str | None = None
    timestamp: str


class DocumentListResponse(BaseModel):
    """ë¬¸ì„œ ëª©ë¡ ì‘ë‹µ ëª¨ë¸"""

    documents: list[DocumentInfo]
    total_count: int
    page: int
    page_size: int
    has_next: bool


class BulkDeleteRequest(BaseModel):
    """ë²Œí¬ ì‚­ì œ ìš”ì²­ ëª¨ë¸"""

    ids: list[str] = Field(..., description="ì‚­ì œí•  ë¬¸ì„œ ID ëª©ë¡")


class BulkDeleteResponse(BaseModel):
    """ë²Œí¬ ì‚­ì œ ì‘ë‹µ ëª¨ë¸"""

    deleted_count: int
    failed_count: int
    failed_ids: list[str] = []
    message: str
    timestamp: str


def get_upload_directory() -> Path:
    """ì—…ë¡œë“œ ë””ë ‰í† ë¦¬ ë°˜í™˜"""
    upload_path = config.get("uploads", {}).get("directory", "./uploads")
    upload_dir = Path(upload_path).resolve()
    try:
        upload_dir.mkdir(exist_ok=True, parents=True)
        temp_dir = upload_dir / "temp"
        temp_dir.mkdir(exist_ok=True)
    except PermissionError:
        upload_dir = Path("/app/uploads")
        upload_dir.mkdir(exist_ok=True, parents=True)
        temp_dir = upload_dir / "temp"
        temp_dir.mkdir(exist_ok=True)
    return upload_dir


def estimate_processing_time(file_size: int, file_type: str) -> float:
    """íŒŒì¼ í¬ê¸°ì™€ íƒ€ìž…ì„ ê¸°ë°˜ìœ¼ë¡œ ì²˜ë¦¬ ì‹œê°„ ì˜ˆì¸¡"""
    base_time = 20.0
    size_mb = file_size / (1024 * 1024)
    processing_rates = {
        "pdf": 15.0,
        "docx": 10.0,
        "xlsx": 20.0,
        "txt": 3.0,
        "md": 3.0,
        "html": 8.0,
        "csv": 12.0,
        "json": 5.0,
    }
    ext = file_type.lower()
    rate = processing_rates.get(ext, 10.0)
    estimated_time = base_time + size_mb * rate
    if size_mb > 10:
        extra_penalty = (size_mb - 10) * 3
        estimated_time += extra_penalty
    return max(30.0, min(estimated_time, 1800.0))


def validate_file(file: UploadFile) -> dict[str, Any]:
    """íŒŒì¼ ê²€ì¦"""
    supported_types = {
        "application/pdf": "pdf",
        "text/plain": "txt",
        "application/vnd.openxmlformats-officedocument.wordprocessingml.document": "docx",
        "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet": "xlsx",
        "text/csv": "csv",
        "text/html": "html",
        "text/markdown": "md",
        "application/json": "json",
    }
    if file.content_type not in supported_types:
        ext = Path(file.filename or "unknown").suffix.lower()[1:]
        if ext not in supported_types.values():
            return {
                "valid": False,
                "error": {
                    "error": "ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹",
                    "message": f"'{file.content_type}' í˜•ì‹ì€ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
                    "suggestion": "ì§€ì› í˜•ì‹: PDF, DOCX, TXT, MD, CSV, XLSX, HTML, JSON",
                    "file_name": file.filename,
                    "file_type": file.content_type,
                    "supported_extensions": [".pdf", ".docx", ".txt", ".md", ".csv", ".xlsx", ".html", ".json"],
                },
            }
        file_type = ext
    else:
        file_type = supported_types[file.content_type]
    max_size = config.get("uploads", {}).get("max_file_size", 50 * 1024 * 1024)
    if file.size and file.size > max_size:
        max_size_mb = max_size / (1024 * 1024)
        file_size_mb = file.size / (1024 * 1024)
        return {
            "valid": False,
            "error": {
                "error": "íŒŒì¼ í¬ê¸° ì´ˆê³¼",
                "message": f"íŒŒì¼ í¬ê¸°({file_size_mb:.1f}MB)ê°€ ìµœëŒ€ í—ˆìš© í¬ê¸°({max_size_mb:.0f}MB)ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤",
                "suggestion": "íŒŒì¼ì„ ì••ì¶•í•˜ê±°ë‚˜ ì—¬ëŸ¬ íŒŒì¼ë¡œ ë¶„í• í•˜ì—¬ ì—…ë¡œë“œí•˜ì„¸ìš”",
                "file_name": file.filename,
                "file_size_mb": round(file_size_mb, 1),
                "max_size_mb": int(max_size_mb),
            },
        }
    return {"valid": True, "file_type": file_type}


async def process_document_background(job_id: str, file_path: Path, filename: str, file_type: str):
    """ë°±ê·¸ë¼ìš´ë“œ ë¬¸ì„œ ì²˜ë¦¬"""
    try:
        upload_jobs[job_id].update(
            {"status": "processing", "progress": 10, "message": "ë¬¸ì„œ ì²˜ë¦¬ ì‹œìž‘..."}
        )
        save_upload_jobs(upload_jobs)
        document_processor = modules.get("document_processor")
        retrieval_module = modules.get("retrieval")
        if not document_processor or not retrieval_module:
            raise Exception("Required modules not available")
        logger.info(f"Loading document: {filename}")
        upload_jobs[job_id].update({"progress": 30, "message": "ë¬¸ì„œ ë¡œë”© ì¤‘..."})
        save_upload_jobs(upload_jobs)
        file_size = file_path.stat().st_size

        # PII ë§ˆìŠ¤í‚¹: íŒŒì¼ëª…ì—ì„œ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ (í™œì„±í™” ì‹œì—ë§Œ)
        # ì˜ˆ: "í™ê¸¸ë™ ê³ ê°ë‹˜.txt" â†’ "ê³ ê°_ê³ ê°ë‹˜.txt"
        if _privacy_masker:
            masked_filename = _privacy_masker.mask_filename(filename)
            if masked_filename != filename:
                logger.info(f"íŒŒì¼ëª… PII ë§ˆìŠ¤í‚¹ ì ìš©: {filename} â†’ {masked_filename}")
        else:
            masked_filename = filename  # PII ë§ˆìŠ¤í‚¹ ë¹„í™œì„±í™” ì‹œ ì›ë³¸ ì‚¬ìš©

        docs = await document_processor.load_document(
            str(file_path),
            {
                "source_file": masked_filename,
                "file_type": file_type,
                "original_file_size": file_size,
            },
        )
        logger.info(f"Splitting document into chunks: {len(docs)} documents")
        upload_jobs[job_id].update({"progress": 50, "message": "ë¬¸ì„œ ë¶„í•  ì¤‘..."})
        save_upload_jobs(upload_jobs)
        chunks = await document_processor.split_documents(docs)
        logger.info(f"Document split into {len(chunks)} chunks")
        upload_jobs[job_id].update(
            {"progress": 70, "message": f"ìž„ë² ë”© ìƒì„± ì¤‘... ({len(chunks)}ê°œ ì²­í¬)"}
        )
        save_upload_jobs(upload_jobs)
        embedded_chunks = await document_processor.embed_chunks(chunks)
        upload_jobs[job_id].update(
            {"progress": 90, "message": f"ë²¡í„° DBì— ì €ìž¥ ì¤‘... ({len(embedded_chunks)}ê°œ ìž„ë² ë”©)"}
        )
        save_upload_jobs(upload_jobs)
        await retrieval_module.add_documents(embedded_chunks)
        try:
            os.unlink(file_path)
        except Exception as e:
            logger.warning(f"Failed to delete temp file: {e}")
        processing_time = datetime.now().timestamp() - upload_jobs[job_id]["start_time"]
        upload_jobs[job_id].update(
            {
                "status": "completed",
                "progress": 100,
                "message": "ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ",
                "chunk_count": len(chunks),
                "processing_time": processing_time,
            }
        )
        save_upload_jobs(upload_jobs)
        logger.info(
            f"Document processing completed: {filename}, {len(chunks)} chunks, {processing_time:.2f}s"
        )
    except Exception as error:
        logger.error(f"Document processing failed: {error}")
        upload_jobs[job_id].update(
            {
                "status": "failed",
                "progress": 0,
                "message": "ë¬¸ì„œ ì²˜ë¦¬ ì‹¤íŒ¨",
                "error_message": str(error),
            }
        )
        save_upload_jobs(upload_jobs)
        try:
            if file_path.exists():
                os.unlink(file_path)
        except Exception:
            pass


@router.post("/upload", response_model=UploadResponse)
async def upload_document(
    background_tasks: BackgroundTasks,
    file: UploadFile = File(...),
    metadata: str | None = Form(None),
):
    """ë¬¸ì„œ ì—…ë¡œë“œ"""
    try:
        validation = validate_file(file)
        if not validation["valid"]:
            raise HTTPException(status_code=400, detail=validation["error"])
        file_type = validation["file_type"]
        job_id = str(uuid4())
        upload_dir = get_upload_directory()
        temp_dir = upload_dir / "temp"
        safe_filename = Path(file.filename or "unknown").name
        if not safe_filename or safe_filename.startswith("."):
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "ìž˜ëª»ëœ íŒŒì¼ëª…",
                    "message": "íŒŒì¼ëª…ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤",
                    "suggestion": "ì˜¬ë°”ë¥¸ íŒŒì¼ëª…ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì‹œ ì—…ë¡œë“œí•˜ì„¸ìš” (ìˆ¨ê¹€ íŒŒì¼ì€ ì—…ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤)",
                    "file_name": file.filename,
                },
            )
        file_path = temp_dir / f"{job_id}_{safe_filename}"
        try:
            resolved_path = file_path.resolve()
            resolved_temp_dir = temp_dir.resolve()
            if not str(resolved_path).startswith(str(resolved_temp_dir)):
                logger.error(f"Path Traversal ì‹œë„ ì°¨ë‹¨: {file.filename}")
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "ë³´ì•ˆ ê²€ì¦ ì‹¤íŒ¨",
                        "message": "íŒŒì¼ ê²½ë¡œì—ì„œ ë³´ì•ˆ ìœ„í˜‘ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤",
                        "suggestion": "íŒŒì¼ëª…ì— íŠ¹ìˆ˜ë¬¸ìžë‚˜ ê²½ë¡œ ë¬¸ìž(.., /)ê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”",
                        "file_name": file.filename,
                    },
                )
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"íŒŒì¼ ê²½ë¡œ ê²€ì¦ ì‹¤íŒ¨: {e}")
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "íŒŒì¼ ê²½ë¡œ ê²€ì¦ ì‹¤íŒ¨",
                    "message": "íŒŒì¼ ê²½ë¡œë¥¼ ê²€ì¦í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                    "suggestion": "íŒŒì¼ëª…ì— íŠ¹ìˆ˜ë¬¸ìžê°€ í¬í•¨ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”",
                    "file_name": file.filename,
                    "technical_error": str(e),
                },
            ) from e
        with open(file_path, "wb") as buffer:
            content = await file.read()
            buffer.write(content)
        file_size = len(content)
        filename = file.filename or "unknown"
        upload_jobs[job_id] = {
            "job_id": job_id,
            "filename": filename,
            "file_type": file_type,
            "file_size": file_size,
            "status": "pending",
            "progress": 0,
            "message": "ì—…ë¡œë“œ ì™„ë£Œ, ì²˜ë¦¬ ëŒ€ê¸° ì¤‘...",
            "start_time": datetime.now().timestamp(),
            "chunk_count": None,
            "processing_time": None,
            "error_message": None,
        }
        save_upload_jobs(upload_jobs)
        background_tasks.add_task(
            process_document_background, job_id, file_path, filename, file_type
        )
        estimated_time = estimate_processing_time(file_size, file_type)
        logger.info(f"Document upload initiated: {file.filename}, job_id: {job_id}")
        size_mb = file_size / (1024 * 1024)
        if estimated_time > 60:
            time_msg = f"ì•½ {estimated_time / 60:.1f}ë¶„"
        else:
            time_msg = f"ì•½ {estimated_time:.0f}ì´ˆ"
        if size_mb > 10:
            warning_msg = (
                " âš ï¸ ëŒ€ìš©ëŸ‰ íŒŒì¼ë¡œ ì¸í•´ ì²˜ë¦¬ ì‹œê°„ì´ ì˜¤ëž˜ ê±¸ë¦´ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë¸Œë¼ìš°ì €ë¥¼ ë‹«ì§€ ë§ˆì„¸ìš”."
            )
        else:
            warning_msg = ""
        user_message = f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ! ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ìž…ë‹ˆë‹¤. ì˜ˆìƒ ì‹œê°„: {time_msg} (íŒŒì¼ í¬ê¸°: {size_mb:.1f}MB){warning_msg}"
        return UploadResponse(
            job_id=job_id,
            message=user_message,
            filename=filename,
            file_size=file_size,
            estimated_processing_time=estimated_time,
            timestamp=datetime.now().isoformat(),
        )
    except HTTPException:
        raise
    except Exception as error:
        logger.error(f"Upload error: {error}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail={
                "error": "ì—…ë¡œë“œ ì‹¤íŒ¨",
                "message": "íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                "suggestion": "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”. ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìžì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”",
                "file_name": file.filename if file and hasattr(file, "filename") else None,
                "retry_after": 30,
                "technical_error": str(error),
            },
        ) from error


@router.get("/upload/status/{job_id}", response_model=JobStatusResponse)
async def get_upload_status(job_id: str):
    """ì—…ë¡œë“œ ìž‘ì—… ìƒíƒœ ì¡°íšŒ"""
    global upload_jobs
    if job_id not in upload_jobs:
        logger.info(f"Job {job_id} not found in memory, reloading from file")
        upload_jobs = load_upload_jobs()
        if job_id not in upload_jobs:
            logger.warning(f"Job {job_id} not found even after reload")
            raise HTTPException(
                status_code=404,
                detail={
                    "error": "ìž‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ",
                    "message": "ìš”ì²­í•˜ì‹  ì—…ë¡œë“œ ìž‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "suggestion": "ì„œë²„ê°€ ìž¬ì‹œìž‘ë˜ì—ˆì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. íŒŒì¼ì„ ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”",
                    "job_id": job_id,
                    "retry_upload": True,
                },
            )
    job = upload_jobs[job_id]
    current_processing_time = None
    if job["status"] == "processing":
        current_processing_time = datetime.now().timestamp() - job["start_time"]
    logger.info(f"Job {job_id} status: {job['status']}, progress: {job['progress']}%")
    return JobStatusResponse(
        job_id=job_id,
        status=job["status"],
        progress=job["progress"],
        message=job["message"],
        filename=job["filename"],
        chunk_count=job["chunk_count"],
        processing_time=job["processing_time"] or current_processing_time,
        error_message=job["error_message"],
        timestamp=datetime.now().isoformat(),
    )


@router.get("/upload/documents", response_model=DocumentListResponse)
async def list_documents(page: int = 1, page_size: int = 20):
    """ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    try:
        retrieval_module = modules.get("retrieval")
        if not retrieval_module:
            raise HTTPException(
                status_code=500,
                detail={
                    "error": "ì‹œìŠ¤í…œ ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€",
                    "message": "ë¬¸ì„œ ê²€ìƒ‰ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "suggestion": "ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ê´€ë¦¬ìžì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”",
                    "module_name": "retrieval",
                    "retry_after": 60,
                },
            )
        logger.info(f"Listing documents: page={page}, page_size={page_size}")
        documents_data = await retrieval_module.list_documents(page=page, page_size=page_size)
        logger.info(f"Retrieved documents_data: {documents_data}")
        documents = []
        for doc_data in documents_data.get("documents", []):
            upload_date = doc_data.get("upload_date", 0)
            if isinstance(upload_date, int | float) and upload_date > 0:
                upload_date = datetime.fromtimestamp(upload_date).isoformat()
            else:
                upload_date = datetime.now().isoformat()
            documents.append(
                DocumentInfo(
                    id=doc_data.get("id", "unknown"),
                    filename=doc_data.get("filename", "unknown"),
                    file_type=doc_data.get("file_type", "unknown"),
                    file_size=doc_data.get("file_size", 0),
                    upload_date=upload_date,
                    status="completed",
                    chunk_count=doc_data.get("chunk_count", 0),
                )
            )
        total_count = documents_data.get("total_count", len(documents))
        response = DocumentListResponse(
            documents=documents,
            total_count=total_count,
            page=page,
            page_size=page_size,
            has_next=page * page_size < total_count,
        )
        logger.info(f"Returning response: {len(documents)} documents, total={total_count}")
        return response
    except HTTPException:
        raise
    except Exception as error:
        logger.error(f"List documents error: {error}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail={
                "error": "ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨",
                "message": "ë¬¸ì„œ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                "suggestion": "ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ê´€ë¦¬ìžì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”",
                "page": page,
                "page_size": page_size,
                "retry_after": 30,
                "technical_error": str(error),
            },
        ) from error


@router.delete("/upload/documents/{document_id}")
async def delete_document(document_id: str):
    """ë¬¸ì„œ ì‚­ì œ"""
    try:
        retrieval_module = modules.get("retrieval")
        if not retrieval_module:
            raise HTTPException(
                status_code=500,
                detail={
                    "error": "ì‹œìŠ¤í…œ ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€",
                    "message": "ë¬¸ì„œ ê²€ìƒ‰ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "suggestion": "ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ê´€ë¦¬ìžì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”",
                    "module_name": "retrieval",
                    "retry_after": 60,
                },
            )
        await retrieval_module.delete_document(document_id)
        logger.info(f"Document deleted: {document_id}")
        return {
            "message": "Document deleted successfully",
            "document_id": document_id,
            "timestamp": datetime.now().isoformat(),
        }
    except HTTPException:
        raise
    except Exception as error:
        logger.error(f"Delete document error: {error}")
        raise HTTPException(
            status_code=500,
            detail={
                "error": "ë¬¸ì„œ ì‚­ì œ ì‹¤íŒ¨",
                "message": "ë¬¸ì„œë¥¼ ì‚­ì œí•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                "suggestion": "ë¬¸ì„œê°€ ì´ë¯¸ ì‚­ì œë˜ì—ˆê±°ë‚˜ ì ‘ê·¼ ê¶Œí•œì´ ì—†ì„ ìˆ˜ ìžˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ê´€ë¦¬ìžì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”",
                "document_id": document_id,
                "retry_after": 30,
                "technical_error": str(error),
            },
        ) from error


@router.post("/upload/documents/bulk-delete", response_model=BulkDeleteResponse)
async def bulk_delete_documents(request: BulkDeleteRequest):
    """ë¬¸ì„œ ì¼ê´„ ì‚­ì œ"""
    try:
        retrieval_module = modules.get("retrieval")
        if not retrieval_module:
            raise HTTPException(
                status_code=500,
                detail={
                    "error": "ì‹œìŠ¤í…œ ëª¨ë“ˆ ì‚¬ìš© ë¶ˆê°€",
                    "message": "ë¬¸ì„œ ê²€ìƒ‰ ëª¨ë“ˆì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
                    "suggestion": "ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•˜ê³  ê´€ë¦¬ìžì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”",
                    "module_name": "retrieval",
                    "retry_after": 60,
                },
            )
        deleted_count = 0
        failed_count = 0
        failed_ids = []
        logger.info(f"Bulk delete requested for {len(request.ids)} documents: {request.ids}")
        for document_id in request.ids:
            try:
                if not document_id or document_id.strip() == "":
                    logger.warning(f"Skipping invalid document ID: {document_id}")
                    failed_count += 1
                    failed_ids.append(document_id)
                    continue
                await retrieval_module.delete_document(document_id)
                deleted_count += 1
                logger.info(f"Successfully deleted document: {document_id}")
            except Exception as delete_error:
                logger.error(f"Failed to delete document {document_id}: {delete_error}")
                failed_count += 1
                failed_ids.append(document_id)
        message = f"Bulk delete completed: {deleted_count} deleted, {failed_count} failed"
        logger.info(message)
        return BulkDeleteResponse(
            deleted_count=deleted_count,
            failed_count=failed_count,
            failed_ids=failed_ids,
            message=message,
            timestamp=datetime.now().isoformat(),
        )
    except HTTPException:
        raise
    except Exception as error:
        logger.error(f"Bulk delete error: {error}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail={
                "error": "ì¼ê´„ ì‚­ì œ ì‹¤íŒ¨",
                "message": "ë¬¸ì„œ ì¼ê´„ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤",
                "suggestion": "ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ê´€ë¦¬ìžì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”",
                "requested_count": len(request.ids) if request and hasattr(request, "ids") else 0,
                "retry_after": 30,
                "technical_error": str(error),
            },
        ) from error


@router.get("/upload/supported-types")
async def get_supported_types():
    """ì§€ì›í•˜ëŠ” íŒŒì¼ íƒ€ìž… ëª©ë¡"""
    return {
        "supported_types": {
            "pdf": {
                "mime_type": "application/pdf",
                "description": "PDF documents",
                "max_size_mb": 10,
            },
            "docx": {
                "mime_type": "application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                "description": "Microsoft Word documents",
                "max_size_mb": 10,
            },
            "xlsx": {
                "mime_type": "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                "description": "Microsoft Excel spreadsheets",
                "max_size_mb": 10,
            },
            "txt": {
                "mime_type": "text/plain",
                "description": "Plain text files",
                "max_size_mb": 10,
            },
            "csv": {
                "mime_type": "text/csv",
                "description": "Comma-separated values",
                "max_size_mb": 10,
            },
            "html": {"mime_type": "text/html", "description": "HTML documents", "max_size_mb": 10},
            "md": {
                "mime_type": "text/markdown",
                "description": "Markdown documents",
                "max_size_mb": 10,
            },
            "json": {
                "mime_type": "application/json",
                "description": "JSON documents",
                "max_size_mb": 10,
            },
        },
        "max_file_size": config.get("uploads", {}).get("max_file_size", 50 * 1024 * 1024),
        "max_files_per_request": 1,
    }
