"""
RAG Pipeline ëª¨ë“ˆ

7ê°œ ë…ë¦½ ë‹¨ê³„ë¡œ ë¶„í•´ëœ RAG íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.
ê° ë‹¨ê³„ëŠ” ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë° ìµœì í™” ê°€ëŠ¥.

ë‹¨ê³„:
1. route_query: ì¿¼ë¦¬ ë¼ìš°íŒ… (ê·œì¹™ ê¸°ë°˜ + LLM í´ë°±)
2. prepare_context: ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ + ì¿¼ë¦¬ í™•ì¥
3. retrieve_documents: MongoDB Atlas í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
4. rerank_documents: ë¦¬ë­í‚¹ (ì„ íƒì )
5. generate_answer: LLM ë‹µë³€ ìƒì„±
6. format_sources: Source ê°ì²´ ë³€í™˜
7. build_result: ìµœì¢… ì‘ë‹µ êµ¬ì„±

ì‘ì„±ì¼: 2025-01-27
ëª©ì : TASK-H4 êµ¬í˜„ - 150ì¤„ ë¸”ë™ë°•ìŠ¤ í•¨ìˆ˜ â†’ 7ê°œ ë…ë¦½ ë©”ì„œë“œ
"""

import asyncio
import os
import time
from collections.abc import Callable
from dataclasses import dataclass, field
from typing import Any, cast

from ...lib.circuit_breaker import CircuitBreakerOpenError
from ...lib.cost_tracker import CostTracker
from ...lib.errors import ErrorCode, GenerationError, RetrievalError
from ...lib.langfuse_client import langfuse_context, observe  # Langfuse íŠ¸ë ˆì´ì‹±
from ...lib.logger import get_logger
from ...lib.metrics import PerformanceMetrics
from ...lib.prompt_sanitizer import contains_output_leakage, validate_document
from ...lib.score_normalizer import RRFScoreNormalizer  # RRF ì ìˆ˜ ì •ê·œí™”
from ...lib.types import RAGResultDict
from ...modules.core.agent.interfaces import AgentResult
from ...modules.core.agent.orchestrator import AgentOrchestrator
from ...modules.core.generation.generator import GenerationResult
from ...modules.core.privacy.masker import PrivacyMasker
from ...modules.core.retrieval.interfaces import IMultiQueryRetriever, SearchResult
from ...modules.core.routing.rule_based_router import RuleBasedRouter
from ...modules.core.sql_search import SQLSearchResult, SQLSearchService
from ..schemas.debug import DebugTrace

logger = get_logger(__name__)


@dataclass
class RouteDecision:
    """
    ì¿¼ë¦¬ ë¼ìš°íŒ… ê²°ì • ê²°ê³¼

    Attributes:
        should_continue: RAG íŒŒì´í”„ë¼ì¸ ê³„ì† ì§„í–‰ ì—¬ë¶€
        immediate_response: ì¦‰ì‹œ ì‘ë‹µ (direct_answer/blockedì¸ ê²½ìš°)
        metadata: ë¼ìš°íŒ… ë©”íƒ€ë°ì´í„° (route, confidence, intent, domain ë“±)

    Examples:
        # ì¦‰ì‹œ ì‘ë‹µ (íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨)
        RouteDecision(
            should_continue=False,
            immediate_response={"answer": "ì•ˆë…•í•˜ì„¸ìš”!", ...},
            metadata={"route": "direct_answer", "confidence": 0.95}
        )

        # RAG ê³„ì† ì§„í–‰
        RouteDecision(
            should_continue=True,
            immediate_response=None,
            metadata={"route": "rag", "domain": "document_query"}
        )
    """

    should_continue: bool
    immediate_response: RAGResultDict | None = None
    metadata: dict[str, Any] = field(default_factory=dict)


@dataclass
class PreparedContext:
    """
        ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ + ì¿¼ë¦¬ í™•ì¥ ê²°ê³¼ (Multi-Query RRF ì§€ì›)

        Attributes:
            session_context: ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (ìµœê·¼ 5ê°œ ëŒ€í™”)
            expanded_query: í™•ì¥ëœ ì¿¼ë¦¬ (ì²« ë²ˆì§¸ ì¿¼ë¦¬, í•˜ìœ„ í˜¸í™˜ì„±)
            original_query: ì›ë³¸ ì¿¼ë¦¬ (ì°¸ì¡°ìš©)
            expanded_queries: í™•ì¥ëœ ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ (Multi-Query RRFìš©, ê¸°ë³¸ 5ê°œ)
            query_weights: ì¿¼ë¦¬ ê°€ì¤‘ì¹˜ ë¦¬ìŠ¤íŠ¸ (1.0, 0.8, 0.6, 0.4, 0.2)

        Examples:
            # Multi-Query RRF
            PreparedContext(
                session_context="ì‚¬ìš©ì: ì•ˆë…•í•˜ì„¸ìš”
    ë´‡: ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?",
                expanded_query="ë¶€ì‚°ì‹œ ì£¼ë¯¼ë“±ë¡ ë°œê¸‰ ë°©ë²• ë° í•„ìš” ì„œë¥˜",
                original_query="ì£¼ë¯¼ë“±ë¡ ë°œê¸‰",
                expanded_queries=["ë¶€ì‚°ì‹œ ì£¼ë¯¼ë“±ë¡ ë°œê¸‰ ë°©ë²•", "ì£¼ë¯¼ë“±ë¡ë“±ë³¸ ì‹ ì²­", ...],
                query_weights=[1.0, 0.8, 0.6, 0.4, 0.2]
            )
    """

    session_context: str | None
    expanded_query: str
    original_query: str
    expanded_queries: list[str] = field(default_factory=list)  # Multi-Query RRFìš©
    query_weights: list[float] = field(default_factory=list)  # ì¿¼ë¦¬ ê°€ì¤‘ì¹˜


@dataclass
class RetrievalResults:
    """
    ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼

    Attributes:
        documents: Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ (langchain_core.documents.Document)
        count: ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜

    Examples:
        RetrievalResults(
            documents=[
                Document(page_content="...", metadata={"source": "doc1.pdf", "score": 0.89}),
                Document(page_content="...", metadata={"source": "doc2.pdf", "score": 0.76})
            ],
            count=2
        )
    """

    documents: list[Any]
    count: int


@dataclass
class RerankResults:
    """
    ë¦¬ë­í‚¹ ê²°ê³¼

    Attributes:
        documents: ë¦¬ë­í‚¹ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
        count: ë¦¬ë­í‚¹ëœ ë¬¸ì„œ ìˆ˜
        reranked: ì‹¤ì œë¡œ ë¦¬ë­í‚¹ì´ ìˆ˜í–‰ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€

    Examples:
        # ë¦¬ë­í‚¹ ì„±ê³µ
        RerankResults(documents=[...], count=10, reranked=True)

        # ë¦¬ë­í‚¹ ì‹¤íŒ¨ (ì›ë³¸ ë°˜í™˜)
        RerankResults(documents=[...], count=15, reranked=False)
    """

    documents: list[Any]
    count: int
    reranked: bool


# GenerationResultëŠ” generator.pyì—ì„œ import (L30)


@dataclass
class FormattedSources:
    """
    í¬ë§·íŒ…ëœ ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸

    Attributes:
        sources: Source ê°ì²´ ë¦¬ìŠ¤íŠ¸ (app.models.prompts.Source)
        count: ì†ŒìŠ¤ ê°œìˆ˜

    Examples:
        FormattedSources(
            sources=[Source(id=0, document="doc1.pdf", relevance=0.89, ...), ...],
            count=5
        )
    """

    sources: list[Any]
    count: int


class PipelineTracker:
    """
    RAG íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ë³„ íƒ€ì´ë° ì¶”ì  í´ë˜ìŠ¤

    8ê°œ ë‹¨ê³„ ê°ê°ì˜ ì‹¤í–‰ ì‹œê°„ì„ ê¸°ë¡í•˜ê³ , ë³‘ëª© ì§€ì ì„ ì‹ë³„.

    ì‚¬ìš©ë²•:
        tracker = PipelineTracker()
        tracker.start_pipeline()

        tracker.start_stage("route_query")
        # ... ì‘ì—… ìˆ˜í–‰ ...
        tracker.end_stage("route_query")

        tracker.end_pipeline()
        metrics = tracker.get_metrics()

    ë©”íŠ¸ë¦­ í˜•ì‹:
        {
            'total_duration_ms': 1250.5,
            'stages': {
                'route_query': {'duration_ms': 45.2, 'percentage': 3.6},
                'retrieve_documents': {'duration_ms': 823.1, 'percentage': 65.8},
                ...
            },
            'slowest_stage': 'retrieve_documents'
        }
    """

    def __init__(self):
        """PipelineTracker ì´ˆê¸°í™”"""
        self.stages: dict[str, dict[str, Any]] = {}  # Multi-Query RRF ë©”íƒ€ë°ì´í„°ë¥¼ ìœ„í•´ Any í—ˆìš©
        self.start_time: float = 0.0
        self.end_time: float = 0.0

    def start_pipeline(self) -> None:
        """íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹œê°„ ê¸°ë¡"""
        self.start_time = time.time()
        logger.debug("Pipeline tracking ì‹œì‘")

    def start_stage(self, stage_name: str) -> None:
        """
        ë‹¨ê³„ ì‹œì‘ ì‹œê°„ ê¸°ë¡

        Args:
            stage_name: ë‹¨ê³„ ì´ë¦„ (ì˜ˆ: "route_query", "retrieve_documents")
        """
        if stage_name not in self.stages:
            self.stages[stage_name] = {}
        self.stages[stage_name]["start"] = time.time()

    def end_stage(self, stage_name: str) -> None:
        """
        ë‹¨ê³„ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡ ë° duration ê³„ì‚°

        Args:
            stage_name: ë‹¨ê³„ ì´ë¦„
        """
        if stage_name in self.stages and "start" in self.stages[stage_name]:
            self.stages[stage_name]["end"] = time.time()
            self.stages[stage_name]["duration"] = (
                self.stages[stage_name]["end"] - self.stages[stage_name]["start"]
            )
        else:
            logger.warning(
                "Stageê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì´ë¯¸ ì¢…ë£Œë¨",
                extra={"stage_name": stage_name}
            )

    def end_pipeline(self) -> None:
        """íŒŒì´í”„ë¼ì¸ ì¢…ë£Œ ì‹œê°„ ê¸°ë¡"""
        self.end_time = time.time()
        logger.debug("Pipeline tracking ì¢…ë£Œ")

    def get_metrics(self) -> dict[str, Any]:
        """
        ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë°˜í™˜

        Returns:
            ë©”íŠ¸ë¦­ ë”•ì…”ë„ˆë¦¬:
            - total_duration_ms: ì „ì²´ ì‹¤í–‰ ì‹œê°„ (ë°€ë¦¬ì´ˆ)
            - stages: ê° ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œê°„ ë° ë¹„ìœ¨
            - slowest_stage: ê°€ì¥ ëŠë¦° ë‹¨ê³„ ì´ë¦„
        """
        total_duration = self.end_time - self.start_time if self.end_time > 0 else 0
        stage_metrics = {}
        for stage, times in self.stages.items():
            duration = times.get("duration", 0)
            percentage = duration / total_duration * 100 if total_duration > 0 else 0
            stage_metrics[stage] = {
                "duration_ms": round(duration * 1000, 1),
                "percentage": round(percentage, 1),
            }
        slowest_stage = None
        if self.stages:
            slowest_stage = max(self.stages.items(), key=lambda x: x[1].get("duration", 0))[0]
        return {
            "total_duration_ms": round(total_duration * 1000, 1),
            "stages": stage_metrics,
            "slowest_stage": slowest_stage,
        }

    def log_summary(self) -> None:
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìš”ì•½ ë¡œê·¸ ì¶œë ¥"""
        metrics = self.get_metrics()
        logger.info("=" * 60)
        logger.info("Pipeline Performance Summary")
        logger.info(
            "ì´ ì‹¤í–‰ ì‹œê°„",
            extra={"total_duration_ms": metrics['total_duration_ms']}
        )
        logger.info(
            "ê°€ì¥ ëŠë¦° ë‹¨ê³„",
            extra={"slowest_stage": metrics.get('slowest_stage', 'N/A')}
        )
        logger.info("-" * 60)
        for stage, data in metrics["stages"].items():
            logger.info(
                "ë‹¨ê³„ë³„ ì„±ëŠ¥",
                extra={
                    "stage": stage,
                    "duration_ms": data['duration_ms'],
                    "percentage": data['percentage']
                }
            )
        logger.info("=" * 60)


class RAGPipeline:
    """
    RAG íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

    8ê°œ ë…ë¦½ ë‹¨ê³„ë¡œ ë¶„í•´ëœ íŒŒì´í”„ë¼ì¸:
    1. route_query: ì¿¼ë¦¬ ë¼ìš°íŒ…
    2. prepare_context: ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„
    3. retrieve_documents: ë¬¸ì„œ ê²€ìƒ‰
    4. rerank_documents: ë¦¬ë­í‚¹
    5. generate_answer: ë‹µë³€ ìƒì„±
    6. self_rag_verify: Self-RAG í’ˆì§ˆ ê²€ì¦ (ì„ íƒì )
    7. format_sources: ì†ŒìŠ¤ í¬ë§·íŒ…
    8. build_result: ê²°ê³¼ êµ¬ì„±

    ê° ë‹¨ê³„ëŠ” ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ë° ìµœì í™” ê°€ëŠ¥.
    """

    # ê¸°ë³¸ê°’ (YAML ì„¤ì •ì´ ì—†ì„ ë•Œë§Œ ì‚¬ìš©)
    FALLBACK_RETRIEVAL_LIMIT = 8
    FALLBACK_MIN_SCORE = 0.05
    FALLBACK_RERANK_TOP_N = 8

    def __init__(
        self,
        config: dict[str, Any],
        query_router: Any | None,
        query_expansion: Any | None,
        retrieval_module: Any,
        generation_module: Any,
        session_module: Any,
        self_rag_module: Any | None,
        extract_topic_func: Callable,
        circuit_breaker_factory: Any,
        cost_tracker: CostTracker,
        performance_metrics: PerformanceMetrics,
        sql_search_service: SQLSearchService | None = None,
        agent_orchestrator: AgentOrchestrator | None = None,
    ):
        """
        RAGPipeline ì´ˆê¸°í™” (ì˜ì¡´ì„± ì£¼ì…)

        Args:
            config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            query_router: ì¿¼ë¦¬ ë¼ìš°í„° (ì„ íƒì )
            query_expansion: ì¿¼ë¦¬ í™•ì¥ ëª¨ë“ˆ (ì„ íƒì )
            retrieval_module: ê²€ìƒ‰ ëª¨ë“ˆ (í•„ìˆ˜)
            generation_module: ìƒì„± ëª¨ë“ˆ (í•„ìˆ˜)
            session_module: ì„¸ì…˜ ëª¨ë“ˆ (í•„ìˆ˜)
            self_rag_module: Self-RAG ëª¨ë“ˆ (ì„ íƒì )
            extract_topic_func: í† í”½ ì¶”ì¶œ í•¨ìˆ˜
            circuit_breaker_factory: Circuit Breaker Factory (í•„ìˆ˜)
            cost_tracker: ë¹„ìš© ì¶”ì ê¸° (í•„ìˆ˜)
            performance_metrics: ì„±ëŠ¥ ë©”íŠ¸ë¦­ (í•„ìˆ˜)
            sql_search_service: SQL ê²€ìƒ‰ ì„œë¹„ìŠ¤ (ì„ íƒì , Phase 3)
            agent_orchestrator: Agent ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (ì„ íƒì , Agentic RAG)
        """
        self.config = config
        self.query_router = query_router
        self.query_expansion = query_expansion
        self.retrieval_module = retrieval_module
        self.generation_module = generation_module
        self.session_module = session_module
        self.self_rag_module = self_rag_module
        self.extract_topic_func = extract_topic_func
        self.circuit_breaker_factory = circuit_breaker_factory
        self.cost_tracker = cost_tracker
        self.performance_metrics = performance_metrics
        self.sql_search_service = sql_search_service  # SQL ê²€ìƒ‰ ì„œë¹„ìŠ¤ (Phase 3)
        self.agent_orchestrator = agent_orchestrator  # Agent ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° (Agentic RAG)

        # YAML ì„¤ì •ì—ì„œ retrieval íŒŒë¼ë¯¸í„° ë¡œë“œ
        rag_config = config.get("rag", {})
        retrieval_config = config.get("retrieval", {})

        self.retrieval_limit = rag_config.get(
            "top_k", retrieval_config.get("top_k", self.FALLBACK_RETRIEVAL_LIMIT)
        )
        self.min_score = retrieval_config.get("min_score", self.FALLBACK_MIN_SCORE)
        self.rerank_top_n = rag_config.get("rerank_top_k", self.FALLBACK_RERANK_TOP_N)

        # RRF ì ìˆ˜ ì •ê·œí™” (0~1 ë²”ìœ„ ë³€í™˜)
        score_norm_config = rag_config.get("score_normalization", {})
        self.score_normalizer = RRFScoreNormalizer.from_config(score_norm_config)

        # ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ (íŒŒì¼ëª…, ë‹µë³€ í…ìŠ¤íŠ¸)
        # privacy.yaml í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ ë¡œë“œ (ì˜¤íƒ ë°©ì§€: ì´ëª¨ë‹˜, í—¬í¼ë‹˜, ë‹´ë‹¹ ë“±)
        # privacy.enabled: false â†’ ë§ˆìŠ¤í‚¹ ì™„ì „ ë¹„í™œì„±í™”
        privacy_config = config.get("privacy", {})
        privacy_enabled = privacy_config.get("enabled", True)

        if privacy_enabled:
            whitelist = privacy_config.get("whitelist", [])
            masking_config = privacy_config.get("masking", {})
            char_config = privacy_config.get("characters", {})

            self.privacy_masker = PrivacyMasker(
                mask_phone=masking_config.get("phone", True),
                mask_name=masking_config.get("name", True),
                mask_email=masking_config.get("email", False),
                phone_mask_char=char_config.get("phone", "*"),
                name_mask_char=char_config.get("name", "*"),
                whitelist=whitelist,  # ê³µìš© í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ (privacy.yaml)
            )
        else:
            self.privacy_masker = None  # PII ë§ˆìŠ¤í‚¹ ë¹„í™œì„±í™”
            logger.info(
                "PII ë§ˆìŠ¤í‚¹ ë¹„í™œì„±í™”",
                extra={"config_key": "privacy.enabled", "value": False}
            )

        logger.info(
            "RAG íŒŒë¼ë¯¸í„° ì„¤ì •",
            extra={
                "top_k": self.retrieval_limit,
                "rerank_top_k": self.rerank_top_n,
                "min_score": self.min_score
            }
        )

        from ..schemas.chat_schemas import Source

        self.Source = Source
        logger.info(
            "RAGPipeline ì´ˆê¸°í™” ì™„ë£Œ",
            extra={
                "sql_search": "í™œì„±í™”" if sql_search_service else "ë¹„í™œì„±í™”",
                "agent": "í™œì„±í™”" if agent_orchestrator else "ë¹„í™œì„±í™”",
                "score_normalization": "í™œì„±í™”" if score_norm_config.get('enabled', True) else "ë¹„í™œì„±í™”"
            }
        )

    def _create_fallback_response(
        self, message: str, start_time: float, routing_metadata: dict[str, Any]
    ) -> RAGResultDict:
        """ë¼ìš°íŒ… ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì‘ë‹µ ìƒì„±"""
        processing_time = time.time() - start_time
        return cast(
            RAGResultDict,
            {
                "answer": "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                "sources": [],
                "tokens_used": 0,
                "topic": self.extract_topic_func(message),
                "processing_time": processing_time,
                "search_results": 0,
                "ranked_results": 0,
                "model_info": {"provider": "system", "model": "fallback"},
                "routing_metadata": routing_metadata,
            },
        )

    async def _execute_parallel_search(
        self,
        message: str,
        prepared_context: PreparedContext,
        options: dict[str, Any],
    ) -> tuple[RetrievalResults, SQLSearchResult | None]:
        """SQL + RAG ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰"""
        if self.sql_search_service and self.sql_search_service.is_enabled():
            logger.info("SQL ê²€ìƒ‰ + RAG ê²€ìƒ‰ ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘")
            rag_task = self.retrieve_documents(
                prepared_context.expanded_queries,
                prepared_context.query_weights,
                prepared_context.session_context,
                options,
            )
            sql_task = self._execute_sql_search(message)

            rag_result, sql_result = await asyncio.gather(
                rag_task, sql_task, return_exceptions=True
            )

            if isinstance(rag_result, Exception):
                logger.error("RAG ê²€ìƒ‰ ì‹¤íŒ¨", extra={"error": str(rag_result)}, exc_info=True)
                raise rag_result
            retrieval_results = rag_result

            sql_search_result = None
            if isinstance(sql_result, Exception):
                logger.warning(
                    "SQL ê²€ìƒ‰ ì‹¤íŒ¨ (ë¬´ì‹œ)",
                    extra={"error": str(sql_result)},
                    exc_info=True
                )
            else:
                sql_search_result = sql_result
                if sql_search_result and sql_search_result.used:
                    row_count = sql_search_result.query_result.row_count if sql_search_result.query_result else 0
                    logger.info(
                        "SQL ê²€ìƒ‰ ì„±ê³µ",
                        extra={
                            "row_count": row_count,
                            "total_time": sql_search_result.total_time
                        }
                    )

            return retrieval_results, sql_search_result
        else:
            retrieval_results = await self.retrieve_documents(
                prepared_context.expanded_queries,
                prepared_context.query_weights,
                prepared_context.session_context,
                options,
            )
            return retrieval_results, None

    def _track_debug_documents(
        self, enable_debug_trace: bool, debug_trace_data: dict[str, Any], documents: list[Any]
    ) -> None:
        """ë””ë²„ê·¸ ì¶”ì ìš© ë¬¸ì„œ ì •ë³´ ê¸°ë¡"""
        if not enable_debug_trace:
            return

        debug_trace_data["retrieved_documents"] = [
            {
                "id": doc.metadata.get("id", "") if hasattr(doc, "metadata") else "",
                "title": doc.metadata.get("title", "") if hasattr(doc, "metadata") else "",
                "chunk_text": (getattr(doc, "page_content", "")[:200] if hasattr(doc, "page_content") else ""),
                "vector_score": doc.metadata.get("score", 0.0) if hasattr(doc, "metadata") else 0.0,
                "bm25_score": doc.metadata.get("bm25_score") if hasattr(doc, "metadata") else None,
                "rerank_score": None,
                "used_in_answer": False,
            }
            for doc in documents
        ]

    def _update_retrieval_metrics(
        self,
        tracker: PipelineTracker,
        prepared_context: PreparedContext,
        sql_search_result: SQLSearchResult | None,
    ) -> None:
        """ê²€ìƒ‰ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        tracker.stages["retrieve_documents"]["multi_query_count"] = len(
            prepared_context.expanded_queries
        )
        tracker.stages["retrieve_documents"]["rrf_enabled"] = (
            len(prepared_context.expanded_queries) > 1
        )
        tracker.stages["retrieve_documents"]["query_weights"] = prepared_context.query_weights
        tracker.stages["retrieve_documents"]["sql_search_used"] = (
            sql_search_result.used if sql_search_result else False
        )

    def _create_debug_trace(
        self,
        enable_debug_trace: bool,
        debug_trace_data: dict[str, Any],
        message: str,
    ) -> DebugTrace | None:
        """DebugTrace ê°ì²´ ìƒì„±"""
        if not enable_debug_trace or not debug_trace_data:
            return None

        try:
            if "query_transformation" not in debug_trace_data:
                debug_trace_data["query_transformation"] = {
                    "original": message,
                    "expanded": None,
                    "final_query": message,
                }
            if "retrieved_documents" not in debug_trace_data:
                debug_trace_data["retrieved_documents"] = []

            debug_trace = DebugTrace(**debug_trace_data)
            logger.debug(
                "DebugTrace ìƒì„± ì™„ë£Œ",
                extra={"document_count": len(debug_trace.retrieved_documents)}
            )
            return debug_trace
        except Exception as e:
            logger.warning(
                "DebugTrace ìƒì„± ì‹¤íŒ¨",
                extra={"error": str(e)},
                exc_info=True
            )
            return None

    @observe(name="RAG Pipeline", capture_input=True, capture_output=True)
    async def execute(
        self, message: str, session_id: str, options: dict[str, Any] | None = None
    ) -> RAGResultDict:
        """
        ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (7ë‹¨ê³„ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜)

        Args:
            message: ì‚¬ìš©ì ì¿¼ë¦¬
            session_id: ì„¸ì…˜ ID
            options: ì¶”ê°€ ì˜µì…˜ (limit, min_score, top_n, enable_debug_trace ë“±)

        Returns:
            í‘œì¤€ ì‘ë‹µ ë”•ì…”ë„ˆë¦¬

        Raises:
            RoutingError: ë¼ìš°íŒ… ì‹¤íŒ¨ ì‹œ
            RetrievalError: ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ
            GenerationError: ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì‹œ
        """
        start_time = time.time()
        options = options or {}
        logger.info("RAG Pipeline ì‹œì‘", extra={"query": message[:50]})

        enable_debug_trace = options.get("enable_debug_trace", False)
        debug_trace_data: dict[str, Any] = {} if enable_debug_trace else {}

        use_agent = options.get("use_agent", False)
        if use_agent and self.agent_orchestrator:
            logger.info("Agent ëª¨ë“œ í™œì„±í™”", extra={"orchestrator": "AgentOrchestrator"})
            return await self._execute_agent_mode(message, session_id, start_time)

        tracker = PipelineTracker()
        tracker.start_pipeline()
        tracker.start_stage("route_query")
        route_decision = await self.route_query(message, session_id, start_time)
        tracker.end_stage("route_query")

        if not route_decision.should_continue:
            logger.info("ë¼ìš°íŒ… ê²°ê³¼: ì¦‰ì‹œ ì‘ë‹µ ë°˜í™˜ (RAG íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨)")
            if route_decision.immediate_response is None:
                logger.error("immediate_responseê°€ Noneì…ë‹ˆë‹¤. ì™„ì „í•œ ê¸°ë³¸ ì‘ë‹µ ë°˜í™˜")
                return self._create_fallback_response(message, start_time, route_decision.metadata)
            return route_decision.immediate_response

        if enable_debug_trace:
            debug_trace_data["original_query"] = message

        tracker.start_stage("prepare_context")
        prepared_context = await self.prepare_context(message, session_id)
        tracker.end_stage("prepare_context")

        if enable_debug_trace:
            debug_trace_data["query_transformation"] = {
                "original": message,
                "expanded": prepared_context.expanded_query if prepared_context.expanded_query != message else None,
                "final_query": prepared_context.expanded_query,
            }

        tracker.start_stage("retrieve_documents")
        retrieval_results, sql_search_result = await self._execute_parallel_search(
            message, prepared_context, options
        )
        tracker.end_stage("retrieve_documents")

        self._track_debug_documents(enable_debug_trace, debug_trace_data, retrieval_results.documents)
        self._update_retrieval_metrics(tracker, prepared_context, sql_search_result)

        tracker.start_stage("rerank_documents")
        rerank_results = await self.rerank_documents(
            prepared_context.expanded_query, retrieval_results.documents, options
        )
        tracker.end_stage("rerank_documents")

        if enable_debug_trace and rerank_results.reranked:
            for i, doc in enumerate(rerank_results.documents):
                if i < len(debug_trace_data["retrieved_documents"]):
                    rerank_score = doc.metadata.get("rerank_score", 0.0) if hasattr(doc, "metadata") else 0.0
                    debug_trace_data["retrieved_documents"][i]["rerank_score"] = rerank_score

        tracker.start_stage("generate_answer")
        generation_options = {**options}
        if sql_search_result and sql_search_result.used:
            generation_options["sql_context"] = sql_search_result.formatted_context
            logger.debug(
                "SQL ì»¨í…ìŠ¤íŠ¸ ì „ë‹¬",
                extra={"context_length": len(sql_search_result.formatted_context)}
            )
        generation_result = await self.generate_answer(
            message, rerank_results.documents, prepared_context.session_context, generation_options
        )
        tracker.end_stage("generate_answer")

        tracker.start_stage("self_rag_verify")
        options_with_debug = {**options}
        if enable_debug_trace:
            options_with_debug["_debug_trace_data"] = debug_trace_data
        generation_result = await self.self_rag_verify(
            message, session_id, generation_result, rerank_results.documents, options_with_debug
        )
        tracker.end_stage("self_rag_verify")

        tracker.start_stage("format_sources")
        formatted_sources = self.format_sources(rerank_results.documents, sql_search_result)
        tracker.end_stage("format_sources")

        tracker.start_stage("build_result")
        debug_trace = self._create_debug_trace(enable_debug_trace, debug_trace_data, message)
        result = self.build_result(
            answer=generation_result.answer,
            sources=formatted_sources.sources,
            tokens_used=generation_result.tokens_used,
            topic=self.extract_topic_func(message),
            processing_time=time.time() - start_time,
            search_count=retrieval_results.count,
            ranked_count=rerank_results.count,
            model_info=generation_result.model_info,
            routing_metadata=route_decision.metadata,
            debug_trace=debug_trace,
        )
        tracker.end_stage("build_result")
        tracker.end_pipeline()
        performance_metrics = tracker.get_metrics()
        tracker.log_summary()
        result["performance_metrics"] = performance_metrics
        logger.info(
            "RAG Pipeline ì™„ë£Œ",
            extra={"processing_time": result['processing_time']}
        )
        return result

    async def route_query(self, message: str, session_id: str, start_time: float) -> RouteDecision:
        """
        1ë‹¨ê³„: ì¿¼ë¦¬ ë¼ìš°íŒ… (ê·œì¹™ ê¸°ë°˜ + LLM í´ë°±)

        - ê·œì¹™ ê¸°ë°˜ ë¼ìš°í„° ìš°ì„  ì‹œë„ (YAML ê·œì¹™)
        - LLM ë¼ìš°í„° í´ë°± (ê·œì¹™ ì‹¤íŒ¨ ì‹œ)
        - direct_answer/blocked ì²˜ë¦¬

        Args:
            message: ì‚¬ìš©ì ì¿¼ë¦¬
            session_id: ì„¸ì…˜ ID
            start_time: íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹œê°„

        Returns:
            RouteDecision: ë¼ìš°íŒ… ê²°ì • (ê³„ì† ì§„í–‰ ì—¬ë¶€ + ì¦‰ì‹œ ì‘ë‹µ)

        Raises:
            RoutingError: ë¼ìš°íŒ… ì‹¤íŒ¨ ì‹œ
        """
        logger.debug("[1ë‹¨ê³„] ì¿¼ë¦¬ ë¼ìš°íŒ… ì‹œì‘")
        routing_metadata = {}
        session_context = None
        if self.session_module:
            try:
                conversation = await self.session_module.get_conversation(
                    session_id, max_exchanges=5
                )
                if conversation and isinstance(conversation, list):
                    session_context = "\n".join(
                        [
                            f"User: {ex.get('user', '')}\nAssistant: {ex.get('assistant', '')}"
                            for ex in conversation
                        ]
                    )
            except Exception as e:
                logger.warning(
                    "ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨",
                    extra={"error": str(e)},
                    exc_info=True
                )
        try:
            rule_router = RuleBasedRouter(enabled=True)
            rule_match = await rule_router.check_rules(message)
            if rule_match:
                routing_metadata = {
                    "route": rule_match.route,
                    "intent": rule_match.intent,
                    "domain": rule_match.domain,
                    "confidence": rule_match.confidence,
                    "source": "rule_based",
                    "rule_name": rule_match.rule_name,
                }
                logger.info(
                    "[ê·œì¹™ ê¸°ë°˜ ë¼ìš°í„°] ë§¤ì¹­",
                    extra={
                        "rule_name": rule_match.rule_name,
                        "route": rule_match.route,
                        "domain": rule_match.domain
                    }
                )
                if rule_match.route == "direct_answer" and rule_match.direct_answer:
                    processing_time = time.time() - start_time
                    immediate_response = {
                        "answer": rule_match.direct_answer,
                        "sources": [],
                        "tokens_used": 0,
                        "topic": self.extract_topic_func(message),
                        "processing_time": processing_time,
                        "search_count": 0,
                        "ranked_count": 0,
                        "model_info": {"provider": "rule_based", "model": "N/A"},
                        "routing_metadata": routing_metadata,
                    }

                    logger.info(
                        "[ì¦‰ì‹œ ì‘ë‹µ] ê·œì¹™ ê¸°ë°˜ ë‹µë³€ ë°˜í™˜",
                        extra={"processing_time": processing_time}
                    )
                    return RouteDecision(
                        should_continue=False,
                        immediate_response=cast(RAGResultDict, immediate_response),
                        metadata=routing_metadata,
                    )
                return RouteDecision(
                    should_continue=True, immediate_response=None, metadata=routing_metadata
                )
        except Exception as rule_error:
            logger.warning(
                "[RuleBasedRouter] ì˜¤ë¥˜",
                extra={"error": str(rule_error)},
                exc_info=True
            )
        if not self.query_router or not self.query_router.enabled:
            logger.info("[LLM ë¼ìš°í„°] ë¹„í™œì„±í™” - RAG ê³„ì† ì§„í–‰")
            return RouteDecision(
                should_continue=True, immediate_response=None, metadata=routing_metadata
            )
        try:
            profile, routing = await self.query_router.analyze_and_route(
                message, session_context=session_context
            )

            # ğŸ†• dataclass ì†ì„± ì ‘ê·¼ìœ¼ë¡œ ìˆ˜ì • (Oracle ê¶Œì¥ì‚¬í•­)
            routing_metadata.update(
                {
                    "llm_route": routing.primary_route,  # âœ… .get() â†’ ì†ì„± ì ‘ê·¼
                    "llm_intent": profile.intent.value if profile.intent else "unknown",  # âœ…
                    "llm_domain": profile.domain,  # âœ…
                    "llm_confidence": routing.confidence,  # âœ…
                    "llm_reasoning": routing.notes or "",  # âœ…
                    "data_source": getattr(profile, "data_source", "general"),  # ğŸ†• ì‹ ê·œ í•„ë“œ
                    "source": routing_metadata.get("source", "llm"),
                    "profile": profile,
                }
            )
            logger.info(
                "[LLM ë¼ìš°í„°] ë¼ìš°íŒ… ì™„ë£Œ",
                extra={
                    "route": routing.primary_route,
                    "data_source": routing_metadata['data_source'],
                    "intent": profile.intent.value if profile.intent else 'unknown',
                    "confidence": routing.confidence
                }
            )
            if routing.primary_route == "blocked":
                processing_time = time.time() - start_time
                immediate_response = {
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì€ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "sources": [],
                    "tokens_used": 0,
                    "topic": self.extract_topic_func(message),
                    "processing_time": processing_time,
                    "search_count": 0,
                    "ranked_count": 0,
                    "model_info": {"provider": "query_router", "model": "N/A"},
                    "routing_metadata": routing_metadata,
                }
                logger.warning(
                    "[ì°¨ë‹¨] ì¿¼ë¦¬ê°€ ì°¨ë‹¨ë¨",
                    extra={"reason": routing.notes}
                )
                return RouteDecision(
                    should_continue=False,
                    immediate_response=cast(RAGResultDict, immediate_response),
                    metadata=routing_metadata,
                )
        except Exception as llm_error:
            logger.warning(
                "[LLM ë¼ìš°í„°] ì˜¤ë¥˜",
                extra={"error": str(llm_error)},
                exc_info=True
            )
            routing_metadata["fallback_reason"] = str(llm_error)
        logger.info("[ë¼ìš°íŒ… ì™„ë£Œ] RAG íŒŒì´í”„ë¼ì¸ ê³„ì† ì§„í–‰")
        return RouteDecision(
            should_continue=True, immediate_response=None, metadata=routing_metadata
        )

    # NOTE: _get_score_multipliers() í•¨ìˆ˜ ì œê±°ë¨ (2026-01-02)
    # ìŠ¤ì½”ì–´ ê°€ì¤‘ì¹˜ëŠ” ScoringService(rag.yamlì˜ scoring ì„¹ì…˜)ì—ì„œ ê´€ë¦¬ë©ë‹ˆë‹¤.
    # ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ: DOMAIN_CUSTOMIZATION_GUIDE.md ì°¸ì¡°

    @observe(name="Query Expansion & Context Preparation")
    async def prepare_context(self, message: str, session_id: str) -> PreparedContext:
        """
        2ë‹¨ê³„: ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ + ì¿¼ë¦¬ í™•ì¥

        - ì„¸ì…˜ ëª¨ë“ˆì—ì„œ ìµœê·¼ 5ê°œ ëŒ€í™” ì¡°íšŒ
        - ì¿¼ë¦¬ í™•ì¥ ëª¨ë“ˆë¡œ ì¿¼ë¦¬ í™•ì¥ (ì„ íƒì )

        Args:
            message: ì›ë³¸ ì¿¼ë¦¬
            session_id: ì„¸ì…˜ ID

        Returns:
            PreparedContext: ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ + í™•ì¥ëœ ì¿¼ë¦¬
        """
        logger.debug("[3ë‹¨ê³„] ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ì‹œì‘")
        session_context = None
        if self.session_module:
            try:
                context_string = await self.session_module.get_context_string(session_id)
                if context_string:
                    session_context = context_string
                    logger.debug(
                        "ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì„±ê³µ",
                        extra={"context_length": len(context_string)}
                    )
                else:
                    logger.debug("ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë¹„ì–´ìˆìŒ")
            except Exception as e:
                logger.warning(
                    "ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨",
                    extra={"error": str(e)},
                    exc_info=True
                )
        # Multi-Query RRF: ëª¨ë“  í™•ì¥ ì¿¼ë¦¬ì™€ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
        expanded_query = message
        expanded_queries: list[str] = []
        query_weights: list[float] = []

        if self.query_expansion:
            try:
                logger.debug("ì¿¼ë¦¬ í™•ì¥ ì‹œë„")
                expansion_result = await self.query_expansion.expand_query(
                    query=message, context=session_context
                )

                if expansion_result and hasattr(expansion_result, "expansions"):
                    if expansion_result.expansions:
                        # metadataì—ì„œ raw_expanded_queries ì¶”ì¶œ (weight ì •ë³´ í¬í•¨)
                        raw_queries = expansion_result.metadata.get("raw_expanded_queries", [])

                        if raw_queries:
                            # ì›ë³¸ ë°ì´í„°ì—ì„œ ì¿¼ë¦¬ì™€ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
                            for item in raw_queries:
                                if isinstance(item, dict):
                                    query = item.get("query", "")
                                    weight = item.get("weight", 1.0)
                                    if query:
                                        expanded_queries.append(query)
                                        query_weights.append(weight)

                        # raw_queriesê°€ ì—†ìœ¼ë©´ expansionsì—ì„œ ì¶”ì¶œ (ê°€ì¤‘ì¹˜ëŠ” ë™ì¼í•˜ê²Œ)
                        if not expanded_queries:
                            expanded_queries = expansion_result.expansions
                            query_weights = [1.0] * len(expanded_queries)

                        expanded_query = expanded_queries[0]  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ (í•˜ìœ„ í˜¸í™˜ì„±)
                        logger.info(
                            "ì¿¼ë¦¬ í™•ì¥ ì„±ê³µ",
                            extra={
                                "query_count": len(expanded_queries),
                                "weights": [f'{w:.1f}' for w in query_weights],
                                "original": message[:30],
                                "expanded": expanded_query[:30]
                            }
                        )
                    else:
                        logger.debug("ì¿¼ë¦¬ í™•ì¥ ê²°ê³¼ ì—†ìŒ, ì›ë³¸ ì‚¬ìš©")
                        expanded_queries = [message]
                        query_weights = [1.0]
                else:
                    logger.debug("ì¿¼ë¦¬ í™•ì¥ ê²°ê³¼ ì—†ìŒ, ì›ë³¸ ì‚¬ìš©")
                    expanded_queries = [message]
                    query_weights = [1.0]
            except Exception as e:
                logger.warning(
                    "ì¿¼ë¦¬ í™•ì¥ ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©",
                    extra={"error": str(e)},
                    exc_info=True
                )
                expanded_queries = [message]
                query_weights = [1.0]
        else:
            # query_expansion ëª¨ë“ˆ ì—†ìŒ
            expanded_queries = [message]
            query_weights = [1.0]

        logger.debug(
            "[3ë‹¨ê³„] ì»¨í…ìŠ¤íŠ¸ ì¤€ë¹„ ì™„ë£Œ",
            extra={"expanded_query": expanded_query[:50]}
        )
        return PreparedContext(
            session_context=session_context,
            expanded_query=expanded_query,
            original_query=message,
            expanded_queries=expanded_queries,
            query_weights=query_weights,
        )

    @observe(name="Document Retrieval (Hybrid Search)")
    async def retrieve_documents(
        self,
        search_queries: list[str] | str,
        query_weights: list[float] | None,
        context: str | None,
        options: dict[str, Any],
    ) -> RetrievalResults:
        """
        3ë‹¨ê³„: MongoDB Atlas í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (Multi-Query RRF ì§€ì›)

        - Multi-Query RRF: ë‹¤ì¤‘ ì¿¼ë¦¬ë¡œ ë³‘ë ¬ ê²€ìƒ‰ í›„ RRF ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ë³‘í•©
        - Single Query: ê¸°ì¡´ ë°©ì‹ (í•˜ìœ„ í˜¸í™˜ì„±)
        - Circuit Breaker ë³´í˜¸
        - ì„±ëŠ¥ ë©”íŠ¸ë¦­ ê¸°ë¡

        Args:
            search_queries: ê²€ìƒ‰ ì¿¼ë¦¬ (ë‹¨ì¼ ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸)
            query_weights: ì¿¼ë¦¬ ê°€ì¤‘ì¹˜ ë¦¬ìŠ¤íŠ¸ (Multi-Query RRFìš©, ì„ íƒì )
            context: ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )
            options: ê²€ìƒ‰ ì˜µì…˜ (limit, min_score ë“±)

        Note:
            ìŠ¤ì½”ì–´ ê°€ì¤‘ì¹˜ëŠ” ScoringService(rag.yamlì˜ scoring ì„¹ì…˜)ì—ì„œ ìë™ ì ìš©ë©ë‹ˆë‹¤.

        Returns:
            RetrievalResults: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (RRF ë³‘í•© ì™„ë£Œ)

        Raises:
            RetrievalError: ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ
        """
        # í•˜ìœ„ í˜¸í™˜ì„±: ë‹¨ì¼ ì¿¼ë¦¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if isinstance(search_queries, str):
            search_queries = [search_queries]
            query_weights = [1.0]

        # query_weights ê¸°ë³¸ê°’
        if not query_weights:
            query_weights = [1.0] * len(search_queries)

        logger.debug(
            "[4ë‹¨ê³„] ë¬¸ì„œ ê²€ìƒ‰ ì‹œì‘",
            extra={
                "query_count": len(search_queries),
                "multi_query_rrf": "í™œì„±í™”" if len(search_queries) > 1 else "ë¹„í™œì„±í™”"
            }
        )

        # Future ê°ì²´ í•´ê²° (DI Containerì—ì„œ Futureë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŒ)
        retrieval_module = self.retrieval_module
        if asyncio.iscoroutine(retrieval_module) or isinstance(retrieval_module, asyncio.Future):
            retrieval_module = await retrieval_module

        if not retrieval_module:
            logger.error("ê²€ìƒ‰ ëª¨ë“ˆ ì—†ìŒ")
            raise RetrievalError(ErrorCode.RETRIEVAL_SEARCH_FAILED)

        cb = self.circuit_breaker_factory.get("document_retrieval")

        async def _search() -> list[SearchResult]:
            """ì‹¤ì œ ê²€ìƒ‰ ë¡œì§ (Circuit Breaker ë‚´ë¶€) - Multi-Query RRF"""
            search_options = {
                "limit": options.get("limit", self.retrieval_limit),
                "min_score": options.get("min_score", self.min_score),
                "context": context,
            }

            # Multi-Query ê²€ìƒ‰: IMultiQueryRetriever Protocol ì²´í¬
            # RetrievalOrchestrator ì§ì ‘ ì‚¬ìš© (í”„ë¡œë•ì…˜)
            if isinstance(retrieval_module, IMultiQueryRetriever):
                return await retrieval_module._search_and_merge(
                    queries=search_queries,
                    top_k=search_options["limit"],
                    filters=None,  # í•„í„° ë¯¸ì‚¬ìš© (í–¥í›„ í™•ì¥ ê°€ëŠ¥)
                    weights=query_weights,
                    use_rrf=True,  # RRF í™œì„±í™”
                )
            # í•˜ìœ„ í˜¸í™˜ì„±: orchestratorë¥¼ ì†ì„±ìœ¼ë¡œ ê°–ëŠ” ê²½ìš°
            elif hasattr(retrieval_module, "orchestrator"):
                orchestrator = retrieval_module.orchestrator
                if isinstance(orchestrator, IMultiQueryRetriever):
                    # orchestrator._search_and_merge ì§ì ‘ í˜¸ì¶œ (RRF ë³‘í•©)
                    return await orchestrator._search_and_merge(
                        queries=search_queries,
                        top_k=search_options["limit"],
                        filters=None,  # í•„í„° ë¯¸ì‚¬ìš© (í–¥í›„ í™•ì¥ ê°€ëŠ¥)
                        weights=query_weights,
                        use_rrf=True,  # RRF í™œì„±í™”
                    )

            # Fallback: ë‹¨ì¼ ì¿¼ë¦¬ ê²€ìƒ‰ (ê¸°ì¡´ ë°©ì‹)
            return cast(
                list[SearchResult], await retrieval_module.search(search_queries[0], search_options)
            )

        try:
            start_time = time.time()
            search_results = await cb.call(_search, fallback=lambda: [])
            latency_ms = (time.time() - start_time) * 1000
            self.performance_metrics.record_latency("retrieve_documents", latency_ms)
            logger.info(
                "[4ë‹¨ê³„] ê²€ìƒ‰ ì™„ë£Œ",
                extra={
                    "document_count": len(search_results),
                    "latency_ms": latency_ms,
                    "multi_query_rrf": "í™œì„±í™”" if len(search_queries) > 1 else "ë¹„í™œì„±í™”"
                }
            )
            return RetrievalResults(documents=search_results, count=len(search_results))
        except CircuitBreakerOpenError:
            logger.warning("Circuit Breaker OPEN - ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì¼ì‹œ ì°¨ë‹¨")
            return RetrievalResults(documents=[], count=0)
        except Exception as e:
            logger.error(
                "[4ë‹¨ê³„] ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨",
                extra={"error": str(e)},
                exc_info=True
            )
            raise RetrievalError(
                ErrorCode.RETRIEVAL_SEARCH_FAILED,
                queries=[q[:50] for q in search_queries],
                error=str(e),
            ) from e

    async def rerank_documents(
        self, search_query: str, search_results: list[Any], options: dict[str, Any]
    ) -> RerankResults:
        """
        4ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ë­í‚¹ (ì„ íƒì )

        - ë¦¬ë­í‚¹ ì„¤ì • í™•ì¸ (config.reranking.enabled)
        - Jina/Cohere/LLM ë¦¬ë­ì»¤ í˜¸ì¶œ
        - ì‹¤íŒ¨ ì‹œ ì›ë³¸ ë°˜í™˜

        Args:
            search_query: ê²€ìƒ‰ ì¿¼ë¦¬
            search_results: ê²€ìƒ‰ ê²°ê³¼ (Document ë¦¬ìŠ¤íŠ¸)
            options: ë¦¬ë­í‚¹ ì˜µì…˜ (top_n ë“±)

        Returns:
            RerankResults: ë¦¬ë­í‚¹ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (reranked=True/False)
        """
        logger.debug("[5ë‹¨ê³„] ë¦¬ë­í‚¹ ì‹œì‘")
        if not search_results:
            logger.debug("ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ, ë¦¬ë­í‚¹ ìŠ¤í‚µ")
            return RerankResults(documents=[], count=0, reranked=False)
        reranking_config = self.config.get("reranking", {})
        retrieval_config = self.config.get("retrieval", {})
        reranking_enabled = reranking_config.get("enabled", False) or retrieval_config.get(
            "enable_reranking", False
        )
        if not reranking_enabled:
            logger.debug("ë¦¬ë­í‚¹ ë¹„í™œì„±í™” - ì›ë³¸ ì‚¬ìš©")
            return RerankResults(
                documents=search_results, count=len(search_results), reranked=False
            )
        # Future ê°ì²´ í•´ê²°
        retrieval_module = self.retrieval_module
        if asyncio.iscoroutine(retrieval_module) or isinstance(retrieval_module, asyncio.Future):
            retrieval_module = await retrieval_module

        if not retrieval_module or not hasattr(retrieval_module, "rerank"):
            logger.warning("ë¦¬ë­í‚¹ ëª¨ë“ˆ ì—†ìŒ - ì›ë³¸ ì‚¬ìš©")
            return RerankResults(
                documents=search_results, count=len(search_results), reranked=False
            )
        try:
            logger.debug(
                "ë¦¬ë­í‚¹ ì‹¤í–‰",
                extra={"document_count": len(search_results)}
            )
            ranked_results = await retrieval_module.rerank(
                query=search_query,
                results=search_results,
                top_n=options.get("top_n", self.rerank_top_n),
            )
            # ë¦¬ë­í‚¹ í›„ min_score í•„í„°ë§
            min_score = reranking_config.get("min_score", 0.05)
            if min_score > 0:
                before_count = len(ranked_results)
                ranked_results = [
                    doc
                    for doc in ranked_results
                    if (hasattr(doc, "score") and doc.score >= min_score)
                    or (hasattr(doc, "metadata") and doc.metadata.get("score", 0) >= min_score)
                ]
                if before_count > len(ranked_results):
                    logger.info(
                        "min_score í•„í„°ë§",
                        extra={
                            "before_count": before_count,
                            "after_count": len(ranked_results),
                            "threshold": min_score
                        }
                    )
            logger.info(
                "[5ë‹¨ê³„] ë¦¬ë­í‚¹ ì™„ë£Œ",
                extra={"document_count": len(ranked_results)}
            )
            return RerankResults(documents=ranked_results, count=len(ranked_results), reranked=True)
        except Exception as e:
            logger.warning(
                "[5ë‹¨ê³„] ë¦¬ë­í‚¹ ì‹¤íŒ¨, ì›ë³¸ ì‚¬ìš©",
                extra={"error": str(e)},
                exc_info=True
            )
            return RerankResults(
                documents=search_results, count=len(search_results), reranked=False
            )

    @observe(name="Answer Generation (LLM)")
    async def generate_answer(
        self, message: str, ranked_results: list[Any], context: str | None, options: dict[str, Any]
    ) -> GenerationResult:
        """
        5ë‹¨ê³„: LLM ë‹µë³€ ìƒì„±

        - LLM ë‹µë³€ ìƒì„± (Gemini/OpenAI/Claude)
        - Circuit Breaker ë³´í˜¸
        - Fallback ë‹µë³€ ì²˜ë¦¬ (LLM ì‹¤íŒ¨ ì‹œ)
        - ë¹„ìš© ì¶”ì  (CostTracker)

        Args:
            message: ì‚¬ìš©ì ì§ˆë¬¸
            ranked_results: ë¦¬ë­í‚¹ëœ ë¬¸ì„œ
            context: ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸
            options: ìƒì„± ì˜µì…˜

        Returns:
            GenerationResult: ë‹µë³€ + í† í° ìˆ˜ + ëª¨ë¸ ì •ë³´

        Raises:
            GenerationError: ë‹µë³€ ìƒì„± ì‹¤íŒ¨ ì‹œ
        """
        logger.debug("[6ë‹¨ê³„] ë‹µë³€ ìƒì„± ì‹œì‘")
        if not self.generation_module:
            logger.error("ìƒì„± ëª¨ë“ˆ ì—†ìŒ")
            return GenerationResult(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                text="ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                tokens_used=0,
                model_used="none",
                provider="none",
                generation_time=0.0,
            )
        cb = self.circuit_breaker_factory.get("answer_generation")
        safe_docs = []
        dropped_count = 0
        for doc in ranked_results or []:
            if validate_document(doc):
                safe_docs.append(doc)
            else:
                dropped_count += 1
                logger.warning(
                    "ë¬¸ì„œ ì¸ì ì…˜ íŒ¨í„´ ê°ì§€ - ì°¨ë‹¨",
                    extra={"total_dropped": dropped_count}
                )
        if dropped_count > 0:
            logger.info(
                "ì•ˆì „ ë¬¸ì„œ í•„í„°ë§ ì™„ë£Œ",
                extra={"safe_count": len(safe_docs), "dropped_count": dropped_count}
            )
        context_documents = safe_docs

        async def _generate() -> GenerationResult:
            """ì‹¤ì œ ë‹µë³€ ìƒì„± ë¡œì§ (Circuit Breaker ë‚´ë¶€)"""
            # session_contextë¥¼ optionsì— í¬í•¨ì‹œì¼œ ì „ë‹¬
            generation_options = {**options, "session_context": context}
            return cast(
                GenerationResult,
                await self.generation_module.generate_answer(
                    query=message, context_documents=context_documents, options=generation_options
                ),
            )

        def _fallback() -> dict[str, Any]:
            """LLM ì‹¤íŒ¨ ì‹œ Fallback ë‹µë³€"""
            if context_documents:
                top_doc = context_documents[0]
                content = getattr(top_doc, "page_content", str(top_doc))[:300]
                return {
                    "answer": f"ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n{content}...\n\n(í˜„ì¬ AI ì„œë¹„ìŠ¤ ì¼ì‹œ ì¥ì• ë¡œ ìƒì„¸ ë‹µë³€ì´ ì–´ë µìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.)",
                    "tokens_used": 0,
                    "model_info": {"provider": "fallback", "model": "document_summary"},
                }
            else:
                return {
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©°, í˜„ì¬ AI ì„œë¹„ìŠ¤ë„ ì¼ì‹œì ìœ¼ë¡œ ì´ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ ì£¼ì‹œê² ì–´ìš”?",
                    "tokens_used": 0,
                    "model_info": {"provider": "fallback", "model": "none"},
                }

        try:
            start_time = time.time()
            generation_result: GenerationResult | dict[str, Any] = await cb.call(
                _generate, fallback=_fallback
            )
            latency_ms = (time.time() - start_time) * 1000
            self.performance_metrics.record_latency("generate_integrated_answer", latency_ms)

            # íƒ€ì… ê°€ë“œ: GenerationResult ë˜ëŠ” dict ì²˜ë¦¬
            # GenerationResult ê°ì²´ì¸ì§€ í™•ì¸ (hasattrë¡œë„ ì²´í¬í•˜ì—¬ ë” ì•ˆì „í•˜ê²Œ)
            if isinstance(generation_result, GenerationResult):
                tokens = generation_result.tokens_used
                provider = generation_result.provider
                answer = generation_result.answer
                model_info = generation_result.model_info
            elif isinstance(generation_result, dict):
                # fallbackì´ dictë¥¼ ë°˜í™˜í•œ ê²½ìš° (Circuit Breaker ë‚´ë¶€ fallback)
                tokens = generation_result.get("tokens_used", 0)
                provider = generation_result.get("model_info", {}).get("provider", "google")
                answer = generation_result.get("answer", "ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                model_info = generation_result.get("model_info", {})
            else:
                # ì˜ˆìƒì¹˜ ëª»í•œ íƒ€ì… (ì•ˆì „ ì¥ì¹˜)
                logger.error(f"âš ï¸ ì˜ˆìƒì¹˜ ëª»í•œ generation_result íƒ€ì…: {type(generation_result)}")
                tokens = 0
                provider = "unknown"
                answer = "ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                model_info = {"provider": "error", "model": "unknown"}

            if tokens > 0 and provider in ["google", "openai", "anthropic"]:
                self.cost_tracker.track_usage(provider, tokens, is_input=False)

            if contains_output_leakage(answer):
                logger.error(
                    "í”„ë¡¬í”„íŠ¸ ëˆ„ì¶œ ê°ì§€ - ë‹µë³€ ì°¨ë‹¨",
                    extra={"preview": answer[:100]}
                )
                answer = "ë³´ì•ˆ ì •ì±…ì— ë”°ë¼ ë‚´ë¶€ ì§€ì‹œì‚¬í•­ì€ ê³µê°œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì´ í•„ìš”í•œ ë‚´ìš©ì„ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                self.performance_metrics.record_error("prompt_leakage_blocked")

            logger.info(
                "[6ë‹¨ê³„] ë‹µë³€ ìƒì„± ì™„ë£Œ",
                extra={
                    "answer_length": len(answer),
                    "latency_ms": latency_ms,
                    "tokens": tokens
                }
            )
            return GenerationResult(
                answer=answer,
                text=answer,
                tokens_used=tokens,
                model_used=model_info.get("model", "unknown"),
                provider=model_info.get("provider", "unknown"),
                generation_time=latency_ms / 1000,
            )
        except CircuitBreakerOpenError:
            # Circuit Breaker ì—ëŸ¬ â†’ ì¼ì‹œì  ì¥ì• , Fallback ì‚¬ìš©
            logger.warning("ğŸš« Circuit Breaker OPEN - LLM ì„œë¹„ìŠ¤ ì¼ì‹œ ì°¨ë‹¨, Fallback ì‚¬ìš©")
            fallback_result = _fallback()
            return GenerationResult(
                answer=fallback_result["answer"],
                text=fallback_result["answer"],
                tokens_used=fallback_result["tokens_used"],
                model_used=fallback_result["model_info"].get("model", "fallback"),
                provider=fallback_result["model_info"].get("provider", "fallback"),
                generation_time=0.0,
            )
        except TimeoutError as e:
            # íƒ€ì„ì•„ì›ƒ ì—ëŸ¬ â†’ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ì¬ì‹œë„ ìœ ë„
            logger.error(
                "[6ë‹¨ê³„] ë‹µë³€ ìƒì„± íƒ€ì„ì•„ì›ƒ",
                extra={"error": str(e)},
                exc_info=True
            )
            raise GenerationError(
                ErrorCode.GENERATION_TIMEOUT,
                session_id=options.get("session_id", "unknown"),
                timeout_seconds=30,
            ) from e
        except ValueError as e:
            # ì…ë ¥ ê²€ì¦ ì—ëŸ¬ â†’ í´ë¼ì´ì–¸íŠ¸ ì—ëŸ¬
            logger.error(
                "[6ë‹¨ê³„] ì˜ëª»ëœ ì…ë ¥",
                extra={"error": str(e)},
                exc_info=True
            )
            raise GenerationError(
                ErrorCode.GENERATION_INVALID_RESPONSE,
                session_id=options.get("session_id", "unknown"),
                error=str(e),
            ) from e
        except Exception as e:
            # ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ â†’ ì„œë²„ ì—ëŸ¬
            logger.error(
                "[6ë‹¨ê³„] ë‹µë³€ ìƒì„± ì‹¤íŒ¨",
                extra={"error": str(e)},
                exc_info=True
            )
            raise GenerationError(
                ErrorCode.GENERATION_REQUEST_FAILED,
                session_id=options.get("session_id", "unknown"),
                error=str(e),
            ) from e

    @observe(name="Self-RAG Quality Verification")
    async def self_rag_verify(
        self,
        message: str,
        session_id: str,
        generation_result: GenerationResult,
        documents: list[Any],
        options: dict[str, Any],
    ) -> GenerationResult:
        """
        6ë‹¨ê³„: Self-RAG í’ˆì§ˆ ê²€ì¦ (ì„ íƒì )

        RAGPipelineì´ ì´ë¯¸ ìƒì„±í•œ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ê³ , í•„ìš”ì‹œì—ë§Œ ì¬ìƒì„±í•©ë‹ˆë‹¤.
        ê¸°ì¡´ ê²€ìƒ‰/ìƒì„± ê²°ê³¼ë¥¼ ì¬í™œìš©í•˜ì—¬ ì¤‘ë³µì„ ìµœì†Œí™”í•©ë‹ˆë‹¤.

        ì›Œí¬í”Œë¡œìš°:
        1. ë³µì¡ë„ ê³„ì‚° (ë‚®ìœ¼ë©´ í’ˆì§ˆ ê²€ì¦ ìŠ¤í‚µ)
        2. ê¸°ì¡´ ë‹µë³€ í’ˆì§ˆ í‰ê°€ (ì¬ê²€ìƒ‰/ì¬ìƒì„± ì—†ì´ í‰ê°€ë§Œ!)
        3. í’ˆì§ˆ >= 0.8 â†’ ê¸°ì¡´ ë‹µë³€ ê·¸ëŒ€ë¡œ ì‚¬ìš© âœ…
        4. í’ˆì§ˆ < 0.8 â†’ ì¬ê²€ìƒ‰(15ê°œ) + ì¬ìƒì„± + Rollback íŒë‹¨

        Args:
            message: ì‚¬ìš©ì ì§ˆë¬¸
            session_id: ì„¸ì…˜ ID
            generation_result: RAGPipelineì´ ìƒì„±í•œ ì´ˆê¸° ë‹µë³€
            documents: RAGPipelineì´ ê²€ìƒ‰í•œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            options: ì¶”ê°€ ì˜µì…˜

        Returns:
            GenerationResult: ìµœì¢… ë‹µë³€ (ê¸°ì¡´ ë‹µë³€ ë˜ëŠ” ì¬ìƒì„± ë‹µë³€)
        """
        logger.debug("[6ë‹¨ê³„] Self-RAG í’ˆì§ˆ ê²€ì¦ ì‹œì‘")

        # â­ ë””ë²„ê¹… ì¶”ì  ë°ì´í„° ì¶”ì¶œ
        debug_trace_data = options.get("_debug_trace_data")

        # Self-RAG ë¹„í™œì„±í™” í™•ì¸
        self_rag_config = self.config.get("self_rag", {})
        if not self_rag_config.get("enabled", False):
            logger.debug("Self-RAG ë¹„í™œì„±í™” - ê¸°ì¡´ ë‹µë³€ ì‚¬ìš©")
            return generation_result

        # Future ê°ì²´ í•´ê²°
        self_rag_module = self.self_rag_module
        if self_rag_module:
            if asyncio.iscoroutine(self_rag_module) or isinstance(self_rag_module, asyncio.Future):
                self_rag_module = await self_rag_module

        # Self-RAG ëª¨ë“ˆ ì—†ìŒ
        if not self_rag_module:
            logger.debug("Self-RAG ëª¨ë“ˆ ì—†ìŒ - ê¸°ì¡´ ë‹µë³€ ì‚¬ìš©")
            return generation_result

        try:
            logger.info("Self-RAG í’ˆì§ˆ ê²€ì¦ ì‹œì‘ (ê¸°ì¡´ ë‹µë³€ ì¬í™œìš© ëª¨ë“œ)")

            # âœ… ìµœì í™”: verify_existing_answer ë©”ì„œë“œ ì‚¬ìš© (ì¤‘ë³µ ì œê±°)
            # RAGPipelineì´ ì´ë¯¸ ìƒì„±í•œ ë‹µë³€ê³¼ ë¬¸ì„œë¥¼ ì „ë‹¬
            self_rag_result = await self_rag_module.verify_existing_answer(
                query=message,
                existing_answer=generation_result.answer,  # âœ… ê¸°ì¡´ ë‹µë³€ ì „ë‹¬
                existing_docs=documents,  # âœ… ê¸°ì¡´ ë¬¸ì„œ ì „ë‹¬
                session_id=session_id,
            )

            # Self-RAGê°€ ì ìš©ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if self_rag_result.used_self_rag:
                logger.info(
                    "Self-RAG ê²€ì¦ ì™„ë£Œ",
                    extra={
                        "complexity": self_rag_result.complexity.score,
                        "regenerated": self_rag_result.regenerated
                    }
                )

                # â­ Self-RAG í‰ê°€ ì¶”ì 
                if debug_trace_data is not None:
                    debug_trace_data["self_rag_evaluation"] = {
                        "initial_quality": self_rag_result.initial_quality.overall if self_rag_result.initial_quality else 0.0,
                        "regenerated": self_rag_result.regenerated,
                        "final_quality": self_rag_result.final_quality.overall if self_rag_result.final_quality else 0.0,
                        "reason": self_rag_result.initial_quality.reasoning if self_rag_result.initial_quality else None,
                    }

                # â­ í’ˆì§ˆ ê²Œì´íŠ¸ ì ìš©
                min_quality = self_rag_config.get("min_quality_to_answer", 0.6)
                final_quality_score = (
                    self_rag_result.final_quality.overall
                    if self_rag_result.final_quality
                    else 0.0
                )

                if final_quality_score < min_quality:
                    logger.warning(
                        "ì €í’ˆì§ˆ ë‹µë³€ ê°ì§€ - ë‹µë³€ ê±°ë¶€",
                        extra={
                            "score": final_quality_score,
                            "threshold": min_quality
                        }
                    )

                    # ê±°ë¶€ ë©”ì‹œì§€ ë°˜í™˜
                    return GenerationResult(
                        answer="ì£„ì†¡í•©ë‹ˆë‹¤. í™•ì‹¤í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•´ì£¼ì‹œê² ì–´ìš”?",
                        text="ì£„ì†¡í•©ë‹ˆë‹¤. í™•ì‹¤í•œ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.",
                        tokens_used=generation_result.tokens_used,
                        model_used=generation_result.model_used,
                        provider=generation_result.provider,
                        generation_time=generation_result.generation_time,
                        refusal_reason="quality_too_low",  # â­ ì‹ ê·œ í•„ë“œ
                        quality_score=final_quality_score,  # â­ ì‹ ê·œ í•„ë“œ
                    )

                # í’ˆì§ˆ ì ìˆ˜ ë¡œê¹… ë° Langfuse Score ê¸°ë¡
                if self_rag_result.initial_quality:
                    initial_q = self_rag_result.initial_quality.overall
                    logger.info("ì´ˆê¸° í’ˆì§ˆ", extra={"score": initial_q})

                    # Langfuse Score ê¸°ë¡: ì´ˆê¸° í’ˆì§ˆ
                    try:
                        langfuse_context.score_current_trace(
                            name="self_rag_initial_quality",
                            value=initial_q,
                            comment=f"Self-RAG ì´ˆê¸° ë‹µë³€ í’ˆì§ˆ (complexity: {self_rag_result.complexity.score:.2f})",
                        )
                    except Exception as e:
                        logger.debug(f"Langfuse Score ê¸°ë¡ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

                    if self_rag_result.regenerated and self_rag_result.final_quality:
                        final_q = self_rag_result.final_quality.overall
                        improvement = final_q - initial_q
                        logger.info(
                            "í’ˆì§ˆ ë¹„êµ",
                            extra={
                                "initial": initial_q,
                                "final": final_q,
                                "improvement": improvement
                            }
                        )

                        # Langfuse Score ê¸°ë¡: ìµœì¢… í’ˆì§ˆ ë° ê°œì„ ë„
                        try:
                            langfuse_context.score_current_trace(
                                name="self_rag_final_quality",
                                value=final_q,
                                comment=f"Self-RAG ì¬ìƒì„± í›„ í’ˆì§ˆ (improvement: {improvement:+.2f})",
                            )
                            langfuse_context.score_current_trace(
                                name="self_rag_improvement",
                                value=improvement,
                                comment="Self-RAG í’ˆì§ˆ ê°œì„ ë„ (final - initial)",
                            )
                        except Exception as e:
                            logger.debug(f"Langfuse Score ê¸°ë¡ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

                # Self-RAG ë‹µë³€ ì¶œë ¥ ëˆ„ì¶œ ê²€ì‚¬
                answer = self_rag_result.answer
                if contains_output_leakage(answer):
                    logger.error(
                        "í”„ë¡¬í”„íŠ¸ ëˆ„ì¶œ ê°ì§€ (Self-RAG) - ë‹µë³€ ì°¨ë‹¨",
                        extra={"preview": answer[:100]}
                    )
                    answer = "ë³´ì•ˆ ì •ì±…ì— ë”°ë¼ ë‚´ë¶€ ì§€ì‹œì‚¬í•­ì€ ê³µê°œë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ì´ í•„ìš”í•œ ë‚´ìš©ì„ ë‹¤ì‹œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
                    self.performance_metrics.record_error("prompt_leakage_blocked")

                # Self-RAG ë‹µë³€ìœ¼ë¡œ êµì²´ (ì¬ìƒì„±ëë“  ì•ˆ ëë“ )
                return GenerationResult(
                    answer=answer,
                    text=answer,
                    tokens_used=(
                        self_rag_result.tokens_used
                        if self_rag_result.regenerated
                        else generation_result.tokens_used
                    ),
                    model_used=generation_result.model_used,
                    provider=generation_result.provider,
                    generation_time=generation_result.generation_time,
                    model_config=generation_result.model_config,
                    quality_score=final_quality_score,  # â­ ì‹ ê·œ í•„ë“œ
                    _model_info_override={
                        **generation_result.model_info,
                        "self_rag_applied": True,
                        "self_rag_regenerated": self_rag_result.regenerated,
                        "complexity_score": self_rag_result.complexity.score,
                        "initial_quality": (
                            self_rag_result.initial_quality.overall
                            if self_rag_result.initial_quality
                            else None
                        ),
                        "final_quality": (
                            self_rag_result.final_quality.overall
                            if self_rag_result.final_quality
                            else None
                        ),
                    },
                )
            else:
                logger.info(
                    "Self-RAG ë¯¸ì ìš© (ë³µì¡ë„ ë‚®ìŒ) - ê¸°ì¡´ ë‹µë³€ ì‚¬ìš©",
                    extra={"complexity": self_rag_result.complexity.score}
                )
                # Self-RAG ë¯¸ì ìš© ì‹œì—ë„ ë©”íƒ€ë°ì´í„° ì¶”ê°€ (API ì‘ë‹µ ì™„ì „ì„± ë³´ì¥)
                return GenerationResult(
                    answer=generation_result.answer,
                    text=generation_result.text,
                    tokens_used=generation_result.tokens_used,
                    model_used=generation_result.model_used,
                    provider=generation_result.provider,
                    generation_time=generation_result.generation_time,
                    model_config=generation_result.model_config,
                    _model_info_override={
                        **generation_result.model_info,
                        "self_rag_applied": False,
                        "complexity_score": self_rag_result.complexity.score,
                    },
                )

        except Exception as e:
            logger.warning(
                "[6ë‹¨ê³„] Self-RAG ê²€ì¦ ì‹¤íŒ¨, ê¸°ì¡´ ë‹µë³€ ì‚¬ìš©",
                extra={"error": str(e)},
                exc_info=True
            )
            return generation_result

    def _format_rag_source(self, idx: int, doc: Any) -> dict[str, Any] | None:
        """RAG ê²€ìƒ‰ ê²°ê³¼ë¥¼ Source ê°ì²´ë¡œ ë³€í™˜"""
        try:
            metadata = getattr(doc, "metadata", {})
            document_name = (
                metadata.get("source_file")
                or metadata.get("source")
                or f"Document {idx + 1}"
            )

            if self.privacy_masker:
                document_name = self.privacy_masker.mask_filename(document_name)

            content_text = getattr(doc, "content", None) or getattr(doc, "page_content", "")
            if content_text and self.privacy_masker:
                content_text = self.privacy_masker.mask_text(content_text)
            content_preview = content_text[:200] if content_text else ""

            raw_score = getattr(doc, "score", 0.0)
            normalized_score = self.score_normalizer.normalize(raw_score)

            source_data = {
                "id": idx,
                "document": document_name,
                "page": metadata.get("page"),
                "chunk": metadata.get("chunk"),
                "relevance": normalized_score,
                "content_preview": content_preview,
                "source_type": "rag",
            }

            if metadata:
                file_path = metadata.get("file_path")
                if file_path and self.privacy_masker:
                    dir_path = os.path.dirname(file_path)
                    file_name = os.path.basename(file_path)
                    masked_name = self.privacy_masker.mask_filename(file_name)
                    file_path = os.path.join(dir_path, masked_name) if dir_path else masked_name

                source_data.update(
                    {
                        "file_type": metadata.get("file_type"),
                        "file_path": file_path,
                        "file_size": metadata.get("file_size"),
                        "total_chunks": metadata.get("total_chunks"),
                        "sheet_name": metadata.get("sheet_name"),
                    }
                )

            return source_data
        except Exception as e:
            logger.warning(
                "ì†ŒìŠ¤ í¬ë§·íŒ… ì‹¤íŒ¨",
                extra={"source_idx": idx, "error": str(e)},
                exc_info=True
            )
            return None

    def _format_sql_row(
        self,
        row: dict[str, Any],
        row_idx: int,
        source_id: int,
        sql_query: str | None,
        category: str | None = None,
    ) -> dict[str, Any]:
        """SQL ê²€ìƒ‰ ê²°ê³¼ì˜ í•œ í–‰ì„ Source ê°ì²´ë¡œ ë³€í™˜"""
        shop_name = row.get("shop_name") or row.get("ì—…ì²´ëª…") or f"ê²°ê³¼ {row_idx + 1}"
        row_preview = ", ".join(f"{k}: {v}" for k, v in row.items() if v is not None)

        if row_preview and self.privacy_masker:
            row_preview = self.privacy_masker.mask_text(row_preview)

        document_name = f"[{category}] {shop_name}" if category else str(shop_name)

        return {
            "id": source_id,
            "document": document_name,
            "page": None,
            "chunk": None,
            "relevance": 100.0,
            "content_preview": row_preview[:200] if row_preview else "SQL ì¿¼ë¦¬ ê²°ê³¼",
            "source_type": "sql",
            "sql_query": sql_query,
            "sql_result_summary": row_preview,
        }

    def _add_multi_query_sql_sources(
        self, sources: list[Any], sql_search_result: SQLSearchResult, max_sources: int
    ) -> int:
        """ë©€í‹° ì¿¼ë¦¬ SQL ê²°ê³¼ë¥¼ sourcesì— ì¶”ê°€"""
        added_count = 0
        for query_result in sql_search_result.query_results:
            if not query_result.success or not query_result.result:
                continue

            sql_result = query_result.result
            sql_query = query_result.query.sql_query
            category = query_result.query.target_category or "ì „ì²´"

            for row_idx, row in enumerate(sql_result.data):
                if added_count >= max_sources:
                    break

                sql_source_data = self._format_sql_row(
                    row, row_idx, len(sources), sql_query, category
                )
                sources.append(self.Source(**sql_source_data))
                added_count += 1

        logger.info(
            "ë©€í‹° SQL ì†ŒìŠ¤ ì¶”ê°€",
            extra={
                "row_count": added_count,
                "query_count": len(sql_search_result.query_results)
            }
        )
        return added_count

    def _add_single_query_sql_sources(
        self, sources: list[Any], sql_search_result: SQLSearchResult, max_sources: int
    ) -> int:
        """ë‹¨ì¼ ì¿¼ë¦¬ SQL ê²°ê³¼ë¥¼ sourcesì— ì¶”ê°€"""
        sql_result = sql_search_result.query_result
        sql_gen = sql_search_result.generation_result
        sql_query = sql_gen.sql_query if sql_gen else None
        added_count = 0

        for row_idx, row in enumerate(sql_result.data[:max_sources]):
            sql_source_data = self._format_sql_row(row, row_idx, len(sources), sql_query)
            sources.append(self.Source(**sql_source_data))
            added_count += 1

        logger.info(
            "SQL ì†ŒìŠ¤ ì¶”ê°€",
            extra={
                "added_count": added_count,
                "total_rows": sql_result.row_count,
                "query": sql_query[:50] if sql_query else 'N/A'
            }
        )
        return added_count

    def format_sources(
        self,
        ranked_results: list[Any],
        sql_search_result: SQLSearchResult | None = None,
    ) -> FormattedSources:
        """
        6ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼ â†’ Source ê°ì²´ ë³€í™˜

        - RAG ë¬¸ì„œ â†’ Source ê°ì²´ ë³€í™˜ (source_type="rag")
        - SQL ê²€ìƒ‰ ê²°ê³¼ â†’ Source ê°ì²´ ë³€í™˜ (source_type="sql")
        - ë©”íƒ€ë°ì´í„° ì •ê·œí™” (file_type, relevance ë“±)

        Args:
            ranked_results: ë¦¬ë­í‚¹ëœ ë¬¸ì„œ (RRF ë³‘í•© ê²°ê³¼)
            sql_search_result: SQL ê²€ìƒ‰ ê²°ê³¼ (ì„ íƒì )

        Returns:
            FormattedSources: Source ê°ì²´ ë¦¬ìŠ¤íŠ¸ (RAG + SQL í†µí•©)
        """
        logger.debug("[6ë‹¨ê³„] ì†ŒìŠ¤ í¬ë§·íŒ… ì‹œì‘")
        sources = []

        try:
            for idx, doc in enumerate(ranked_results):
                source_data = self._format_rag_source(idx, doc)
                if source_data:
                    sources.append(self.Source(**source_data))

            if sql_search_result and sql_search_result.used:
                try:
                    max_sql_sources = 10
                    if sql_search_result.is_multi_query and sql_search_result.query_results:
                        self._add_multi_query_sql_sources(sources, sql_search_result, max_sql_sources)
                    elif sql_search_result.query_result:
                        self._add_single_query_sql_sources(sources, sql_search_result, max_sql_sources)
                except Exception as sql_err:
                    logger.warning(
                        "SQL ì†ŒìŠ¤ í¬ë§·íŒ… ì‹¤íŒ¨ (ë¬´ì‹œ)",
                        extra={"error": str(sql_err)},
                        exc_info=True
                    )

            logger.debug(
                "[6ë‹¨ê³„] ì†ŒìŠ¤ í¬ë§·íŒ… ì™„ë£Œ",
                extra={"source_count": len(sources), "type": "RAG + SQL"}
            )
            return FormattedSources(sources=sources, count=len(sources))
        except Exception as e:
            logger.error(
                "[6ë‹¨ê³„] ì†ŒìŠ¤ ë¦¬ìŠ¤íŠ¸ í¬ë§·íŒ… ì‹¤íŒ¨",
                extra={"error": str(e)},
                exc_info=True
            )
            return FormattedSources(sources=[], count=0)

    def build_result(
        self,
        answer: str,
        sources: list[Any],
        tokens_used: int,
        topic: str,
        processing_time: float,
        search_count: int,
        ranked_count: int,
        model_info: dict[str, Any],
        routing_metadata: dict[str, Any] | None,
        debug_trace: DebugTrace | None = None,  # â­ ì‹ ê·œ íŒŒë¼ë¯¸í„°
    ) -> RAGResultDict:
        """
        7ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ë”•ì…”ë„ˆë¦¬ êµ¬ì„±

        - í‘œì¤€ ì‘ë‹µ í˜•ì‹ ìƒì„±
        - ë¼ìš°íŒ… ë©”íƒ€ë°ì´í„° í¬í•¨ (ì„ íƒì )
        - ë””ë²„ê¹… ì¶”ì  ì •ë³´ í¬í•¨ (ì„ íƒì )

        Args:
            answer: ìƒì„±ëœ ë‹µë³€
            sources: Source ê°ì²´ ë¦¬ìŠ¤íŠ¸
            tokens_used: ì‚¬ìš©ëœ í† í° ìˆ˜
            topic: ì¶”ì¶œëœ í† í”½
            processing_time: ì´ ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)
            search_count: ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜
            ranked_count: ë¦¬ë­í‚¹ëœ ë¬¸ì„œ ìˆ˜
            model_info: ëª¨ë¸ ì •ë³´
            routing_metadata: ë¼ìš°íŒ… ë©”íƒ€ë°ì´í„°
            debug_trace: ë””ë²„ê¹… ì¶”ì  ì •ë³´ (enable_debug_trace=True ì‹œ)

        Returns:
            í‘œì¤€ ì‘ë‹µ ë”•ì…”ë„ˆë¦¬

        Note:
            ì´ ë©”ì„œë“œëŠ” ë™ê¸° í•¨ìˆ˜ (async ë¶ˆí•„ìš”)
        """
        logger.debug("[8ë‹¨ê³„] ê²°ê³¼ êµ¬ì„± ì‹œì‘")

        # model_info í‘œì¤€í™” (API ì‘ë‹µ ì¼ê´€ì„± ë³´ì¥)
        if model_info:
            # í•„ìˆ˜ í•„ë“œ ë³´ì¥ + í•˜ìœ„ í˜¸í™˜ì„±
            standardized_model_info = {
                "provider": model_info.get("provider", "unknown"),
                "model": model_info.get("model", "unknown"),
                "model_used": model_info.get("model", model_info.get("model_used", "unknown")),
                "self_rag_applied": model_info.get("self_rag_applied", False),
            }

            # ì„ íƒì  í•„ë“œ (ì¡´ì¬í•˜ëŠ” ê²½ìš°ë§Œ ì¶”ê°€)
            optional_fields = [
                "complexity_score",
                "initial_quality",
                "final_quality",
                "self_rag_regenerated",
            ]
            for field in optional_fields:
                if field in model_info and model_info[field] is not None:
                    standardized_model_info[field] = model_info[field]
        else:
            # model_infoê°€ ì—†ëŠ” ê²½ìš° ì•ˆì „í•œ ê¸°ë³¸ê°’ (ë°©ì–´ì  í”„ë¡œê·¸ë˜ë°)
            logger.warning("model_infoê°€ None - ê¸°ë³¸ê°’ ì‚¬ìš©")
            standardized_model_info = {
                "provider": "unknown",
                "model": "unknown",
                "model_used": "unknown",
                "self_rag_applied": False,
            }

        # PII ë§ˆìŠ¤í‚¹: ìµœì¢… ë‹µë³€ì—ì„œ ê°œì¸ì •ë³´ ë§ˆìŠ¤í‚¹ (í™œì„±í™” ì‹œì—ë§Œ)
        masked_answer = answer
        if self.privacy_masker:
            masked_answer = self.privacy_masker.mask_text(answer)

        result = {
            "answer": masked_answer,
            "sources": sources,
            "tokens_used": tokens_used,
            "topic": topic,
            "processing_time": processing_time,
            "search_results": search_count,
            "ranked_results": ranked_count,
            "model_info": standardized_model_info,
        }

        if routing_metadata:
            result["routing_metadata"] = routing_metadata

        # â­ ë””ë²„ê¹… ì¶”ì  ì •ë³´ ì¶”ê°€
        if debug_trace is not None:
            result["debug_trace"] = debug_trace

        logger.debug(
            "[8ë‹¨ê³„] ê²°ê³¼ êµ¬ì„± ì™„ë£Œ",
            extra={
                "search_count": search_count,
                "ranked_count": ranked_count
            }
        )
        return cast(RAGResultDict, result)

    async def _execute_sql_search(self, query: str) -> SQLSearchResult | None:
        """
        SQL ê²€ìƒ‰ ì‹¤í–‰ (ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ)

        RAG ê²€ìƒ‰ê³¼ ë³‘ë ¬ë¡œ ì‹¤í–‰ë˜ë©°, ì‹¤íŒ¨í•´ë„ íŒŒì´í”„ë¼ì¸ì€ ê³„ì† ì§„í–‰ë©ë‹ˆë‹¤.
        íƒ€ì„ì•„ì›ƒê³¼ ì—ëŸ¬ í•¸ë“¤ë§ì´ ì ìš©ë©ë‹ˆë‹¤.

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸

        Returns:
            SQLSearchResult | None: SQL ê²€ìƒ‰ ê²°ê³¼ ë˜ëŠ” None (ì‹¤íŒ¨/ë¹„í™œì„±í™” ì‹œ)
        """
        if not self.sql_search_service:
            return None

        try:
            # SQL ê²€ìƒ‰ ì„¤ì •ì—ì„œ íƒ€ì„ì•„ì›ƒ ì¡°íšŒ
            sql_config = self.config.get("sql_search", {}).get("pipeline", {})
            timeout = sql_config.get("timeout", 8)  # ê¸°ë³¸ 8ì´ˆ

            # íƒ€ì„ì•„ì›ƒ ì ìš©
            result = await asyncio.wait_for(self.sql_search_service.search(query), timeout=timeout)

            return result

        except TimeoutError:
            logger.warning(
                "SQL ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ",
                extra={"timeout_seconds": timeout}
            )
            return SQLSearchResult(
                success=False,
                generation_result=None,
                query_result=None,
                formatted_context="",
                total_time=timeout,
                used=False,
                error="SQL ê²€ìƒ‰ íƒ€ì„ì•„ì›ƒ",
            )
        except Exception as e:
            logger.warning(
                "SQL ê²€ìƒ‰ ì‹¤íŒ¨",
                extra={"error": str(e)},
                exc_info=True
            )
            return SQLSearchResult(
                success=False,
                generation_result=None,
                query_result=None,
                formatted_context="",
                total_time=0,
                used=False,
                error=str(e),
            )

    async def _execute_agent_mode(
        self, message: str, session_id: str, start_time: float
    ) -> RAGResultDict:
        """
        Agent ëª¨ë“œ ì‹¤í–‰ (Agentic RAG)

        AgentOrchestratorë¥¼ ì‚¬ìš©í•˜ì—¬ ReAct íŒ¨í„´ ê¸°ë°˜ ì—ì´ì „íŠ¸ ë£¨í”„ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
        ê¸°ì¡´ 7ë‹¨ê³„ íŒŒì´í”„ë¼ì¸ ëŒ€ì‹  LLMì´ ë„êµ¬ë¥¼ ì„ íƒí•˜ê³  ì‹¤í–‰í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

        Args:
            message: ì‚¬ìš©ì ì¿¼ë¦¬
            session_id: ì„¸ì…˜ ID
            start_time: íŒŒì´í”„ë¼ì¸ ì‹œì‘ ì‹œê°„

        Returns:
            RAGResultDict: Agent ëª¨ë“œ ì‘ë‹µ (metadata.mode="agent" í¬í•¨)

        Raises:
            GenerationError: Agent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        """
        # ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ
        session_context = ""
        if self.session_module:
            try:
                context_string = await self.session_module.get_context_string(session_id)
                if context_string:
                    session_context = context_string
                    logger.debug(
                        "ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ë¡œë“œ ì„±ê³µ",
                        extra={"context_length": len(context_string)}
                    )
            except Exception as e:
                logger.warning(
                    "ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ ì‹¤íŒ¨",
                    extra={"error": str(e)},
                    exc_info=True
                )

        try:
            # AgentOrchestrator ì‹¤í–‰
            agent_result: AgentResult = await self.agent_orchestrator.run(
                query=message,
                session_context=session_context,
            )

            # Agent ê²°ê³¼ë¥¼ RAGResultDict í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            processing_time = time.time() - start_time

            # Source ê°ì²´ ë³€í™˜ (Agent sourcesëŠ” dict í˜•íƒœì¼ ìˆ˜ ìˆìŒ)
            formatted_sources = []
            for idx, source in enumerate(agent_result.sources or []):
                if isinstance(source, dict):
                    formatted_sources.append(
                        self.Source(
                            id=idx,
                            document=source.get("source", source.get("title", f"Source {idx + 1}")),
                            page=source.get("page"),
                            chunk=source.get("chunk"),
                            relevance=source.get("relevance", source.get("score", 0.0)),
                            content_preview=source.get(
                                "content_preview", source.get("content", "")[:200]
                            ),
                            source_type="agent",
                        )
                    )
                else:
                    # ì´ë¯¸ Source ê°ì²´ì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    formatted_sources.append(source)

            result: RAGResultDict = cast(
                RAGResultDict,
                {
                    "answer": agent_result.answer,
                    "sources": formatted_sources,
                    "tokens_used": 0,  # Agent ëª¨ë“œì—ì„œëŠ” ê°œë³„ ì¶”ì  ì–´ë ¤ì›€
                    "topic": self.extract_topic_func(message),
                    "processing_time": processing_time,
                    "search_results": len(agent_result.sources or []),
                    "ranked_results": len(agent_result.sources or []),
                    "model_info": {
                        "provider": "agent",
                        "model": "agent_orchestrator",
                        "model_used": "agent_orchestrator",
                    },
                    "metadata": {
                        "mode": "agent",
                        "steps_taken": agent_result.steps_taken,
                        "tools_used": agent_result.tools_used,
                        "total_time": agent_result.total_time,
                        "success": agent_result.success,
                    },
                },
            )

            logger.info(
                "Agent ëª¨ë“œ ì™„ë£Œ",
                extra={
                    "steps_taken": agent_result.steps_taken,
                    "processing_time": processing_time,
                    "tools_count": len(agent_result.tools_used)
                }
            )

            return result

        except Exception as e:
            logger.error(
                "Agent ëª¨ë“œ ì‹¤í–‰ ì‹¤íŒ¨",
                extra={"error": str(e)},
                exc_info=True
            )
            raise GenerationError(
                ErrorCode.GENERATION_REQUEST_FAILED,
                session_id=session_id,
                error=str(e),
            ) from e
