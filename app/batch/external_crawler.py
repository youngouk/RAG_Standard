"""
ì™¸ë¶€ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬ (Playwright ê¸°ë°˜)
====================================
ê¸°ëŠ¥: ë¹„Notion(ì™¸ë¶€ ì›¹) ì†ŒìŠ¤ í¬ë¡¤ë§ (ì„¤ì • ê¸°ë°˜)
ìš©ë„: Notion APIë¡œ ì ‘ê·¼ ë¶ˆê°€ëŠ¥í•œ ì™¸ë¶€ ì‚¬ì´íŠ¸ ë°ì´í„° ìˆ˜ì§‘

ëŒ€ìƒ ì†ŒìŠ¤:
- `app/config/features/batch.yaml`ì˜ `batch.external.sources`ì— ì •ì˜ëœ í•­ëª©

ì°¸ê³ : Notion ì†ŒìŠ¤ëŠ” notion_batch.pyì—ì„œ ì²˜ë¦¬
"""

import asyncio
import os
from dataclasses import dataclass, field
from typing import Any

import httpx
from langchain_text_splitters import RecursiveCharacterTextSplitter
from playwright.async_api import async_playwright

from app.lib.logger import get_logger

logger = get_logger(__name__)


# ============================================================================
# ì„¤ì •
# ============================================================================


@dataclass
class ExternalSourceConfig:
    """ì™¸ë¶€ ì†ŒìŠ¤ ì„¤ì •"""

    name: str  # source_fileë¡œ ì‚¬ìš©
    url: str  # í¬ë¡¤ë§ ì‹œì‘ URL
    content_selector: str = "main"  # ì½˜í…ì¸  ì˜ì—­ CSS ì…€ë ‰í„°
    max_depth: int = 2  # ì¬ê·€ ê¹Šì´
    link_whitelist: list[str] = field(default_factory=list)  # í—ˆìš© ë§í¬ íŒ¨í„´


# ì™¸ë¶€ ì†ŒìŠ¤ ì„¤ì •
# - Blank ì‹œìŠ¤í…œì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸/ì‚¬ì´íŠ¸ë¥¼ ì½”ë“œì— í•˜ë“œì½”ë”©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
# - ì‹¤ì œ ì†ŒìŠ¤ëŠ” ì„¤ì • íŒŒì¼ì—ì„œ ì •ì˜í•˜ê³ , í•„ìš” ì‹œ run_external_batch()ì— ì£¼ì…í•˜ì„¸ìš”.
EXTERNAL_SOURCES: list[ExternalSourceConfig] = []


@dataclass
class CrawlResult:
    """í¬ë¡¤ë§ ê²°ê³¼"""

    source_name: str
    pages: list[dict]  # [{"url": str, "content": str, "title": str}]
    total_pages: int
    error_message: str = ""


@dataclass
class ChunkData:
    """ì²­í¬ ë°ì´í„°"""

    content: str
    source_file: str
    chunk_index: int
    url: str


@dataclass
class BatchResult:
    """ë°°ì¹˜ ê²°ê³¼"""

    source_name: str
    total_pages: int
    total_chunks: int
    uploaded_chunks: int
    deleted_chunks: int
    success: bool
    error_message: str = ""
    processing_time_seconds: float = 0.0


# ============================================================================
# ì™¸ë¶€ í¬ë¡¤ëŸ¬
# ============================================================================


class ExternalCrawler:
    """
    Playwright ê¸°ë°˜ ì™¸ë¶€ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬

    ì£¼ìš” ê¸°ëŠ¥:
    - JavaScript ë Œë”ë§ ì§€ì›
    - ì¬ê·€ ë§í¬ íƒìƒ‰
    - í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹
    """

    def __init__(
        self,
        chunk_size: int = 1400,
        chunk_overlap: int = 200,
        weaviate_url: str | None = None,
    ):
        """
        í¬ë¡¤ëŸ¬ ì´ˆê¸°í™”

        Args:
            chunk_size: ì²­í¬ í¬ê¸°
            chunk_overlap: ì²­í¬ ì˜¤ë²„ë©
            weaviate_url: Weaviate URL
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.weaviate_url = weaviate_url or os.getenv(
            "WEAVIATE_URL", "https://weaviate-production-70aa.up.railway.app"
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=["\n\n", "\n", ". ", " ", ""],
        )
        self._http_client: httpx.AsyncClient | None = None

        logger.info("âœ… ExternalCrawler ì´ˆê¸°í™” ì™„ë£Œ")

    async def _get_http_client(self) -> httpx.AsyncClient:
        """HTTP í´ë¼ì´ì–¸íŠ¸ ì§€ì—° ì´ˆê¸°í™”"""
        if self._http_client is None or self._http_client.is_closed:
            self._http_client = httpx.AsyncClient(timeout=60.0)
        return self._http_client

    async def close(self) -> None:
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        if self._http_client and not self._http_client.is_closed:
            await self._http_client.aclose()

    async def crawl_source(self, config: ExternalSourceConfig) -> CrawlResult:
        """
        ë‹¨ì¼ ì†ŒìŠ¤ í¬ë¡¤ë§

        HTTPS ì‹¤íŒ¨ ì‹œ HTTPë¡œ ìë™ fallback ì‹œë„

        Args:
            config: ì†ŒìŠ¤ ì„¤ì •

        Returns:
            CrawlResult: í¬ë¡¤ë§ ê²°ê³¼
        """
        pages: list[dict] = []
        visited: set[str] = set()

        # HTTPS â†’ HTTP fallback ì§€ì›
        urls_to_try = [config.url]
        if config.url.startswith("https://"):
            # HTTPS ì‹¤íŒ¨ ì‹œ HTTPë¡œ fallback
            http_url = config.url.replace("https://", "http://", 1)
            urls_to_try.append(http_url)

        last_error: str = ""

        for url_to_try in urls_to_try:
            logger.info(f"ğŸŒ í¬ë¡¤ë§ ì‹œì‘: {config.name} ({url_to_try})")
            pages = []
            visited = set()

            try:
                async with async_playwright() as p:
                    browser = await p.chromium.launch(headless=True)
                    context = await browser.new_context(
                        user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                        "AppleWebKit/537.36 (KHTML, like Gecko) "
                        "Chrome/120.0.0.0 Safari/537.36"
                    )

                    # ì¬ê·€ í¬ë¡¤ë§ (í˜„ì¬ ì‹œë„ URL ì‚¬ìš©)
                    await self._crawl_recursive(
                        context=context,
                        config=config,
                        url=url_to_try,
                        depth=0,
                        visited=visited,
                        pages=pages,
                    )

                    await browser.close()

                logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {config.name} ({len(pages)}í˜ì´ì§€)")

                return CrawlResult(
                    source_name=config.name,
                    pages=pages,
                    total_pages=len(pages),
                )

            except Exception as e:
                error_str = str(e)
                last_error = error_str

                # SSL/ì¸ì¦ì„œ ê´€ë ¨ ì˜¤ë¥˜ì¸ ê²½ìš° HTTP fallback ì‹œë„
                ssl_errors = ["SSL", "CERT", "certificate", "ERR_CERT", "TLS"]
                is_ssl_error = any(err in error_str for err in ssl_errors)

                if is_ssl_error and url_to_try.startswith("https://"):
                    logger.warning(
                        f"âš ï¸ HTTPS ì—°ê²° ì‹¤íŒ¨ (SSL ì˜¤ë¥˜), HTTPë¡œ fallback ì‹œë„: {config.name}"
                    )
                    continue  # HTTPë¡œ ì¬ì‹œë„
                else:
                    logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨ ({config.name}): {e}")
                    break  # SSL ì˜¤ë¥˜ê°€ ì•„ë‹ˆë©´ ì¤‘ë‹¨

        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨
        return CrawlResult(
            source_name=config.name,
            pages=pages,
            total_pages=len(pages),
            error_message=last_error,
        )

    async def _crawl_recursive(
        self,
        context: Any,
        config: ExternalSourceConfig,
        url: str,
        depth: int,
        visited: set[str],
        pages: list[dict],
    ) -> None:
        """
        ì¬ê·€ í¬ë¡¤ë§

        Args:
            context: Playwright ë¸Œë¼ìš°ì € ì»¨í…ìŠ¤íŠ¸
            config: ì†ŒìŠ¤ ì„¤ì •
            url: í¬ë¡¤ë§ URL
            depth: í˜„ì¬ ê¹Šì´
            visited: ë°©ë¬¸í•œ URL ì§‘í•©
            pages: ìˆ˜ì§‘ëœ í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸
        """
        # ì¤‘ë³µ ë°©ë¬¸ ë°©ì§€
        normalized_url = url.rstrip("/")
        if normalized_url in visited:
            return
        visited.add(normalized_url)

        # ê¹Šì´ ì œí•œ
        if depth > config.max_depth:
            return

        logger.debug(f"  ğŸ“„ [{depth}] {url}")

        try:
            page = await context.new_page()
            await page.goto(url, wait_until="networkidle", timeout=30000)

            # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
            await asyncio.sleep(1)

            # ì œëª© ì¶”ì¶œ
            title = await page.title()

            # ì½˜í…ì¸  ì¶”ì¶œ
            content = ""
            for selector in config.content_selector.split(", "):
                try:
                    element = await page.query_selector(selector)
                    if element:
                        content = await element.inner_text()
                        break
                except Exception:
                    continue

            if not content:
                # fallback: body ì „ì²´
                body = await page.query_selector("body")
                if body:
                    content = await body.inner_text()

            # í˜ì´ì§€ ì €ì¥
            if content.strip():
                pages.append(
                    {
                        "url": url,
                        "title": title,
                        "content": f"[{title}]\n\n{content}",
                    }
                )

            # ë§í¬ ì¶”ì¶œ ë° ì¬ê·€ í¬ë¡¤ë§
            if depth < config.max_depth:
                links = await self._extract_links(page, config)
                await page.close()

                for link in links:
                    if link not in visited:
                        await self._crawl_recursive(
                            context, config, link, depth + 1, visited, pages
                        )
            else:
                await page.close()

        except Exception as e:
            error_str = str(e)
            logger.warning(f"âš ï¸ í˜ì´ì§€ í¬ë¡¤ë§ ì‹¤íŒ¨ ({url}): {e}")

            # ë£¨íŠ¸ í˜ì´ì§€(depth=0)ì—ì„œ SSL ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒìœ„ë¡œ ì „íŒŒ
            # â†’ crawl_source()ì—ì„œ HTTP fallback ì‹œë„
            ssl_errors = ["SSL", "CERT", "certificate", "ERR_CERT", "TLS"]
            is_ssl_error = any(err in error_str for err in ssl_errors)

            if depth == 0 and is_ssl_error:
                raise  # ìƒìœ„ë¡œ ì „íŒŒí•˜ì—¬ HTTP fallback íŠ¸ë¦¬ê±°

    async def _extract_links(self, page: Any, config: ExternalSourceConfig) -> list[str]:
        """
        í˜ì´ì§€ì—ì„œ ë§í¬ ì¶”ì¶œ

        Args:
            page: Playwright í˜ì´ì§€
            config: ì†ŒìŠ¤ ì„¤ì •

        Returns:
            ë§í¬ URL ë¦¬ìŠ¤íŠ¸
        """
        links = []

        try:
            elements = await page.query_selector_all("a[href]")
            for element in elements:
                href = await element.get_attribute("href")
                if not href:
                    continue

                # ì ˆëŒ€ URL ë³€í™˜
                if href.startswith("/"):
                    base_url = "/".join(config.url.split("/")[:3])
                    href = base_url + href
                elif not href.startswith("http"):
                    continue

                # í™”ì´íŠ¸ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
                if config.link_whitelist:
                    if not any(pattern in href for pattern in config.link_whitelist):
                        continue

                # ì•µì»¤, ìë°”ìŠ¤í¬ë¦½íŠ¸ ì œì™¸
                if "#" in href:
                    href = href.split("#")[0]
                if href.startswith("javascript:"):
                    continue

                links.append(href)

        except Exception as e:
            logger.warning(f"âš ï¸ ë§í¬ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

        return list(set(links))  # ì¤‘ë³µ ì œê±°

    async def process_source(
        self, config: ExternalSourceConfig, dry_run: bool = False
    ) -> BatchResult:
        """
        ë‹¨ì¼ ì†ŒìŠ¤ ì²˜ë¦¬: í¬ë¡¤ë§ â†’ ì²­í‚¹ â†’ Weaviate ì—…ë¡œë“œ

        Args:
            config: ì†ŒìŠ¤ ì„¤ì •
            dry_run: Trueë©´ Weaviate ì—…ë¡œë“œ ê±´ë„ˆëœ€

        Returns:
            BatchResult: ì²˜ë¦¬ ê²°ê³¼
        """
        import time

        start_time = time.time()

        try:
            # 1. í¬ë¡¤ë§
            crawl_result = await self.crawl_source(config)

            if not crawl_result.pages:
                return BatchResult(
                    source_name=config.name,
                    total_pages=0,
                    total_chunks=0,
                    uploaded_chunks=0,
                    deleted_chunks=0,
                    success=False,
                    error_message=crawl_result.error_message or "í˜ì´ì§€ ì—†ìŒ",
                )

            # 2. ì²­í‚¹
            all_chunks: list[ChunkData] = []
            for page_data in crawl_result.pages:
                chunks = self.text_splitter.split_text(page_data["content"])
                for i, chunk in enumerate(chunks):
                    all_chunks.append(
                        ChunkData(
                            content=chunk,
                            source_file=config.name,
                            chunk_index=i,
                            url=page_data["url"],
                        )
                    )

            logger.info(f"ğŸ“¦ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")

            # 3. Weaviate ì—…ì„œíŠ¸
            if dry_run:
                logger.info("ğŸ”¸ Dry-run ëª¨ë“œ: Weaviate ì—…ë¡œë“œ ê±´ë„ˆëœ€")
                deleted_count = 0
                uploaded_count = len(all_chunks)
            else:
                deleted_count = await self._delete_existing_data(config.name)
                uploaded_count = await self._upload_chunks(all_chunks)

            elapsed = time.time() - start_time

            return BatchResult(
                source_name=config.name,
                total_pages=len(crawl_result.pages),
                total_chunks=len(all_chunks),
                uploaded_chunks=uploaded_count,
                deleted_chunks=deleted_count,
                success=True,
                processing_time_seconds=elapsed,
            )

        except Exception as e:
            elapsed = time.time() - start_time
            logger.error(f"âŒ ì†ŒìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨ ({config.name}): {e}")
            return BatchResult(
                source_name=config.name,
                total_pages=0,
                total_chunks=0,
                uploaded_chunks=0,
                deleted_chunks=0,
                success=False,
                error_message=str(e),
                processing_time_seconds=elapsed,
            )

    async def _delete_existing_data(self, source_file: str) -> int:
        """Weaviateì—ì„œ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ"""
        client = await self._get_http_client()

        # ê°œìˆ˜ í™•ì¸
        count_query = {
            "query": f"""{{
                Aggregate {{
                    Documents(where: {{
                        path: ["source_file"]
                        operator: Equal
                        valueText: "{source_file}"
                    }}) {{
                        meta {{ count }}
                    }}
                }}
            }}"""
        }

        try:
            count_response = await client.post(
                f"{self.weaviate_url}/v1/graphql",
                json=count_query,
            )
            count_data = count_response.json()
            existing_count = (
                count_data.get("data", {})
                .get("Aggregate", {})
                .get("Documents", [{}])[0]
                .get("meta", {})
                .get("count", 0)
            )
        except Exception:
            existing_count = 0

        if existing_count == 0:
            return 0

        # ì‚­ì œ
        delete_payload = {
            "match": {
                "class": "Documents",
                "where": {
                    "path": ["source_file"],
                    "operator": "Equal",
                    "valueText": source_file,
                },
            },
        }

        try:
            # httpx.AsyncClient.delete()ëŠ” json íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŒ
            # request() ë©”ì„œë“œ ì‚¬ìš©
            response = await client.request(
                "DELETE",
                f"{self.weaviate_url}/v1/batch/objects",
                json=delete_payload,
            )

            if response.status_code in (200, 204):
                logger.info(f"ğŸ—‘ï¸ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ: {source_file} ({existing_count}ê°œ)")
                return existing_count

        except Exception as e:
            logger.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {e}")

        return 0

    async def _upload_chunks(self, chunks: list[ChunkData]) -> int:
        """ì²­í¬ë¥¼ Weaviateì— ì—…ë¡œë“œ"""
        if not chunks:
            return 0

        client = await self._get_http_client()
        uploaded = 0
        batch_size = 100

        for i in range(0, len(chunks), batch_size):
            batch = chunks[i : i + batch_size]

            objects = [
                {
                    "class": "Documents",
                    "properties": {
                        "content": chunk.content,
                        "source_file": chunk.source_file,
                        "chunk_index": chunk.chunk_index,
                    },
                }
                for chunk in batch
            ]

            try:
                response = await client.post(
                    f"{self.weaviate_url}/v1/batch/objects",
                    json={"objects": objects},
                )

                if response.status_code == 200:
                    result = response.json()
                    success_count = sum(
                        1 for obj in result if obj.get("result", {}).get("status") == "SUCCESS"
                    )
                    uploaded += success_count

            except Exception as e:
                logger.error(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")

        logger.info(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ: {uploaded}/{len(chunks)}ê°œ")
        return uploaded

    async def run_batch(
        self, sources: list[ExternalSourceConfig] | None = None, dry_run: bool = False
    ) -> list[BatchResult]:
        """
        ì „ì²´ ë°°ì¹˜ ì‹¤í–‰

        Args:
            sources: ì²˜ë¦¬í•  ì†ŒìŠ¤ ëª©ë¡ (Noneì´ë©´ ê¸°ë³¸ ì†ŒìŠ¤ ì „ì²´)
            dry_run: Trueë©´ Weaviate ì—…ë¡œë“œ ê±´ë„ˆëœ€

        Returns:
            ë°°ì¹˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        sources = sources or EXTERNAL_SOURCES
        results: list[BatchResult] = []

        logger.info("=" * 60)
        logger.info("ğŸŒ ì™¸ë¶€ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ë°°ì¹˜ ì‹œì‘")
        logger.info(f"ğŸ“Š ëŒ€ìƒ: {[s.name for s in sources]}")
        logger.info("=" * 60)

        for config in sources:
            logger.info(f"\n{'â”€' * 40}")
            logger.info(f"ğŸ“ [{config.name}] ì²˜ë¦¬ ì‹œì‘")
            logger.info(f"{'â”€' * 40}")

            result = await self.process_source(config, dry_run=dry_run)
            results.append(result)

            if result.success:
                logger.info(
                    f"âœ… [{config.name}] ì™„ë£Œ: "
                    f"{result.total_pages}í˜ì´ì§€ â†’ {result.uploaded_chunks}ì²­í¬"
                )
            else:
                logger.error(f"âŒ [{config.name}] ì‹¤íŒ¨: {result.error_message}")

        await self.close()

        total_chunks = sum(r.uploaded_chunks for r in results)
        logger.info("\n" + "=" * 60)
        logger.info(f"âœ… ì™¸ë¶€ í¬ë¡¤ë§ ë°°ì¹˜ ì™„ë£Œ: ì´ {total_chunks}ê°œ ì²­í¬")
        logger.info("=" * 60)

        return results


# ============================================================================
# í¸ì˜ í•¨ìˆ˜
# ============================================================================


async def run_external_batch(
    source_names: list[str] | None = None,
    dry_run: bool = False,
) -> list[BatchResult]:
    """
    ì™¸ë¶€ ì‚¬ì´íŠ¸ ë°°ì¹˜ ì‹¤í–‰ í¸ì˜ í•¨ìˆ˜

    Args:
        source_names: ì²˜ë¦¬í•  ì†ŒìŠ¤ëª… ëª©ë¡ (Noneì´ë©´ ì „ì²´)
        dry_run: Trueë©´ Weaviate ì—…ë¡œë“œ ê±´ë„ˆëœ€

    Returns:
        ë°°ì¹˜ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    if source_names:
        sources = [s for s in EXTERNAL_SOURCES if s.name in source_names]
    else:
        sources = EXTERNAL_SOURCES

    crawler = ExternalCrawler()
    return await crawler.run_batch(sources=sources, dry_run=dry_run)


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse

    parser = argparse.ArgumentParser(description="ì™¸ë¶€ ì‚¬ì´íŠ¸ í¬ë¡¤ëŸ¬")
    parser.add_argument(
        "--source",
        "-s",
        default="all",
        help="ì²˜ë¦¬í•  ì†ŒìŠ¤ëª… (ì„¤ì •ì— ì •ì˜ëœ source name ë˜ëŠ” all)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Weaviate ì—…ë¡œë“œ ê±´ë„ˆëœ€",
    )

    args = parser.parse_args()

    source_names = None if args.source == "all" else [args.source]

    results = await run_external_batch(source_names=source_names, dry_run=args.dry_run)

    # ê²°ê³¼ ìš”ì•½
    print("\n" + "=" * 60)
    print("ğŸ“Š ì™¸ë¶€ í¬ë¡¤ë§ ê²°ê³¼ ìš”ì•½")
    print("=" * 60)

    for result in results:
        status = "âœ…" if result.success else "âŒ"
        print(
            f"{status} {result.source_name}: "
            f"{result.total_pages}í˜ì´ì§€ â†’ {result.uploaded_chunks}ì²­í¬ "
            f"({result.processing_time_seconds:.1f}ì´ˆ)"
        )


if __name__ == "__main__":
    asyncio.run(main())
