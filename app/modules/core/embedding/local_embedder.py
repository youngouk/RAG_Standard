"""
ë¡œì»¬ ì„ë² ë” êµ¬í˜„

sentence-transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
API í‚¤ ì—†ì´ ë™ì‘í•˜ë©°, Quickstart í™˜ê²½ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.

ì§€ì› ëª¨ë¸:
- Qwen/Qwen3-Embedding-0.6B (ê¸°ë³¸): 1024ì°¨ì›, 32K ì»¨í…ìŠ¤íŠ¸, 100+ ì–¸ì–´
- intfloat/multilingual-e5-small: 384ì°¨ì›, ê²½ëŸ‰

ì‚¬ìš© ì˜ˆì‹œ:
    embedder = LocalEmbedder()
    vectors = embedder.embed_documents(["ë¬¸ì„œ1", "ë¬¸ì„œ2"])
    query_vector = embedder.embed_query("ê²€ìƒ‰ ì¿¼ë¦¬")
"""

from __future__ import annotations

import logging
from typing import Any

import numpy as np
from sentence_transformers import SentenceTransformer

from app.modules.core.embedding.interfaces import BaseEmbedder

logger = logging.getLogger(__name__)


# ì§€ì› ëª¨ë¸ ì •ë³´
SUPPORTED_LOCAL_MODELS: dict[str, dict[str, Any]] = {
    "Qwen/Qwen3-Embedding-0.6B": {
        "dimensions": 1024,
        "max_seq_length": 32768,
        "description": "Qwen3 ì„ë² ë”© ëª¨ë¸ (0.6B íŒŒë¼ë¯¸í„°, ë‹¤êµ­ì–´ ì§€ì›)",
    },
    "intfloat/multilingual-e5-small": {
        "dimensions": 384,
        "max_seq_length": 512,
        "description": "ê²½ëŸ‰ ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸",
    },
}

# ê¸°ë³¸ ëª¨ë¸
DEFAULT_LOCAL_MODEL = "Qwen/Qwen3-Embedding-0.6B"


class LocalEmbedder(BaseEmbedder):
    """
    ë¡œì»¬ ì„ë² ë” í´ë˜ìŠ¤

    sentence-transformersë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œì»¬ì—ì„œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    ì²« ì‹¤í–‰ ì‹œ HuggingFace Hubì—ì„œ ëª¨ë¸ì„ ìë™ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.

    Attributes:
        model: SentenceTransformer ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
        normalize: L2 ì •ê·œí™” ì—¬ë¶€ (ê¸°ë³¸: True)
        batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸: 32)
    """

    def __init__(
        self,
        model_name: str = DEFAULT_LOCAL_MODEL,
        output_dimensionality: int | None = None,
        batch_size: int = 32,
        normalize: bool = True,
        device: str | None = None,
        **kwargs: Any,
    ) -> None:
        """
        LocalEmbedder ì´ˆê¸°í™”

        Args:
            model_name: HuggingFace ëª¨ë¸ ì´ë¦„ (ê¸°ë³¸: Qwen/Qwen3-Embedding-0.6B)
            output_dimensionality: ì¶œë ¥ ë²¡í„° ì°¨ì› (Noneì´ë©´ ëª¨ë¸ ê¸°ë³¸ê°’ ì‚¬ìš©)
            batch_size: ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸° (ê¸°ë³¸: 32)
            normalize: L2 ì •ê·œí™” ì—¬ë¶€ (ê¸°ë³¸: True)
            device: ì—°ì‚° ë””ë°”ì´ìŠ¤ (Noneì´ë©´ ìë™ ì„ íƒ, "cpu" ë˜ëŠ” "cuda")

        Raises:
            Exception: ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨ ì‹œ
        """
        # ëª¨ë¸ ì •ë³´ í™•ì¸
        model_info = SUPPORTED_LOCAL_MODELS.get(model_name, {})
        default_dim = model_info.get("dimensions", 1024)

        # ì°¨ì› ì„¤ì • (ëª…ì‹œì  ì§€ì • > ëª¨ë¸ ê¸°ë³¸ê°’)
        actual_dim = output_dimensionality or default_dim

        # ë¶€ëª¨ í´ë˜ìŠ¤ ì´ˆê¸°í™”
        super().__init__(
            model_name=model_name,
            output_dimensionality=actual_dim,
            api_key=None,  # ë¡œì»¬ ëª¨ë¸ì€ API í‚¤ ë¶ˆí•„ìš”
        )

        self._batch_size = batch_size
        self._normalize = normalize
        self._device = device

        # ëª¨ë¸ ë¡œë“œ (ì²« ì‹¤í–‰ ì‹œ ìë™ ë‹¤ìš´ë¡œë“œ)
        logger.info(f"ğŸ”„ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
        try:
            self._model = SentenceTransformer(
                model_name,
                device=device,
                trust_remote_code=True,  # Qwen ëª¨ë¸ í•„ìš”
            )
            logger.info(
                f"âœ… ë¡œì»¬ ì„ë² ë” ì´ˆê¸°í™” ì™„ë£Œ: model={model_name}, "
                f"dim={actual_dim}, device={self._model.device}"
            )
        except Exception as e:
            logger.error(f"âŒ ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise

    @property
    def batch_size(self) -> int:
        """ë°°ì¹˜ ì²˜ë¦¬ í¬ê¸°"""
        return self._batch_size

    def embed_documents(self, texts: list[str]) -> list[list[float]]:
        """
        ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸ (list[list[float]])
        """
        if not texts:
            return []

        try:
            # sentence-transformersë¡œ ì„ë² ë”© ìƒì„±
            embeddings = self._model.encode(
                texts,
                batch_size=self._batch_size,
                normalize_embeddings=self._normalize,
                show_progress_bar=False,
                convert_to_numpy=True,
            )

            # numpy array â†’ list[list[float]] ë³€í™˜
            result = embeddings.tolist()

            logger.debug(f"ğŸ“Š ë¬¸ì„œ {len(texts)}ê°œ ì„ë² ë”© ì™„ë£Œ (dim={len(result[0])})")
            return result

        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            # graceful degradation: ì˜ë²¡í„° ë°˜í™˜
            return [[0.0] * self._output_dimensionality for _ in texts]

    def embed_query(self, text: str) -> list[float]:
        """
        ë‹¨ì¼ ì¿¼ë¦¬ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

        Args:
            text: ì„ë² ë”©í•  ì¿¼ë¦¬ í…ìŠ¤íŠ¸

        Returns:
            ì„ë² ë”© ë²¡í„° (list[float])
        """
        if not text:
            return [0.0] * self._output_dimensionality

        try:
            # ë‹¨ì¼ ì¿¼ë¦¬ ì„ë² ë”©
            embedding = self._model.encode(
                text,
                normalize_embeddings=self._normalize,
                show_progress_bar=False,
                convert_to_numpy=True,
            )

            # numpy array â†’ list[float] ë³€í™˜
            result = embedding.tolist()

            logger.debug(f"ğŸ“Š ì¿¼ë¦¬ ì„ë² ë”© ì™„ë£Œ (dim={len(result)})")
            return result

        except Exception as e:
            logger.error(f"âŒ ì¿¼ë¦¬ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return [0.0] * self._output_dimensionality

    async def aembed_documents(self, texts: list[str]) -> list[list[float]]:
        """
        ë¹„ë™ê¸° ë¬¸ì„œ ì„ë² ë”© (ë™ê¸° ë©”ì„œë“œ ë˜í•‘)

        Note:
            sentence-transformersëŠ” ë„¤ì´í‹°ë¸Œ ë¹„ë™ê¸°ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
            ë™ê¸° ë©”ì„œë“œë¥¼ ë˜í•‘í•©ë‹ˆë‹¤.
        """
        return self.embed_documents(texts)

    async def aembed_query(self, text: str) -> list[float]:
        """
        ë¹„ë™ê¸° ì¿¼ë¦¬ ì„ë² ë”© (ë™ê¸° ë©”ì„œë“œ ë˜í•‘)
        """
        return self.embed_query(text)

    def validate_embedding(self, embedding: list[float]) -> bool:
        """
        ì„ë² ë”© ë²¡í„° ìœ íš¨ì„± ê²€ì¦

        Args:
            embedding: ê²€ì¦í•  ì„ë² ë”© ë²¡í„°

        Returns:
            ìœ íš¨ ì—¬ë¶€ (True/False)
        """
        if not embedding:
            return False

        # ì°¨ì› ê²€ì¦
        if len(embedding) != self._output_dimensionality:
            logger.warning(
                f"âš ï¸ ì„ë² ë”© ì°¨ì› ë¶ˆì¼ì¹˜: "
                f"expected={self._output_dimensionality}, got={len(embedding)}"
            )
            return False

        return True
