"""
Retrieval Orchestrator - Facade Patternìœ¼ë¡œ ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° í†µí•©

## ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°(Orchestrator)ë€?
ë³µì¡í•œ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ ì—¬ëŸ¬ êµ¬ì„±ìš”ì†Œ(Retriever, Reranker, Cache)ë¥¼ í•˜ë‚˜ì˜ ê°„ë‹¨í•œ ì¸í„°í˜ì´ìŠ¤ë¡œ í†µí•©í•˜ëŠ”
Facade íŒ¨í„´ êµ¬í˜„ì…ë‹ˆë‹¤.

## ì™œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°ê°€ í•„ìš”í•œê°€?

### 1. ë³µì¡ì„± ìˆ¨ê¹€ (Complexity Hiding)
í´ë¼ì´ì–¸íŠ¸ê°€ ì—¬ëŸ¬ ëª¨ë“ˆì˜ ì„¸ë¶€ì‚¬í•­ì„ ì•Œ í•„ìš” ì—†ì´ `search_and_rerank()` í•œ ë²ˆì˜ í˜¸ì¶œë¡œ
ì „ì²´ ê²€ìƒ‰ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Without Orchestrator:**
```python
# í´ë¼ì´ì–¸íŠ¸ê°€ ë³µì¡í•œ ìˆœì„œë¥¼ ì§ì ‘ ê´€ë¦¬í•´ì•¼ í•¨
retriever = MongoDBRetriever(...)
reranker = JinaReranker(...)
cache = MemoryCacheManager(...)

# 1. ìºì‹œ í™•ì¸
cache_key = cache.generate_cache_key(query, top_k)
cached = await cache.get(cache_key)
if cached:
    return cached

# 2. ê²€ìƒ‰ ì‹¤í–‰
results = await retriever.search(query, top_k)

# 3. ë¦¬ë­í‚¹ ì‹¤í–‰
reranked = await reranker.rerank(query, results, top_k)

# 4. ìºì‹œ ì €ì¥
await cache.set(cache_key, reranked)
```

**With Orchestrator:**
```python
# ê°„ë‹¨í•œ í•œ ì¤„ í˜¸ì¶œ
results = await orchestrator.search_and_rerank(query, top_k)
```

### 2. ì›Œí¬í”Œë¡œìš° ì¡°ìœ¨ (Workflow Coordination)
ê²€ìƒ‰ â†’ ìºì‹± â†’ ë¦¬ë­í‚¹ì˜ ë³µì¡í•œ ìˆœì„œì™€ ì—ëŸ¬ ì²˜ë¦¬ë¥¼ ë‚´ë¶€ì—ì„œ ìë™ìœ¼ë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.

### 3. ìœ ì—°í•œ êµ¬ì„± (Flexible Configuration)
Dependency Injectionì„ í†µí•´ ë‹¤ì–‘í•œ Retriever/Reranker ì¡°í•©ì„ ì‰½ê²Œ êµì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# MongoDB + Gemini Flash ì¡°í•© (í”„ë¡œë•ì…˜)
orchestrator1 = RetrievalOrchestrator(
    retriever=MongoDBRetriever(...),
    reranker=GeminiFlashReranker(...),
    cache=MemoryCacheManager(...)
)

# MongoDB + Jina ì¡°í•© (ëŒ€ì•ˆ)
orchestrator2 = RetrievalOrchestrator(
    retriever=MongoDBRetriever(...),
    reranker=CohereReranker(...),
    cache=RedisCacheManager(...)
)
```

### 4. ê´€ì‹¬ì‚¬ ë¶„ë¦¬ (Separation of Concerns)
- Retriever: ë²¡í„° ê²€ìƒ‰ì—ë§Œ ì§‘ì¤‘
- Reranker: ë¦¬ë­í‚¹ì—ë§Œ ì§‘ì¤‘
- Cache: ìºì‹±ì—ë§Œ ì§‘ì¤‘
- Orchestrator: ì „ì²´ íë¦„ ì¡°ìœ¨ì—ë§Œ ì§‘ì¤‘

### 5. í…ŒìŠ¤íŠ¸ ìš©ì´ì„± (Testability)
ê° êµ¬ì„±ìš”ì†Œë¥¼ ë…ë¦½ì ìœ¼ë¡œ Mock/Testí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# í…ŒìŠ¤íŠ¸ ì‹œ Mock ì£¼ì… ê°€ëŠ¥
mock_retriever = MockRetriever()
mock_reranker = MockReranker()
orchestrator = RetrievalOrchestrator(mock_retriever, mock_reranker, cache)
```

## ì•„í‚¤í…ì²˜ ë‹¤ì´ì–´ê·¸ë¨

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Client (chat.py, APIs)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚ search_and_rerank(query, top_k)
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     RetrievalOrchestrator (Facade)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 1. ìºì‹œ í™•ì¸ â†’ ICacheManager              â”‚  â”‚
â”‚  â”‚ 2. ê²€ìƒ‰ ì‹¤í–‰ â†’ IRetriever                 â”‚  â”‚
â”‚  â”‚ 3. ë¦¬ë­í‚¹ â†’ IReranker (ì„ íƒì )            â”‚  â”‚
â”‚  â”‚ 4. ìºì‹œ ì €ì¥ â†’ ICacheManager              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚              â”‚              â”‚
         â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MongoDBRet...â”‚ â”‚GeminiFlash..â”‚ â”‚ MemoryCache..â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ê¸°ì¡´ ì½”ë“œ ê¸°ë°˜
ì´ ì½”ë“œëŠ” ê¸°ì¡´ `retrieval_rerank.py`ì˜ RetrievalModuleì˜ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¶”ì¶œí•˜ì—¬
Facade íŒ¨í„´ìœ¼ë¡œ ì¬êµ¬ì„±í•œ ê²ƒì…ë‹ˆë‹¤.

âš ï¸ ì£¼ì˜: ê¸°ì¡´ ê²€ì¦ëœ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤. ìƒˆë¡œ ì‘ì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.
"""

from typing import TYPE_CHECKING, Any

from ....lib.logger import get_logger
from ....lib.types import HealthCheckDict, OrchestratorStatsDict
from .interfaces import ICacheManager, IReranker, IRetriever, SearchResult
from .query_expansion import IQueryExpansionEngine
from .scoring import ScoringService

# ìˆœí™˜ ì°¸ì¡° ë°©ì§€ë¥¼ ìœ„í•œ TYPE_CHECKING ë¸”ë¡
if TYPE_CHECKING:
    from ...graph.interfaces import IGraphStore
    from .hybrid_search.interfaces import IHybridSearchStrategy

logger = get_logger(__name__)


class RetrievalOrchestrator:
    """
    ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš°ë¥¼ ì¡°ìœ¨í•˜ëŠ” Facade í´ë˜ìŠ¤

    ì—­í• :
    1. Retriever, Reranker, Cacheë¥¼ í•˜ë‚˜ì˜ ì¸í„°í˜ì´ìŠ¤ë¡œ í†µí•©
    2. ê²€ìƒ‰ â†’ ìºì‹± â†’ ë¦¬ë­í‚¹ì˜ ë³µì¡í•œ ìˆœì„œë¥¼ ë‚´ë¶€ì—ì„œ ìë™ ê´€ë¦¬
    3. ì—ëŸ¬ í•¸ë“¤ë§ ë° í´ë°± ë¡œì§ ì œê³µ
    4. í†µê³„ ìˆ˜ì§‘ ë° ëª¨ë‹ˆí„°ë§

    ê¸°ì¡´ ì½”ë“œ ê¸°ë°˜: retrieval_rerank.pyì˜ RetrievalModule ì›Œí¬í”Œë¡œìš°
    """

    def __init__(
        self,
        retriever: IRetriever,
        reranker: IReranker | None = None,
        cache: ICacheManager | None = None,
        query_expansion: IQueryExpansionEngine | None = None,
        graph_store: "IGraphStore | None" = None,
        hybrid_strategy: "IHybridSearchStrategy | None" = None,
        config: dict[str, Any] | None = None,
    ):
        """
        Args:
            retriever: ë²¡í„° ê²€ìƒ‰ì„ ë‹´ë‹¹í•˜ëŠ” Retriever (í•„ìˆ˜)
            reranker: ë¦¬ë­í‚¹ì„ ë‹´ë‹¹í•˜ëŠ” Reranker (ì„ íƒì )
            cache: ìºì‹±ì„ ë‹´ë‹¹í•˜ëŠ” CacheManager (ì„ íƒì )
            query_expansion: ì¿¼ë¦¬ í™•ì¥ ì—”ì§„ (ì„ íƒì )
            graph_store: ê·¸ë˜í”„ ì €ì¥ì†Œ (ì„ íƒì , í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš©)
            hybrid_strategy: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì „ëµ (ì„ íƒì , ì§ì ‘ ì œê³µ)
            config: ì¶”ê°€ ì„¤ì • (ê²€ìƒ‰ ì˜µì…˜, ë¦¬ë­í‚¹ ì˜µì…˜ ë“±)
                - hybrid_search: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì •
                    - enabled: í™œì„±í™” ì—¬ë¶€ (ê¸°ë³¸ê°’: False)
                    - vector_weight: ë²¡í„° ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.6)
                    - graph_weight: ê·¸ë˜í”„ ê²€ìƒ‰ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: 0.4)
                    - rrf_k: RRF ìƒìˆ˜ (ê¸°ë³¸ê°’: 60)
        """
        self.retriever = retriever
        self.reranker = reranker
        self.cache = cache
        self.query_expansion = query_expansion
        self.graph_store = graph_store
        self.config = config or {}

        # ğŸ†• ScoringService ì´ˆê¸°í™” (ì„¤ì • ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš©)
        scoring_config = self.config.get("scoring", {})
        self.scoring_service = ScoringService(scoring_config)

        # ğŸ†• í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì „ëµ ì„¤ì •
        # ìš°ì„ ìˆœìœ„: ì§ì ‘ ì£¼ì… > graph_store ê¸°ë°˜ ìë™ ìƒì„±
        self._hybrid_strategy = hybrid_strategy

        # graph_rag ì„¤ì •ì—ì„œ hybrid_search ì½ê¸°
        graph_rag_config = self.config.get("graph_rag", {})
        hybrid_config = graph_rag_config.get("hybrid_search", {})

        if self._hybrid_strategy is None and graph_store is not None:
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì„¤ì • í™•ì¸
            hybrid_enabled = hybrid_config.get("enabled", True)  # graph_storeê°€ ìˆìœ¼ë©´ ê¸°ë³¸ í™œì„±í™”

            if hybrid_enabled:
                # VectorGraphHybridSearch ìë™ ìƒì„±
                from .hybrid_search import VectorGraphHybridSearch

                self._hybrid_strategy = VectorGraphHybridSearch(
                    retriever=retriever,
                    graph_store=graph_store,
                    config=hybrid_config,
                )
                logger.info("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í™œì„±í™” (ë²¡í„°+ê·¸ë˜í”„ RRF)")

        # ğŸ†• ìë™ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ í™œì„±í™” í”Œë˜ê·¸ ì„¤ì •
        # YAML ì„¤ì •ì˜ graph_rag.hybrid_search.auto_enable ê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ê²°ì •
        # ì¡°ê±´: enabled=true AND auto_enable=true AND hybrid_strategy ì¡´ì¬
        self._auto_use_graph = (
            hybrid_config.get("enabled", True)
            and hybrid_config.get("auto_enable", False)
            and self._hybrid_strategy is not None
        )

        if self._auto_use_graph:
            logger.info("í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìë™ í™œì„±í™”ë¨ (auto_enable=true)")

        # í†µê³„
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "retrieval_count": 0,
            "rerank_count": 0,
            "query_expansion_count": 0,
            "hybrid_search_count": 0,  # ğŸ†• í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ íšŸìˆ˜
        }

        logger.info(
            "RetrievalOrchestrator ì´ˆê¸°í™”",
            extra={
                "retriever": type(retriever).__name__,
                "reranker": type(reranker).__name__ if reranker else "None",
                "cache": type(cache).__name__ if cache else "None",
                "query_expansion": type(query_expansion).__name__ if query_expansion else "None",
                "graph_store": type(graph_store).__name__ if graph_store else "None",
                "hybrid_strategy": type(self._hybrid_strategy).__name__ if self._hybrid_strategy else "None",
                "scoring_service": self.scoring_service
            }
        )

    async def initialize(self) -> None:
        """ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë° ëª¨ë“  êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™”"""
        try:
            logger.info("RetrievalOrchestrator ì´ˆê¸°í™” ì‹œì‘...")

            # Retriever ì´ˆê¸°í™”
            if hasattr(self.retriever, "initialize"):
                await self.retriever.initialize()
                logger.debug(
                    "Retriever ì´ˆê¸°í™” ì™„ë£Œ",
                    extra={"retriever": type(self.retriever).__name__}
                )

            # Reranker ì´ˆê¸°í™” (ì„ íƒì )
            if self.reranker and hasattr(self.reranker, "initialize"):
                await self.reranker.initialize()
                logger.debug(
                    "Reranker ì´ˆê¸°í™” ì™„ë£Œ",
                    extra={"reranker": type(self.reranker).__name__}
                )

            # Cache ì´ˆê¸°í™” (ì„ íƒì )
            if self.cache and hasattr(self.cache, "initialize"):
                await self.cache.initialize()
                logger.debug(
                    "Cache ì´ˆê¸°í™” ì™„ë£Œ",
                    extra={"cache": type(self.cache).__name__}
                )

            logger.info("RetrievalOrchestrator ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(
                "RetrievalOrchestrator ì´ˆê¸°í™” ì‹¤íŒ¨",
                extra={"error": str(e)},
                exc_info=True
            )
            raise

    async def close(self) -> None:
        """ëª¨ë“  êµ¬ì„±ìš”ì†Œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            if hasattr(self.retriever, "close"):
                await self.retriever.close()

            if self.reranker and hasattr(self.reranker, "close"):
                await self.reranker.close()

            if self.cache and hasattr(self.cache, "close"):
                await self.cache.close()

            logger.info("RetrievalOrchestrator ì¢…ë£Œ ì™„ë£Œ")

        except Exception as e:
            logger.error(
                "RetrievalOrchestrator ì¢…ë£Œ ì‹¤íŒ¨",
                extra={"error": str(e)},
                exc_info=True
            )

    def _apply_txt_limit(self, results: list) -> list:
        """
        TXT íŒŒì¼ ë‹¤ì–‘ì„± ì œí•œ ì ìš© (ìµœëŒ€ 15ê°œ)

        ì£¼ì˜: ê¸°ì¡´ 7ê°œ ì œí•œì„ 15ê°œë¡œ ìƒí–¥ ì¡°ì •í•˜ì—¬ ë‹¤ë¥¸ íŒŒì¼ íƒ€ì…ê³¼ í•¨ê»˜
        ì¶©ë¶„í•œ ë¬¸ì„œ ìˆ˜ë¥¼ í™•ë³´í•  ìˆ˜ ìˆë„ë¡ í•¨

        Args:
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Returns:
            TXT ì œí•œì´ ì ìš©ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        txt_count = 0
        txt_limit = 15  # 7 â†’ 15ë¡œ ìƒí–¥ ì¡°ì •
        diverse_results = []
        original_count = len(results)

        logger.info(
            "TXT ë‹¤ì–‘ì„± ì œí•œ ì‹œì‘",
            extra={"original_count": original_count}
        )

        for result in results:
            try:
                # ì•ˆì „í•œ metadata ì ‘ê·¼
                metadata = getattr(result, "metadata", {}) or {}
                file_type = metadata.get("file_type", "") if isinstance(metadata, dict) else ""
                file_type = file_type.upper() if file_type else ""
            except Exception as e:
                logger.warning(
                    "file_type ì ‘ê·¼ ì‹¤íŒ¨",
                    extra={"error": str(e)},
                    exc_info=True
                )
                file_type = ""

            if file_type == "TXT":
                if txt_count < txt_limit:
                    diverse_results.append(result)
                    txt_count += 1
                # txt_limit ì´ˆê³¼ ì‹œ í•´ë‹¹ ê²°ê³¼ ì œì™¸
            else:
                diverse_results.append(result)

        logger.info(
            f"TXT ë‹¤ì–‘ì„± ì œí•œ ì™„ë£Œ: {original_count}ê°œ â†’ {len(diverse_results)}ê°œ "
            f"(TXT {txt_count}/{txt_limit}ê°œ)"
        )

        return diverse_results

    async def search_and_rerank(
        self,
        query: str,
        top_k: int = 15,
        filters: dict[str, Any] | None = None,
        rerank_enabled: bool = True,
        query_expansion_enabled: bool | None = None,  # None = ìë™ íŒë‹¨
        use_graph: bool | None = None,  # ğŸ†• None = auto_enable ì„¤ì •ì— ë”°ë¼ ìë™ ê²°ì •
    ) -> list[SearchResult]:
        """
        í†µí•© ê²€ìƒ‰ + ë¦¬ë­í‚¹ ì›Œí¬í”Œë¡œìš° (Facade ë©”ì„œë“œ)

        ê¸°ì¡´ ì½”ë“œ: retrieval_rerank.pyì˜ search() + rerank() ì¡°í•©

        ì›Œí¬í”Œë¡œìš°:
        1. ìºì‹œ í™•ì¸ (ìºì‹œ ë§¤ë‹ˆì €ê°€ ìˆëŠ” ê²½ìš°)
        2. ì¿¼ë¦¬ í™•ì¥ (Query Expansion ì—”ì§„ì´ ìˆëŠ” ê²½ìš°)
        3. ê²€ìƒ‰ ì‹¤í–‰:
           - use_graph=True && í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ ìˆìŒ: ë²¡í„°+ê·¸ë˜í”„ RRF ê²°í•© ê²€ìƒ‰
           - ê·¸ ì™¸: ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰
        4. ê²°ê³¼ ë³‘í•© ë° ì¤‘ë³µ ì œê±°
        5. ë¦¬ë­í‚¹ ì‹¤í–‰ (Reranker, í™œì„±í™”ëœ ê²½ìš°)
        6. ìºì‹œ ì €ì¥ (ìºì‹œ ë§¤ë‹ˆì €ê°€ ìˆëŠ” ê²½ìš°)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            filters: ê²€ìƒ‰ í•„í„° (ë©”íƒ€ë°ì´í„° ë“±)
            rerank_enabled: ë¦¬ë­í‚¹ í™œì„±í™” ì—¬ë¶€
            query_expansion_enabled: ì¿¼ë¦¬ í™•ì¥ í™œì„±í™” ì—¬ë¶€
                - None: ìë™ íŒë‹¨ (config ë˜ëŠ” ì¿¼ë¦¬ ë³µì¡ë„ ê¸°ë°˜)
                - True: ê°•ì œ í™œì„±í™”
                - False: ê°•ì œ ë¹„í™œì„±í™”
            use_graph: ê·¸ë˜í”„ ê²€ìƒ‰ í¬í•¨ ì—¬ë¶€
                - None: auto_enable ì„¤ì •ì— ë”°ë¼ ìë™ ê²°ì • (ê¸°ë³¸ê°’)
                - True: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ë²¡í„°+ê·¸ë˜í”„ RRF) ê°•ì œ ì‚¬ìš©
                - False: ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ë§Œ ê°•ì œ ì‚¬ìš©

        Returns:
            ê²€ìƒ‰ ë° ë¦¬ë­í‚¹ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        self.stats["total_requests"] += 1

        # ğŸ†• use_graph ìë™ ê²°ì •
        # Noneì´ë©´ _auto_use_graph ì„¤ì •ê°’ ì‚¬ìš©, ëª…ì‹œì  ê°’ì´ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        effective_use_graph = use_graph if use_graph is not None else self._auto_use_graph

        cache_key = None  # ìºì‹œ í‚¤ ì´ˆê¸°í™”

        try:
            # Step 1: ìºì‹œ í™•ì¸ (ì„ íƒì )
            if self.cache:
                try:
                    cache_key = self.cache.generate_cache_key(query, top_k, filters)  # type: ignore[attr-defined]
                    cached_results = await self.cache.get(cache_key)

                    if cached_results:
                        self.stats["cache_hits"] += 1
                        logger.info(
                            f"ìºì‹œ íˆíŠ¸: query='{query[:50]}...', results={len(cached_results)}"
                        )
                        # ìºì‹œëœ ê²°ê³¼ì—ë„ TXT ì œí•œ ì ìš©
                        filtered_results = self._apply_txt_limit(cached_results)
                        return filtered_results

                    self.stats["cache_misses"] += 1
                except Exception as e:
                    logger.warning(
                        f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨, ì§ì ‘ ê²€ìƒ‰ìœ¼ë¡œ ìš°íšŒ: {e}",
                        exc_info=True,
                        extra={"query": query[:100]}
                    )
                    # ìºì‹œ ì‹¤íŒ¨ ì‹œ ì§ì ‘ ê²€ìƒ‰ ì§„í–‰

            # Step 2: ì¿¼ë¦¬ í™•ì¥ (ì„ íƒì )
            search_queries = [query]  # ê¸°ë³¸ê°’: ì›ë³¸ ì¿¼ë¦¬ë§Œ ì‚¬ìš©
            expanded_query_obj = None

            if self.query_expansion:
                # ì¿¼ë¦¬ í™•ì¥ í™œì„±í™” ì—¬ë¶€ íŒë‹¨
                should_expand = query_expansion_enabled

                if should_expand is None:
                    # ìë™ íŒë‹¨: config ë˜ëŠ” ì¿¼ë¦¬ ë³µì¡ë„ ê¸°ë°˜
                    # config.yamlì˜ query_expansion.enabled ë˜ëŠ” multi_query.enable_query_expansion ì‚¬ìš©
                    query_exp_config = self.config.get("query_expansion", {})
                    multi_query_config = self.config.get("multi_query", {})
                    should_expand = query_exp_config.get(
                        "enabled", multi_query_config.get("enable_query_expansion", True)
                    )

                if should_expand:
                    try:
                        logger.debug(
                            "ì¿¼ë¦¬ í™•ì¥ ì‹œì‘",
                            extra={"query": query[:50]}
                        )
                        expanded_query_obj = await self.query_expansion.expand(query)
                        search_queries = expanded_query_obj.all_queries
                        self.stats["query_expansion_count"] += 1

                        logger.info(
                            "ì¿¼ë¦¬ í™•ì¥ ì™„ë£Œ",
                            extra={
                                "query_count": len(search_queries),
                                "complexity": expanded_query_obj.complexity.value,
                                "intent": expanded_query_obj.intent.value
                            }
                        )
                    except Exception as e:
                        logger.warning(
                            "ì¿¼ë¦¬ í™•ì¥ ì‹¤íŒ¨, ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©",
                            extra={"error": str(e)},
                            exc_info=True
                        )
                        search_queries = [query]

            # Step 3: ê²€ìƒ‰ ì‹¤í–‰ (í•˜ì´ë¸Œë¦¬ë“œ ë˜ëŠ” ë²¡í„° ê²€ìƒ‰)
            # ğŸ†• í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: effective_use_graph=True && í•˜ì´ë¸Œë¦¬ë“œ ì „ëµ ì¡´ì¬
            if effective_use_graph and self._hybrid_strategy is not None:
                logger.info(
                    "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹œì‘",
                    extra={"query": query[:50], "top_k": top_k}
                )

                try:
                    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤í–‰ (ë²¡í„° + ê·¸ë˜í”„ RRF ê²°í•©)
                    hybrid_result = await self._hybrid_strategy.search(
                        query=query,
                        top_k=top_k * 2,  # ë¦¬ë­í‚¹ìš© ì—¬ìœ ë¶„
                    )
                    search_results = hybrid_result.documents
                    self.stats["hybrid_search_count"] += 1

                    logger.info(
                        "í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ",
                        extra={
                            "result_count": len(search_results),
                            "vector_count": hybrid_result.vector_count,
                            "graph_count": hybrid_result.graph_count
                        }
                    )
                except Exception as e:
                    logger.error(
                        f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}, ë¹ˆ ê²°ê³¼ ë°˜í™˜ (ì„œë¹„ìŠ¤ ê³„ì† ë™ì‘)",
                        exc_info=True,
                        extra={"query": query[:100]}
                    )
                    # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜ (ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ë°©ì§€)
                    search_results = []

            # ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ (ë‹¤ì¤‘ ì¿¼ë¦¬ ì§€ì›)
            else:
                logger.info(
                    "ë²¡í„° ê²€ìƒ‰ ì‹œì‘",
                    extra={"query_count": len(search_queries), "top_k": top_k}
                )

                try:
                    if len(search_queries) == 1:
                        # ë‹¨ì¼ ì¿¼ë¦¬: ê¸°ì¡´ ë¡œì§ ìœ ì§€
                        search_results = await self.retriever.search(query, top_k, filters)
                        self.stats["retrieval_count"] += 1
                    else:
                        # ë‹¤ì¤‘ ì¿¼ë¦¬: ë³‘ë ¬ ê²€ìƒ‰ ë° ê²°ê³¼ ë³‘í•©
                        search_results = await self._search_and_merge(search_queries, top_k, filters)
                        self.stats["retrieval_count"] += len(search_queries)

                    logger.info(
                        "ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ",
                        extra={"result_count": len(search_results)}
                    )
                except Exception as e:
                    logger.error(
                        f"ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}, ë¹ˆ ê²°ê³¼ ë°˜í™˜ (ì„œë¹„ìŠ¤ ê³„ì† ë™ì‘)",
                        exc_info=True,
                        extra={"query": query[:100]}
                    )
                    # Retriever ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜ (ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ë°©ì§€)
                    search_results = []

            # Step 3: ë¦¬ë­í‚¹ ì‹¤í–‰ (ì„ íƒì )
            final_results = search_results

            if rerank_enabled and self.reranker and search_results:
                logger.info(
                    "ë¦¬ë­í‚¹ ì‹œì‘",
                    extra={"result_count": len(search_results)}
                )
                try:
                    reranked_results = await self.reranker.rerank(query, search_results, top_k)
                    self.stats["rerank_count"] += 1

                    if reranked_results:
                        final_results = reranked_results
                        logger.info(
                            "ë¦¬ë­í‚¹ ì™„ë£Œ",
                            extra={"result_count": len(final_results)}
                        )
                    else:
                        logger.warning("ë¦¬ë­í‚¹ ê²°ê³¼ ì—†ìŒ, ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©")
                except Exception as e:
                    logger.error(
                        f"ë¦¬ë­í‚¹ ì‹¤íŒ¨: {e}, ì›ë³¸ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©",
                        exc_info=True,
                        extra={"query": query[:100]}
                    )
                    # ë¦¬ë­í‚¹ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ê²°ê³¼ë¡œ fallback

            # Step 3.5: TXT íŒŒì¼ ë‹¤ì–‘ì„± ì œí•œ (ìµœëŒ€ 15ê°œ)
            final_results = self._apply_txt_limit(final_results)

            # Step 4: ìºì‹œ ì €ì¥ (ì„ íƒì )
            if self.cache and cache_key:
                try:
                    await self.cache.set(cache_key, final_results)
                    logger.debug(
                        "ìºì‹œ ì €ì¥ ì™„ë£Œ",
                        extra={"result_count": len(final_results)}
                    )
                except Exception as e:
                    logger.warning(
                        f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}",
                        exc_info=True,
                        extra={"query": query[:100]}
                    )
                    # ìºì‹œ ì €ì¥ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ (ê²€ìƒ‰ ê²°ê³¼ëŠ” ì •ìƒ ë°˜í™˜)

            logger.info(
                "search_and_rerank ì™„ë£Œ",
                extra={
                    "query": query[:50],
                    "result_count": len(final_results),
                    "reranked": rerank_enabled and self.reranker is not None
                }
            )

            return final_results

        except Exception as e:
            logger.error(
                f"search_and_rerank ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬: {e}, ë¹ˆ ê²°ê³¼ ë°˜í™˜",
                exc_info=True,
                extra={"query": query[:100]}
            )
            # ì˜ˆìƒì¹˜ ëª»í•œ ì—ëŸ¬ ë°œìƒ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜ (ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ë°©ì§€)
            return []

    async def _rerank_only(
        self,
        query: str,
        results: list[SearchResult],
        top_k: int = 15,
    ) -> list[SearchResult]:
        """
        ë¦¬ë­í‚¹ë§Œ ìˆ˜í–‰í•˜ëŠ” ë‚´ë¶€ ë©”ì„œë“œ (ê²€ìƒ‰ ì—†ìŒ)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            results: ê²€ìƒ‰ ê²°ê³¼
            top_k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜

        Returns:
            ë¦¬ë­í‚¹ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        if not self.reranker:
            logger.warning("Rerankerê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì›ë³¸ ê²°ê³¼ ë°˜í™˜")
            return results

        try:
            reranked = await self.reranker.rerank(query, results, top_k)
            self.stats["rerank_count"] += 1
            return reranked

        except Exception as e:
            logger.error(
                f"ë¦¬ë­í‚¹ ì‹¤íŒ¨: {e}, ì›ë³¸ ê²°ê³¼ ë°˜í™˜",
                exc_info=True,
                extra={"query": query[:100]}
            )
            return results

    async def health_check(self) -> HealthCheckDict:
        """
        ëª¨ë“  êµ¬ì„±ìš”ì†Œì˜ í—¬ìŠ¤ ì²´í¬

        Returns:
            ê° êµ¬ì„±ìš”ì†Œì˜ ìƒíƒœ ë”•ì…”ë„ˆë¦¬
        """
        health: HealthCheckDict = {}

        try:
            # Retriever í—¬ìŠ¤ ì²´í¬
            if hasattr(self.retriever, "health_check"):
                health["retriever"] = await self.retriever.health_check()
            else:
                health["retriever"] = True

            # Reranker í—¬ìŠ¤ ì²´í¬
            if self.reranker:
                if hasattr(self.reranker, "health_check"):
                    health["reranker"] = await self.reranker.health_check()
                else:
                    health["reranker"] = True
            else:
                health["reranker"] = None  # Not configured

            # Cache í—¬ìŠ¤ ì²´í¬
            if self.cache:
                if hasattr(self.cache, "health_check"):
                    health["cache"] = await self.cache.health_check()
                else:
                    health["cache"] = True
            else:
                health["cache"] = None  # Not configured

        except Exception as e:
            logger.error(
                "í—¬ìŠ¤ ì²´í¬ ì‹¤íŒ¨",
                extra={"error": str(e)},
                exc_info=True
            )
            health["error"] = str(e)

        return health

    def get_stats(self) -> OrchestratorStatsDict:  # type: ignore[return-value]
        """
        ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° ë° ëª¨ë“  êµ¬ì„±ìš”ì†Œì˜ í†µê³„ ë°˜í™˜

        Returns:
            í†µí•© í†µê³„ ë”•ì…”ë„ˆë¦¬
        """
        stats = {
            "orchestrator": {
                "total_requests": self.stats["total_requests"],
                "cache_hits": self.stats["cache_hits"],
                "cache_misses": self.stats["cache_misses"],
                "retrieval_count": self.stats["retrieval_count"],
                "rerank_count": self.stats["rerank_count"],
                "query_expansion_count": self.stats["query_expansion_count"],
                "cache_hit_rate": (
                    self.stats["cache_hits"] / self.stats["total_requests"] * 100
                    if self.stats["total_requests"] > 0
                    else 0.0
                ),
            }
        }

        # Retriever í†µê³„
        if hasattr(self.retriever, "get_stats"):
            stats["retriever"] = self.retriever.get_stats()

        # Reranker í†µê³„
        if self.reranker and hasattr(self.reranker, "get_stats"):
            stats["reranker"] = self.reranker.get_stats()

        # Cache í†µê³„
        if self.cache and hasattr(self.cache, "get_stats"):
            stats["cache"] = self.cache.get_stats()

        # Query Expansion í†µê³„
        if self.query_expansion and hasattr(self.query_expansion, "get_stats"):
            stats["query_expansion"] = self.query_expansion.get_stats()

        return stats  # type: ignore[return-value]

    # ========================================
    # ë ˆê±°ì‹œ í˜¸í™˜ì„± ì–´ëŒ‘í„° ë©”ì„œë“œ (Backward Compatibility)
    # ========================================

    async def search(self, query: str, options: dict[str, Any] | None = None) -> list[SearchResult]:
        """
        ë ˆê±°ì‹œ RetrievalModule.search() í˜¸í™˜ ì–´ëŒ‘í„°

        RAGPipelineì—ì„œ ì‚¬ìš©í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤:
        retrieval_module.search(query, options)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            options: ê²€ìƒ‰ ì˜µì…˜ ë”•ì…”ë„ˆë¦¬
                - limit: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜ (ê¸°ë³¸ê°’: 15)
                - min_score: ìµœì†Œ ì ìˆ˜ ì„ê³„ê°’ (ê¸°ë³¸ê°’: 0.5) - í˜„ì¬ ë¬´ì‹œë¨
                - context: ì„¸ì…˜ ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì )

        Returns:
            ê²€ìƒ‰ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸ (SearchResult í˜•ì‹)

        Note:
            search_and_rerank()ë¡œ ìœ„ì„í•˜ë˜, ë¦¬ë­í‚¹ì€ ë¹„í™œì„±í™”
            (ë¦¬ë­í‚¹ì€ ë³„ë„ rerank() ë©”ì„œë“œë¡œ í˜¸ì¶œë¨)
        """
        options = options or {}
        top_k = options.get("limit", 15)
        # min_scoreëŠ” í˜„ì¬ Orchestratorì—ì„œ ì§€ì›í•˜ì§€ ì•ŠìŒ (í–¥í›„ ì¶”ê°€ ê°€ëŠ¥)
        # contextëŠ” query_expansionì—ì„œ ì‚¬ìš© ê°€ëŠ¥

        logger.debug(
            "[Adapter] search() í˜¸ì¶œ",
            extra={"query": query[:50], "top_k": top_k}
        )

        # search_and_rerank() í˜¸ì¶œ (ë¦¬ë­í‚¹ ë¹„í™œì„±í™”)
        results = await self.search_and_rerank(
            query=query,
            top_k=top_k,
            rerank_enabled=False,  # ë¦¬ë­í‚¹ì€ ë³„ë„ rerank() ë©”ì„œë“œì—ì„œ ìˆ˜í–‰
            query_expansion_enabled=self.config.get("query_expansion_enabled", True),
        )

        logger.debug(
            "[Adapter] search() ì™„ë£Œ",
            extra={"result_count": len(results)}
        )
        return results

    async def rerank(
        self, query: str, results: list[SearchResult], top_n: int | None = None
    ) -> list[SearchResult]:
        """
        ë ˆê±°ì‹œ RetrievalModule.rerank() í˜¸í™˜ ì–´ëŒ‘í„°

        RAGPipelineì—ì„œ ì‚¬ìš©í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤:
        retrieval_module.rerank(query, results, top_n)

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            results: ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            top_n: ë°˜í™˜í•  ìƒìœ„ Nê°œ ê²°ê³¼ (Noneì´ë©´ ëª¨ë“  ê²°ê³¼ ë°˜í™˜)

        Returns:
            ë¦¬ë­í‚¹ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Note:
            _rerank_results() ë‚´ë¶€ ë©”ì„œë“œë¡œ ìœ„ì„
        """
        if not results:
            logger.debug("[Adapter] rerank() í˜¸ì¶œ: ê²°ê³¼ ì—†ìŒ")
            return []

        logger.debug(
            "[Adapter] rerank() í˜¸ì¶œ",
            extra={
                "query": query[:50],
                "result_count": len(results),
                "top_n": top_n
            }
        )

        # ë‚´ë¶€ ë¦¬ë­í‚¹ ë©”ì„œë“œ í˜¸ì¶œ
        reranked = await self._rerank_only(
            query=query, results=results, top_k=top_n if top_n else 15
        )

        logger.debug(
            "[Adapter] rerank() ì™„ë£Œ",
            extra={"result_count": len(reranked)}
        )
        return reranked

    async def add_documents(self, documents: list[dict[str, Any]]) -> dict[str, Any]:
        """
        ë ˆê±°ì‹œ RetrievalModule.add_documents() í˜¸í™˜ ì–´ëŒ‘í„°

        upload.pyì—ì„œ ì‚¬ìš©í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤:
        retrieval_module.add_documents(embedded_chunks)

        Args:
            documents: ì—…ë¡œë“œí•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
                ê° ë¬¸ì„œëŠ” ë‹¤ìŒ êµ¬ì¡°ë¥¼ ê°€ì ¸ì•¼ í•¨:
                {
                    "content": str,           # í•„ìˆ˜: ë¬¸ì„œ ë‚´ìš©
                    "embedding": list[float], # í•„ìˆ˜: ì„ë² ë”© ë²¡í„°
                    "metadata": dict,         # ì„ íƒ: ë©”íƒ€ë°ì´í„°
                }

        Returns:
            ì—…ë¡œë“œ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬:
            {
                "success_count": int,
                "error_count": int,
                "total_count": int,
                "errors": list[str],
            }

        Note:
            WeaviateRetriever.add_documents()ë¡œ ìœ„ì„
        """
        if not hasattr(self.retriever, "add_documents"):
            raise NotImplementedError(
                f"Retriever {type(self.retriever).__name__}ëŠ” add_documentsë¥¼ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            )

        logger.debug(
            "[Adapter] add_documents() í˜¸ì¶œ",
            extra={"document_count": len(documents)}
        )

        # WeaviateRetriever.add_documents()ë¡œ ìœ„ì„
        result = await self.retriever.add_documents(documents)  # type: ignore[no-any-return]

        logger.debug(
            f"[Adapter] add_documents() ì™„ë£Œ: "
            f"ì„±ê³µ {result['success_count']}ê°œ, ì‹¤íŒ¨ {result['error_count']}ê°œ"
        )

        return result  # type: ignore[no-any-return]

    # ========== ë‚´ë¶€ í—¬í¼ ë©”ì„œë“œ ==========

    async def _search_and_merge(
        self,
        queries: list[str],
        top_k: int,
        filters: dict[str, Any] | None = None,
        weights: list[float] | None = None,
        use_rrf: bool = True,
    ) -> list[SearchResult]:
        """
        ë‹¤ì¤‘ ì¿¼ë¦¬ ë³‘ë ¬ ê²€ìƒ‰ ë° RRF ê¸°ë°˜ ê²°ê³¼ ë³‘í•©

        **RRF (Reciprocal Rank Fusion) ì•Œê³ ë¦¬ì¦˜**:
        ê° ì¿¼ë¦¬ ê²°ê³¼ì—ì„œ ë¬¸ì„œì˜ ìˆœìœ„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ì—¬ í†µí•©í•©ë‹ˆë‹¤.

        Score(doc) = Î£ [weight_i / (k + rank_i)]
        - k: RRF ìƒìˆ˜ (ê¸°ë³¸ê°’ 60)
        - rank_i: ië²ˆì§¸ ì¿¼ë¦¬ ê²°ê³¼ì—ì„œì˜ ìˆœìœ„ (0-based)
        - weight_i: ië²ˆì§¸ ì¿¼ë¦¬ì˜ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’ 1.0)

        **ì¥ì **:
        - ë‹¤ì–‘í•œ ì¿¼ë¦¬ ê´€ì ì˜ ê²°ê³¼ í†µí•©
        - ì—¬ëŸ¬ ì¿¼ë¦¬ì—ì„œ ê³µí†µìœ¼ë¡œ ìƒìœ„ ë­í¬ëœ ë¬¸ì„œ ìš°ëŒ€
        - ê°œë³„ ì ìˆ˜ë³´ë‹¤ ìˆœìœ„ ê¸°ë°˜ìœ¼ë¡œ ê³µì •í•œ í†µí•©

        **ìŠ¤ì½”ì–´ë§ (Scoring)**:
        ScoringServiceë¥¼ í†µí•´ ì„¤ì • ê¸°ë°˜ ê°€ì¤‘ì¹˜ê°€ ì ìš©ë©ë‹ˆë‹¤.
        - collection_weight_enabled: ì»¬ë ‰ì…˜ë³„ ê°€ì¤‘ì¹˜
        - file_type_weight_enabled: íŒŒì¼ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜

        Args:
            queries: ê²€ìƒ‰í•  ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸
            top_k: ìµœì¢… ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            filters: ê²€ìƒ‰ í•„í„°
            weights: ê° ì¿¼ë¦¬ì˜ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ê°’: ëª¨ë‘ 1.0)
            use_rrf: RRF ì‚¬ìš© ì—¬ë¶€ (Falseë©´ ë‹¨ìˆœ ì ìˆ˜ ë³‘í•©)

        Returns:
            RRF ì ìˆ˜ë¡œ ì •ë ¬ëœ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

        Example:
            queries = ["ë¶€ì‚° ì£¼ë¯¼ë“±ë¡ ë°œê¸‰", "ë¶€ì‚°ì‹œ ë“±ë³¸ ì‹ ì²­", "ì£¼ë¯¼ë“±ë¡ ì˜¨ë¼ì¸"]
            weights = [1.0, 0.8, 0.6]
            results = await _search_and_merge(queries, 15, weights=weights)
        """
        import asyncio

        # ê°€ì¤‘ì¹˜ ê¸°ë³¸ê°’ ì„¤ì •
        if weights is None:
            weights = [1.0] * len(queries)
        elif len(weights) != len(queries):
            logger.warning(
                f"ì¿¼ë¦¬ ìˆ˜({len(queries)})ì™€ ê°€ì¤‘ì¹˜ ìˆ˜({len(weights)}) ë¶ˆì¼ì¹˜, "
                f"ê°€ì¤‘ì¹˜ë¥¼ 1.0ìœ¼ë¡œ íŒ¨ë”©"
            )
            weights = weights + [1.0] * (len(queries) - len(weights))

        # ëª¨ë“  ì¿¼ë¦¬ë¥¼ ë³‘ë ¬ë¡œ ê²€ìƒ‰ (ê°ê° top_k*2ê°œ ê²€ìƒ‰)
        # top_k*2ë¡œ ê²€ìƒ‰í•˜ëŠ” ì´ìœ : RRF í†µí•© ì‹œ ë” ë§ì€ í›„ë³´ í™•ë³´
        search_top_k = top_k * 2
        search_tasks = [self.retriever.search(q, search_top_k, filters) for q in queries]

        logger.info(
            "Multi-Query ë³‘ë ¬ ê²€ìƒ‰ ì‹œì‘",
            extra={
                "query_count": len(queries),
                "search_top_k": search_top_k,
                "rrf": "í™œì„±í™”" if use_rrf else "ë¹„í™œì„±í™”"
            }
        )

        start_time = asyncio.get_event_loop().time()
        results_per_query = await asyncio.gather(*search_tasks, return_exceptions=True)
        search_time = (asyncio.get_event_loop().time() - start_time) * 1000

        logger.info(
            "ë³‘ë ¬ ê²€ìƒ‰ ì™„ë£Œ",
            extra={"search_time_ms": search_time}
        )

        # RRF ë˜ëŠ” ë‹¨ìˆœ ë³‘í•©
        if use_rrf:
            merged_results = self._rrf_merge(
                results_per_query, queries, weights, top_k
            )
        else:
            merged_results = self._simple_merge(results_per_query, queries, top_k)

        logger.info(
            "ê²°ê³¼ ë³‘í•© ì™„ë£Œ",
            extra={
                "merged_count": len(merged_results),
                "search_time_ms": search_time
            }
        )

        # TXT íŒŒì¼ ë‹¤ì–‘ì„± ì œí•œ ì ìš© (ìµœëŒ€ 15ê°œ)
        # RAGPipelineì—ì„œ _search_and_mergeë¥¼ ì§ì ‘ í˜¸ì¶œí•˜ëŠ” ê²½ìš°ì—ë„ ì œí•œì´ ì ìš©ë˜ë„ë¡ í•¨
        merged_results = self._apply_txt_limit(merged_results)

        return merged_results

    def _rrf_merge(
        self,
        results_per_query: list[
            list[SearchResult] | BaseException
        ],  # asyncio.gather with return_exceptions=True
        queries: list[str],
        weights: list[float],
        top_k: int,
        rrf_k: int = 60,
    ) -> list[SearchResult]:
        """
        RRF (Reciprocal Rank Fusion) ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê²°ê³¼ í†µí•©

        ìŠ¤ì½”ì–´ë§:
            ScoringServiceë¥¼ í†µí•´ ì„¤ì • ê¸°ë°˜ ê°€ì¤‘ì¹˜ê°€ ì ìš©ë©ë‹ˆë‹¤.
            - collection_weight_enabled: ì»¬ë ‰ì…˜ë³„ ê°€ì¤‘ì¹˜
            - file_type_weight_enabled: íŒŒì¼ íƒ€ì…ë³„ ê°€ì¤‘ì¹˜
            ê¸°ë³¸ê°’ì€ ëª¨ë‘ ë¹„í™œì„±í™” (ìˆœìˆ˜ RRF ì ìˆ˜ ë°˜í™˜)

        Args:
            results_per_query: ê° ì¿¼ë¦¬ì˜ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            queries: ì¿¼ë¦¬ ë¦¬ìŠ¤íŠ¸ (ë¡œê¹…ìš©)
            weights: ê° ì¿¼ë¦¬ì˜ ê°€ì¤‘ì¹˜
            top_k: ìµœì¢… ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            rrf_k: RRF ìƒìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ 60)

        Returns:
            RRF ì ìˆ˜ë¡œ ì •ë ¬ëœ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        # ë¬¸ì„œë³„ RRF ì ìˆ˜ ê³„ì‚°
        doc_scores: dict[str, float] = {}  # {doc_id: rrf_score}
        doc_objects: dict[str, SearchResult] = {}  # {doc_id: SearchResult}
        doc_appearances: dict[str, int] = {}  # {doc_id: ë“±ì¥ íšŸìˆ˜}

        for query_idx, results in enumerate(results_per_query):
            if isinstance(results, BaseException):  # asyncio.gather with return_exceptions=True
                logger.warning(
                    "ì¿¼ë¦¬ ì‹¤íŒ¨",
                    extra={
                        "query_index": query_idx + 1,
                        "total_queries": len(queries),
                        "error": str(results)
                    },
                    exc_info=True
                )
                continue

            weight = weights[query_idx]

            for rank, result in enumerate(results):
                # ë¬¸ì„œ ID ì¶”ì¶œ
                doc_id = self._get_doc_id(result)

                if not doc_id:
                    continue

                # RRF ì ìˆ˜ ê³„ì‚°: weight / (k + rank)
                rrf_score = weight / (rrf_k + rank)

                # ì ìˆ˜ ëˆ„ì 
                doc_scores[doc_id] = doc_scores.get(doc_id, 0.0) + rrf_score

                # ë¬¸ì„œ ê°ì²´ ì €ì¥ (ì²« ë“±ì¥ ì‹œ)
                if doc_id not in doc_objects:
                    doc_objects[doc_id] = result

                # ë“±ì¥ íšŸìˆ˜ ì¹´ìš´íŠ¸
                doc_appearances[doc_id] = doc_appearances.get(doc_id, 0) + 1

        # ì„¤ì • ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš© (ScoringService ì‚¬ìš©)
        # ê¸°ë³¸ê°’: ë¹„í™œì„±í™” â†’ ìˆœìˆ˜ RRF ì ìˆ˜ ë°˜í™˜ (Blank System ì›ì¹™)
        scoring_active = (
            self.scoring_service.collection_weight_enabled or
            self.scoring_service.file_type_weight_enabled
        )

        if scoring_active:
            logger.info(
                "ScoringService ê°€ì¤‘ì¹˜ ì ìš© ì‹œì‘",
                extra={"document_count": len(doc_objects)}
            )
            weight_applied_count = 0

            for doc_id, result in doc_objects.items():
                original_score = doc_scores[doc_id]

                # ë©”íƒ€ë°ì´í„°ì—ì„œ ì»¬ë ‰ì…˜ê³¼ íŒŒì¼íƒ€ì… ì¶”ì¶œ
                metadata = result.metadata or {}
                collection = metadata.get("_collection", "Documents")
                file_type = metadata.get("file_type", "")

                # ScoringServiceë¥¼ í†µí•œ ê°€ì¤‘ì¹˜ ì ìš©
                adjusted_score = self.scoring_service.apply_weight(
                    score=original_score,
                    collection=collection,
                    file_type=file_type,
                )

                # ì ìˆ˜ê°€ ë³€ê²½ëœ ê²½ìš° ë©”íƒ€ë°ì´í„°ì— ê¸°ë¡
                if adjusted_score != original_score:
                    result.metadata = metadata
                    result.metadata["_score_before_weight"] = original_score
                    weight_applied_count += 1

                doc_scores[doc_id] = adjusted_score

            logger.info(
                "ê°€ì¤‘ì¹˜ ì ìš© ì™„ë£Œ",
                extra={"weighted_documents": weight_applied_count}
            )

        # RRF ì ìˆ˜ë¡œ ì •ë ¬ (ê°€ì¤‘ì¹˜ ì ìš© í›„)
        sorted_doc_ids = sorted(
            doc_scores.keys(), key=lambda doc_id: doc_scores[doc_id], reverse=True
        )

        # SearchResult ê°ì²´ì— RRF ì ìˆ˜ ì ìš©
        merged_results = []
        for doc_id in sorted_doc_ids[:top_k]:
            result = doc_objects[doc_id]
            rrf_score = doc_scores[doc_id]

            # ì›ë³¸ ì ìˆ˜ ìœ ì§€í•˜ë©´ì„œ RRF ì ìˆ˜ ì¶”ê°€
            if hasattr(result, "score"):
                result.metadata = result.metadata or {}
                result.metadata["original_score"] = result.score
                result.metadata["rrf_score"] = rrf_score
                result.metadata["query_appearances"] = doc_appearances[doc_id]
                result.score = rrf_score  # RRF ì ìˆ˜ë¡œ êµì²´
            elif isinstance(result, dict):
                result["metadata"] = result.get("metadata", {})
                result["metadata"]["original_score"] = result.get("score", 0.0)
                result["metadata"]["rrf_score"] = rrf_score
                result["metadata"]["query_appearances"] = doc_appearances[doc_id]
                result["score"] = rrf_score

            merged_results.append(result)

        if len(doc_appearances) > 0:
            avg_appearances = sum(doc_appearances.values()) / len(doc_appearances)
            logger.info(
                "RRF ë³‘í•© ì™„ë£Œ",
                extra={
                    "merged_count": len(merged_results),
                    "avg_appearances": avg_appearances
                }
            )
        else:
            logger.info(
                "RRF ë³‘í•© ì™„ë£Œ",
                extra={"merged_count": len(merged_results)}
            )

        # ìƒìœ„ ê²°ê³¼ ë¡œê·¸ (ë””ë²„ê¹…ìš©, ê°€ì¤‘ì¹˜ ì ìš© ì‹œ)
        if merged_results and scoring_active:
            top3 = merged_results[:3]
            top3_info = [
                {
                    "collection": r.metadata.get('_collection', 'unknown'),
                    "score": r.score
                }
                for r in top3
            ]
            logger.info(
                "ìƒìœ„ 3ê°œ ê²°ê³¼",
                extra={"top_results": top3_info}
            )

        return merged_results

    def _simple_merge(
        self,
        results_per_query: list[
            list[SearchResult] | BaseException
        ],  # asyncio.gather with return_exceptions=True
        queries: list[str],
        top_k: int,
    ) -> list[SearchResult]:
        """
        ë‹¨ìˆœ ë³‘í•© (ì¤‘ë³µ ì œê±° + ì ìˆ˜ìˆœ ì •ë ¬)

        RRFë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì˜ í´ë°± ë¡œì§
        """
        merged_results = []
        seen_ids = set()

        for i, results in enumerate(results_per_query):
            if isinstance(results, BaseException):  # asyncio.gather with return_exceptions=True
                logger.warning(
                    "ì¿¼ë¦¬ ê²€ìƒ‰ ì‹¤íŒ¨",
                    extra={
                        "query_index": i + 1,
                        "total_queries": len(queries),
                        "error": str(results)
                    },
                    exc_info=True
                )
                continue

            for result in results:
                doc_id = self._get_doc_id(result)

                if doc_id and doc_id not in seen_ids:
                    seen_ids.add(doc_id)
                    merged_results.append(result)

        # ì›ë³¸ ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        merged_results.sort(
            key=lambda x: getattr(x, "score", 0.0),  # SearchResultëŠ” dataclass, .get() ë¶ˆí•„ìš”
            reverse=True,
        )

        return merged_results[:top_k]

    def _get_doc_id(self, result: SearchResult | dict) -> str | None:
        """
        SearchResultì—ì„œ ë¬¸ì„œ ID ì¶”ì¶œ

        Args:
            result: SearchResult ê°ì²´ ë˜ëŠ” dict

        Returns:
            ë¬¸ì„œ ID ë˜ëŠ” None
        """
        if hasattr(result, "id"):
            return result.id
        elif isinstance(result, dict):
            return result.get("id")

        # IDê°€ ì—†ëŠ” ê²½ìš° content ê¸°ë°˜ í•´ì‹œ ìƒì„±
        if hasattr(result, "content"):
            return str(hash(result.content))
        elif isinstance(result, dict) and "content" in result:
            return str(hash(result["content"]))

        return None
