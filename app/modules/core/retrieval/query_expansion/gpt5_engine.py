"""
GPT-5-nano ê¸°ë°˜ ì§€ëŠ¥í˜• ì¿¼ë¦¬ í™•ì¥ ì—”ì§„

Phase 1.4: ë ˆê±°ì‹œ ì˜ì¡´ì„± ì™„ì „ ì œê±°
Phase 2.0: OpenAI ì§ì ‘ í˜¸ì¶œ ì œê±°, llm_factory í•„ìˆ˜í™”

- LLM Factoryë¥¼ í†µí•œ Multi-LLM fallback ì§€ì›
- ë ˆê±°ì‹œ OpenAI ì§ì ‘ í˜¸ì¶œ ì½”ë“œ ì œê±°
- IQueryExpansionEngine ì¸í„°í˜ì´ìŠ¤ ì¤€ìˆ˜
"""

import asyncio
import json
import re
from typing import Any, TypedDict, cast

import structlog
from cachetools import TTLCache

from app.lib.circuit_breaker import (
    CircuitBreakerConfig,
    CircuitBreakerFactory,
)

from .interface import (
    ExpandedQuery,
    IQueryExpansionEngine,
    QueryComplexity,
    SearchIntent,
)

logger = structlog.get_logger(__name__)


class Stats(TypedDict):
    """GPT5QueryExpansionEngine ì„±ëŠ¥ í†µê³„ íƒ€ì…"""

    total_expansions: int
    successful_expansions: int
    cache_hits: int
    cache_misses: int
    average_response_time: float
    complexity_distribution: dict[str, int]
    intent_distribution: dict[str, int]
    gpt5_api_calls: int
    json_parse_failures: int


class GPT5QueryExpansionEngine(IQueryExpansionEngine):
    """
    GPT-5-nano ê¸°ë°˜ Query Expansion Engine

    **Phase 1.4 ë¦¬íŒ©í† ë§**: ë ˆê±°ì‹œ ì˜ì¡´ì„± ì™„ì „ ì œê±°
    - ê¸°ì¡´ documents/query_expansion.pyì˜ ê²€ì¦ëœ ë¡œì§ ì¬í™œìš©
    - GPT-5-nano API í˜¸ì¶œ, 3ë‹¨ê³„ JSON íŒŒì‹±, TTL ìºì‹± í¬í•¨
    - IQueryExpansionEngine ì¸í„°í˜ì´ìŠ¤ ì¤€ìˆ˜

    Features:
    - GPT-5-nano ê¸°ë°˜ ë‹¤ì¤‘ ì¿¼ë¦¬ ìƒì„± (ê¸°ë³¸ 5ê°œ)
    - TTLCache (1000ê°œ, 86400ì´ˆ = 1ì¼)
    - 3ë‹¨ê³„ JSON íŒŒì‹± (ì •ìƒ â†’ ì½”ë“œë¸”ë¡ â†’ ìˆ˜ë™ ì¶”ì¶œ)
    - ë³µì¡ë„ ë° ì˜ë„ ìë™ ë¶„ì„
    - Circuit Breaker ë³´í˜¸
    - LLM Factory ì§€ì› (Multi-LLM fallback)
    """

    def __init__(
        self,
        api_key: str = "",  # ë ˆê±°ì‹œ í˜¸í™˜, ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
        num_expansions: int = 5,
        max_tokens: int = 500,
        temperature: float = 0.7,
        cache_size: int = 1000,
        cache_ttl: int = 86400,  # 1ì¼
        llm_factory: Any = None,  # í•„ìˆ˜ (Noneì´ë©´ ì—ëŸ¬)
        provider: str = "google",  # LLM Factoryì˜ ì„ í˜¸ ì œê³µì (google = Gemini Flash)
        circuit_breaker_factory: CircuitBreakerFactory | None = None,
    ):
        """
        Args:
            api_key: (Deprecated) ì‚¬ìš©ë˜ì§€ ì•ŠìŒ, í•˜ìœ„ í˜¸í™˜ì„± ìœ ì§€ìš©
            num_expansions: ìƒì„±í•  í™•ì¥ ì¿¼ë¦¬ ìˆ˜
            max_tokens: LLM ì‘ë‹µ ìµœëŒ€ í† í°
            temperature: ìƒì„± ì˜¨ë„ (0.0-1.0)
            cache_size: ìºì‹œ ìµœëŒ€ í¬ê¸°
            cache_ttl: ìºì‹œ ìœ íš¨ ì‹œê°„ (ì´ˆ)
            llm_factory: LLM Factory ì¸ìŠ¤í„´ìŠ¤ (í•„ìˆ˜)
            provider: LLM Factoryì˜ ì„ í˜¸ ì œê³µì (google, openai, anthropic)
            circuit_breaker_factory: DI Containerì˜ CircuitBreaker íŒ©í† ë¦¬

        Raises:
            ValueError: llm_factoryê°€ Noneì¸ ê²½ìš°
        """
        # llm_factory í•„ìˆ˜ ê²€ì¦
        if llm_factory is None:
            raise ValueError(
                "llm_factoryëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. "
                "DI Containerì˜ AppContainer.llm_factory()ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
            )

        self.num_expansions = num_expansions
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.llm_factory = llm_factory
        self.provider = provider  # ì„ í˜¸ ì œê³µì ì €ì¥
        self.circuit_breaker_factory = circuit_breaker_factory

        # TTL ìºì‹œ ì´ˆê¸°í™”
        self.expansion_cache: TTLCache[str, ExpandedQuery] = TTLCache(
            maxsize=cache_size, ttl=cache_ttl
        )
        logger.info(
            "ì¿¼ë¦¬ í™•ì¥ ìºì‹œ ì´ˆê¸°í™”",
            maxsize=cache_size,
            ttl_seconds=cache_ttl,
        )

        # ì„±ëŠ¥ í†µê³„
        self.stats: Stats = {
            "total_expansions": 0,
            "successful_expansions": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "average_response_time": 0.0,
            "complexity_distribution": {c.value: 0 for c in QueryComplexity},
            "intent_distribution": {i.value: 0 for i in SearchIntent},
            "gpt5_api_calls": 0,
            "json_parse_failures": 0,
        }

        # GPT-5-nanoìš© ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸
        self.expansion_prompt = self._create_expansion_prompt()

    def _create_expansion_prompt(self) -> str:
        """GPT-5-nanoìš© ìµœì í™”ëœ ì¿¼ë¦¬ í™•ì¥ í”„ë¡¬í”„íŠ¸"""
        return """ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ í™•ì¥ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ì‚¬ìš©ì ì¿¼ë¦¬ë¥¼ ë¶„ì„í•˜ê³  ê²€ìƒ‰ íš¨ìœ¨ì„±ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•´ í™•ì¥í•´ì£¼ì„¸ìš”.

**ë¶„ì„ ìš”êµ¬ì‚¬í•­:**
1. ë™ì˜ì–´ ë° ìœ ì‚¬ì–´ ë°œêµ´ (í•œêµ­ì–´ íŠ¹ì„± ê³ ë ¤)
2. í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜ ë¶€ì—¬ (0.1-1.0)
3. ê²€ìƒ‰ ì˜ë„ ë¶„ë¥˜ (factual/procedural/conceptual/comparative/problem_solving)
4. ì¿¼ë¦¬ ë³µì¡ë„ í‰ê°€ (simple/medium/complex/contextual)
5. ë‹¤ì–‘í•œ ê´€ì ì˜ í™•ì¥ ì¿¼ë¦¬ ìƒì„± (ê° ì¿¼ë¦¬ë³„ ê°€ì¤‘ì¹˜ í¬í•¨)

**ì‘ë‹µ í˜•ì‹:** ìˆœìˆ˜ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡(```)ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.

{{
  "original_query": "ì›ë³¸ ì¿¼ë¦¬",
  "synonyms": ["ë™ì˜ì–´1", "ë™ì˜ì–´2", "ë™ì˜ì–´3"],
  "related_terms": ["ê´€ë ¨ìš©ì–´1", "ê´€ë ¨ìš©ì–´2", "ê´€ë ¨ìš©ì–´3"],
  "core_keywords": [
    {{"keyword": "í•µì‹¬í‚¤ì›Œë“œ1", "weight": 0.9}},
    {{"keyword": "í•µì‹¬í‚¤ì›Œë“œ2", "weight": 0.7}},
    {{"keyword": "í•µì‹¬í‚¤ì›Œë“œ3", "weight": 0.5}}
  ],
  "intent": "factual",
  "complexity": "medium",
  "expanded_queries": [
    {{"query": "í™•ì¥ì¿¼ë¦¬1", "weight": 1.0, "focus": "ì£¼ìš”_ê´€ì "}},
    {{"query": "í™•ì¥ì¿¼ë¦¬2", "weight": 0.8, "focus": "ë³´ì¡°_ê´€ì "}},
    {{"query": "í™•ì¥ì¿¼ë¦¬3", "weight": 0.6, "focus": "ì„¸ë¶€_ê´€ì "}}
  ],
  "search_strategy": "hybrid"
}}

**ì‚¬ìš©ì ì¿¼ë¦¬:** {query}

**ì¤‘ìš”:**
- ìˆœìˆ˜ JSONë§Œ ë°˜í™˜ (ì½”ë“œ ë¸”ë¡ ``` ê¸ˆì§€)
- intent ê°’: factual, procedural, conceptual, comparative, problem_solving ì¤‘ í•˜ë‚˜
- complexity ê°’: simple, medium, complex, contextual ì¤‘ í•˜ë‚˜
- search_strategy ê°’: broad, focused, hybrid, contextual ì¤‘ í•˜ë‚˜
- weightëŠ” 0.1-1.0 ë²”ìœ„ì˜ ì‹¤ìˆ˜
- ê²€ìƒ‰ íš¨ìœ¨ì„±ì— ì§‘ì¤‘"""

    async def expand_query(
        self, query: str, context: list[dict] | None = None
    ) -> ExpandedQuery | None:
        """
        ì¿¼ë¦¬ í™•ì¥ ë©”ì¸ ë©”ì„œë“œ (ìºì‹± ì ìš©)

        Args:
            query: ì›ë³¸ ì‚¬ìš©ì ì¿¼ë¦¬
            context: ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (ì„ íƒì , í–¥í›„ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í™•ì¥ì— ì‚¬ìš© ê°€ëŠ¥)

        Returns:
            ExpandedQuery ê°ì²´ ë˜ëŠ” None (ì‹¤íŒ¨ì‹œ)
        """
        start_time = asyncio.get_event_loop().time()
        self.stats["total_expansions"] += 1

        # ìºì‹œ í‚¤ ìƒì„± (ì¿¼ë¦¬ ì •ê·œí™”)
        cache_key = query.strip().lower()

        # ìºì‹œ í™•ì¸
        if cache_key in self.expansion_cache:
            self.stats["cache_hits"] += 1
            cached_result = self.expansion_cache[cache_key]
            elapsed = asyncio.get_event_loop().time() - start_time
            logger.info(
                "âœ… ìºì‹œ íˆíŠ¸!",
                query=query[:50],
                elapsed_ms=f"{elapsed*1000:.1f}",
                cache_hit_rate=f"{self.stats['cache_hits']/self.stats['total_expansions']*100:.1f}%",
            )
            return cached_result

        # ìºì‹œ ë¯¸ìŠ¤
        self.stats["cache_misses"] += 1
        logger.debug("âŒ ìºì‹œ ë¯¸ìŠ¤, ì¿¼ë¦¬ í™•ì¥ ê²€í†  ì‹œì‘", query=query[:50])

        try:
            # 1. ì‚¬ì „ í•„í„°ë§ - ê°„ë‹¨í•œ ì¿¼ë¦¬ëŠ” ë¹ ë¥¸ ì²˜ë¦¬
            if self._is_simple_query(query):
                logger.info("âœ… ë‹¨ìˆœ ì¿¼ë¦¬ë¡œ íŒë‹¨, GPT-5-nano í˜¸ì¶œ ìƒëµ", query=query[:50])
                result = self._create_simple_expansion(query)
                self.expansion_cache[cache_key] = result
                return result

            # 2. GPT-5-nano í˜¸ì¶œ
            logger.info("ğŸ”„ ë³µì¡í•œ ì¿¼ë¦¬ë¡œ íŒë‹¨, GPT-5-nano í˜¸ì¶œ ì‹œì‘", query=query[:50])
            logger.debug("GPT-5-nano ì¿¼ë¦¬ í™•ì¥ ì‹œì‘", query=query)
            expanded_data = await self._call_gpt5_nano(query)

            if not expanded_data:
                logger.warning("GPT-5 í™•ì¥ ì‹¤íŒ¨, í´ë°± ì‚¬ìš©", query=query)
                return self._create_fallback_expansion(query)

            # 3. êµ¬ì¡°í™”ëœ ê°ì²´ ìƒì„±
            expanded_query = self._parse_expansion_result(expanded_data, query)

            # 4. í†µê³„ ì—…ë°ì´íŠ¸
            processing_time = asyncio.get_event_loop().time() - start_time
            self._update_stats(expanded_query, processing_time)

            # 5. ìºì‹œì— ì €ì¥
            self.expansion_cache[cache_key] = expanded_query

            logger.info(
                "ì¿¼ë¦¬ í™•ì¥ ì™„ë£Œ (ìºì‹œ ì €ì¥)",
                expanded_count=len(expanded_query.expansions),
                intent=expanded_query.intent.value,
                complexity=expanded_query.complexity.value,
                processing_time_ms=f"{processing_time*1000:.1f}",
            )

            return expanded_query

        except Exception as e:
            logger.error("ì¿¼ë¦¬ í™•ì¥ ì˜¤ë¥˜", error=str(e))
            # í´ë°± ê²°ê³¼ë„ ìºì‹± (ì‹¤íŒ¨ ë°˜ë³µ ë°©ì§€)
            fallback_result = self._create_fallback_expansion(query)
            self.expansion_cache[cache_key] = fallback_result
            return fallback_result

    def _is_simple_query(self, query: str) -> bool:
        """ê°„ë‹¨í•œ ì¿¼ë¦¬ íŒë³„ ë¡œì§"""
        # ì§§ì€ í‚¤ì›Œë“œì„± ì¿¼ë¦¬
        if len(query.strip()) < 10:
            return True

        # ë‹¨ìˆœ ëª…ì‚¬êµ¬
        simple_patterns = [
            lambda q: len(q.split()) <= 2,  # 2ë‹¨ì–´ ì´í•˜
            lambda q: not any(char in q for char in "?!"),  # ì§ˆë¬¸/ê°íƒ„ë¬¸ ì•„ë‹˜
            lambda q: not any(word in q for word in ["ì–´ë–»ê²Œ", "ì™œ", "ë¬´ì—‡", "ì–¸ì œ", "ì–´ë””ì„œ"]),
        ]

        return sum(pattern(query) for pattern in simple_patterns) >= 2

    async def _call_gpt5_nano(self, query: str) -> dict[str, Any] | None:
        """GPT-5-nano API í˜¸ì¶œ (ê°•í™”ëœ JSON íŒŒì‹±)"""
        try:
            self.stats["gpt5_api_calls"] += 1

            # GPT-5 ìƒˆë¡œìš´ APIë¥¼ ìœ„í•œ input í…ìŠ¤íŠ¸ ì¤€ë¹„
            input_text = f"""System: You are a Korean query expansion specialist. Always respond in valid JSON format.

User: {self.expansion_prompt.format(query=query)}"""

            # Circuit Breaker ì„¤ì •
            cb_config = CircuitBreakerConfig(
                failure_threshold=3, timeout=15.0, error_rate_threshold=0.3
            )
            if self.circuit_breaker_factory:
                breaker = self.circuit_breaker_factory.get("query_expansion", cb_config)
            else:
                # circuit_breaker_factoryê°€ ì—†ìœ¼ë©´ breaker ì—†ì´ ì§ì ‘ í˜¸ì¶œ
                breaker = None

            # LLM Factory ì‚¬ìš© (Multi-LLM fallback)
            if breaker:
                content, provider = await breaker.call(
                    self.llm_factory.generate_with_fallback,
                    prompt=input_text,
                    system_prompt=None,
                    preferred_provider=self.provider,
                )
            else:
                # Circuit Breaker ì—†ì´ ì§ì ‘ í˜¸ì¶œ
                content, provider = await self.llm_factory.generate_with_fallback(
                    prompt=input_text,
                    system_prompt=None,
                    preferred_provider=self.provider,
                )
            logger.debug(
                "ì¿¼ë¦¬ í™•ì¥ ì‘ë‹µ (LLM Factory)",
                provider=provider,
                length=len(content),
            )

            # ê°•í™”ëœ JSON íŒŒì‹± (3ë‹¨ê³„ í´ë°±)
            parsed_json = self._parse_json_with_fallback(content)

            if parsed_json:
                return parsed_json
            else:
                self.stats["json_parse_failures"] += 1
                return None

        except Exception as e:
            logger.error("GPT-5-nano API í˜¸ì¶œ ì˜¤ë¥˜", error=str(e))
            return None

    def _parse_json_with_fallback(self, content: str) -> dict[str, Any] | None:
        """
        3ë‹¨ê³„ í´ë°± JSON íŒŒì‹±

        1ë‹¨ê³„: ì •ìƒ JSON íŒŒì‹±
        2ë‹¨ê³„: ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ í›„ íŒŒì‹±
        3ë‹¨ê³„: ìˆ˜ë™ í•„ë“œ ì¶”ì¶œ íŒŒì‹±
        """
        # === 1ë‹¨ê³„: ì •ìƒ JSON íŒŒì‹± ì‹œë„ ===
        try:
            return cast(dict[str, Any], json.loads(content))
        except json.JSONDecodeError as e1:
            logger.debug("1ë‹¨ê³„ íŒŒì‹± ì‹¤íŒ¨", error=str(e1))

        # === 2ë‹¨ê³„: JSON ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ ì‹œë„ ===
        try:
            # ```json ... ``` ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r"```json\s*(\{.*?\})\s*```", content, re.DOTALL)
            if json_match:
                extracted_json = json_match.group(1)
                logger.debug("JSON ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ ì„±ê³µ")
                return cast(dict[str, Any], json.loads(extracted_json))

            # ``` ... ``` ì¼ë°˜ ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ (json í‚¤ì›Œë“œ ì—†ì´)
            json_match = re.search(r"```\s*(\{.*?\})\s*```", content, re.DOTALL)
            if json_match:
                extracted_json = json_match.group(1)
                logger.debug("ì¼ë°˜ ì½”ë“œ ë¸”ë¡ ì¶”ì¶œ ì„±ê³µ")
                return cast(dict[str, Any], json.loads(extracted_json))

            # ì½”ë“œ ë¸”ë¡ ì—†ì´ { ... } ì¶”ì¶œ
            json_match = re.search(r"\{.*\}", content, re.DOTALL)
            if json_match:
                extracted_json = json_match.group(0)
                logger.debug("ì¤‘ê´„í˜¸ ë¸”ë¡ ì¶”ì¶œ ì„±ê³µ")
                return cast(dict[str, Any], json.loads(extracted_json))

        except json.JSONDecodeError as e2:
            logger.debug("2ë‹¨ê³„ íŒŒì‹± ì‹¤íŒ¨", error=str(e2))
        except Exception as e2:
            logger.warning("2ë‹¨ê³„ ì¶”ì¶œ ì˜¤ë¥˜", error=str(e2))

        # === 3ë‹¨ê³„: ìˆ˜ë™ í•„ë“œ ì¶”ì¶œ í´ë°± ===
        logger.warning("3ë‹¨ê³„ ìˆ˜ë™ íŒŒì‹± ì‹œë„", content_preview=content[:200])
        try:
            return self._manual_parse_json_fields(content)
        except Exception as e3:
            logger.error("3ë‹¨ê³„ ìˆ˜ë™ íŒŒì‹± ì‹¤íŒ¨", error=str(e3))
            logger.error("íŒŒì‹± ì‹¤íŒ¨í•œ ì „ì²´ ì‘ë‹µ", content=content)
            return None

    def _manual_parse_json_fields(self, content: str) -> dict[str, Any]:
        """
        ìˆ˜ë™ JSON í•„ë“œ ì¶”ì¶œ (ìµœì¢… í´ë°±)

        ë¶ˆì™„ì „í•œ JSONì—ì„œ ìµœì†Œ í•„ìˆ˜ í•„ë“œë§Œ ì¶”ì¶œ
        """
        result: dict[str, Any] = {
            "original_query": "",
            "synonyms": [],
            "related_terms": [],
            "core_keywords": [],
            "intent": "factual",
            "complexity": "medium",
            "expanded_queries": [],
            "search_strategy": "hybrid",
        }

        # ì •ê·œì‹ìœ¼ë¡œ í•„ë“œ ì¶”ì¶œ ì‹œë„
        try:
            # original_query ì¶”ì¶œ
            query_match = re.search(r'"original_query"\s*:\s*"([^"]*)"', content)
            if query_match:
                result["original_query"] = query_match.group(1)

            # synonyms ë°°ì—´ ì¶”ì¶œ
            synonyms_match = re.search(r'"synonyms"\s*:\s*\[(.*?)\]', content, re.DOTALL)
            if synonyms_match:
                synonyms_str = synonyms_match.group(1)
                result["synonyms"] = [
                    s.strip().strip('"') for s in synonyms_str.split(",") if s.strip()
                ]

            # expanded_queries ì¶”ì¶œ
            exp_queries_match = re.search(r'"expanded_queries"\s*:\s*\[(.*?)\]', content, re.DOTALL)
            if exp_queries_match:
                exp_str = exp_queries_match.group(1)
                # ê°„ë‹¨í•œ query í•„ë“œë§Œ ì¶”ì¶œ
                query_matches = re.findall(r'"query"\s*:\s*"([^"]*)"', exp_str)
                result["expanded_queries"] = [
                    {"query": q, "weight": 0.8, "focus": "extracted"} for q in query_matches
                ]

            logger.info(
                "ìˆ˜ë™ íŒŒì‹± ì„±ê³µ",
                expanded_queries_count=len(result["expanded_queries"]),
            )
            return result

        except Exception as e:
            logger.error("ìˆ˜ë™ í•„ë“œ ì¶”ì¶œ ì‹¤íŒ¨", error=str(e))
            raise

    def _parse_expansion_result(
        self, raw_data: dict[str, Any], original_query: str
    ) -> ExpandedQuery:
        """GPT-5 ì‘ë‹µì„ êµ¬ì¡°í™”ëœ ê°ì²´ë¡œ ë³€í™˜ (ì‹ ê·œ ì¸í„°í˜ì´ìŠ¤ ìŠ¤í™)"""
        try:
            # expanded_queriesì—ì„œ query ë¬¸ìì—´ë§Œ ì¶”ì¶œ
            expanded_queries_raw = raw_data.get("expanded_queries", [])
            expansions = []
            if isinstance(expanded_queries_raw, list):
                for item in expanded_queries_raw:
                    if isinstance(item, dict) and "query" in item:
                        expansions.append(item["query"])
                    elif isinstance(item, str):
                        expansions.append(item)

            # í™•ì¥ ì¿¼ë¦¬ê°€ ì—†ìœ¼ë©´ ì›ë³¸ ì¿¼ë¦¬ ì‚¬ìš©
            if not expansions:
                expansions = [original_query]

            return ExpandedQuery(
                original=original_query,
                expansions=expansions,
                complexity=QueryComplexity(raw_data.get("complexity", "medium")),
                intent=SearchIntent(raw_data.get("intent", "factual")),
                metadata={
                    "synonyms": raw_data.get("synonyms", []),
                    "related_terms": raw_data.get("related_terms", []),
                    "core_keywords": raw_data.get("core_keywords", []),
                    "search_strategy": raw_data.get("search_strategy", "hybrid"),
                    "raw_expanded_queries": expanded_queries_raw,
                },
            )
        except (ValueError, KeyError) as e:
            logger.warning("í™•ì¥ ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜, í´ë°± ì‚¬ìš©", error=str(e))
            return self._create_fallback_expansion(original_query)

    def _create_simple_expansion(self, query: str) -> ExpandedQuery:
        """ê°„ë‹¨í•œ ì¿¼ë¦¬ìš© ë¹ ë¥¸ í™•ì¥ (ì‹ ê·œ ì¸í„°í˜ì´ìŠ¤ ìŠ¤í™)"""
        keywords = query.split()

        return ExpandedQuery(
            original=query,
            expansions=[query],
            complexity=QueryComplexity.SIMPLE,
            intent=SearchIntent.FACTUAL,
            metadata={
                "synonyms": [],
                "related_terms": [],
                "core_keywords": [
                    {"keyword": kw, "weight": 1.0 - (i * 0.1)} for i, kw in enumerate(keywords[:3])
                ],
                "search_strategy": "focused",
            },
        )

    def _create_fallback_expansion(self, query: str) -> ExpandedQuery:
        """í´ë°± í™•ì¥ (GPT-5 ì‹¤íŒ¨ì‹œ, ì‹ ê·œ ì¸í„°í˜ì´ìŠ¤ ìŠ¤í™)"""
        keywords = query.split()

        # í™•ì¥ ì¿¼ë¦¬: ì›ë³¸ + í‚¤ì›Œë“œ ì¡°í•©
        expansions = [query]
        if keywords:
            keyword_query = " ".join(keywords)
            if keyword_query != query:
                expansions.append(keyword_query)

        return ExpandedQuery(
            original=query,
            expansions=expansions,
            complexity=QueryComplexity.MEDIUM,
            intent=SearchIntent.FACTUAL,
            metadata={
                "synonyms": [],
                "related_terms": [],
                "core_keywords": [{"keyword": kw, "weight": 0.8} for kw in keywords],
                "search_strategy": "hybrid",
            },
        )

    def _update_stats(self, expanded_query: ExpandedQuery, processing_time: float) -> None:
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.stats["successful_expansions"] += 1
        self.stats["complexity_distribution"][expanded_query.complexity.value] += 1
        self.stats["intent_distribution"][expanded_query.intent.value] += 1

        # í‰ê·  ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
        total_expansions = self.stats["successful_expansions"]
        current_avg = self.stats["average_response_time"]
        self.stats["average_response_time"] = (
            current_avg * (total_expansions - 1) + processing_time
        ) / total_expansions

    def get_stats(self) -> dict[str, Any]:
        """í™•ì¥ ì„±ëŠ¥ í†µê³„"""
        success_rate = (
            self.stats["successful_expansions"] / max(1, self.stats["total_expansions"])
        ) * 100

        return {
            **self.stats,
            "success_rate_percentage": round(success_rate, 2),
            "average_response_time_ms": round(self.stats["average_response_time"] * 1000, 2),
            "cache_size": len(self.expansion_cache),
            "cache_maxsize": self.expansion_cache.maxsize,
            "cache_ttl": self.expansion_cache.ttl,
        }

    # ==================== IQueryExpansionEngine ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„ ====================

    async def expand(self, query: str, num_expansions: int = 5, **kwargs: Any) -> ExpandedQuery:
        """
        ì¸í„°í˜ì´ìŠ¤ ë©”ì„œë“œ: ì¿¼ë¦¬ í™•ì¥

        ê¸°ì¡´ expand_query() ë©”ì„œë“œì— ìœ„ì„í•˜ì—¬ ì¤‘ë³µ êµ¬í˜„ ë°©ì§€

        Args:
            query: í™•ì¥í•  ì›ë³¸ ì¿¼ë¦¬
            num_expansions: ìƒì„±í•  í™•ì¥ ì¿¼ë¦¬ ìˆ˜ (í˜„ì¬ ë¬´ì‹œ, self.num_expansions ì‚¬ìš©)
            **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„° (í™•ì¥ì„±)

        Returns:
            ExpandedQuery ê°ì²´
        """
        # ê¸°ì¡´ ê²€ì¦ëœ ë¡œì§ ì¬ì‚¬ìš© (DRY ì›ì¹™)
        result = await self.expand_query(query)
        if result is None:
            # í´ë°±: ê¸°ë³¸ í™•ì¥ ë°˜í™˜
            logger.warning("expand() ì‹¤íŒ¨, í´ë°± ì‚¬ìš©", query=query)
            return self._create_fallback_expansion(query)
        return result

    async def analyze_complexity(self, query: str) -> QueryComplexity:
        """
        ì¿¼ë¦¬ ë³µì¡ë„ ë¶„ì„

        expand_query() ê²°ê³¼ì—ì„œ ë³µì¡ë„ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
        ìºì‹± ë•ë¶„ì— ì¤‘ë³µ API í˜¸ì¶œ ì—†ìŒ

        Args:
            query: ë¶„ì„í•  ì¿¼ë¦¬

        Returns:
            QueryComplexity Enum (SIMPLE, MEDIUM, COMPLEX, CONTEXTUAL)
        """
        try:
            result = await self.expand_query(query)
            if result:
                return result.complexity
            else:
                # í´ë°±: MEDIUM ë°˜í™˜
                logger.warning("ë³µì¡ë„ ë¶„ì„ ì‹¤íŒ¨, MEDIUM ë°˜í™˜", query=query)
                return QueryComplexity.MEDIUM
        except Exception as e:
            logger.error("ë³µì¡ë„ ë¶„ì„ ì˜¤ë¥˜", query=query, error=str(e))
            return QueryComplexity.MEDIUM

    async def detect_intent(self, query: str) -> SearchIntent:
        """
        ê²€ìƒ‰ ì˜ë„ ê°ì§€

        expand_query() ê²°ê³¼ì—ì„œ ì˜ë„ë§Œ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜
        ìºì‹± ë•ë¶„ì— ì¤‘ë³µ API í˜¸ì¶œ ì—†ìŒ

        Args:
            query: ë¶„ì„í•  ì¿¼ë¦¬

        Returns:
            SearchIntent Enum (FACTUAL, PROCEDURAL, CONCEPTUAL, COMPARATIVE, PROBLEM_SOLVING)
        """
        try:
            result = await self.expand_query(query)
            if result:
                return result.intent
            else:
                # í´ë°±: FACTUAL ë°˜í™˜
                logger.warning("ì˜ë„ ê°ì§€ ì‹¤íŒ¨, FACTUAL ë°˜í™˜", query=query)
                return SearchIntent.FACTUAL
        except Exception as e:
            logger.error("ì˜ë„ ê°ì§€ ì˜¤ë¥˜", query=query, error=str(e))
            return SearchIntent.FACTUAL

    async def health_check(self) -> bool:
        """
        LLM API ì—°ê²° ìƒíƒœ í™•ì¸

        LLM Factoryê°€ ì¡´ì¬í•˜ë©´ ì •ìƒìœ¼ë¡œ ê°„ì£¼ (íŒ©í† ë¦¬ ë‚´ë¶€ì—ì„œ fallback ì²˜ë¦¬)

        Returns:
            True: LLM íŒ©í† ë¦¬ ì •ìƒ, False: LLM íŒ©í† ë¦¬ ì—†ìŒ
        """
        # llm_factoryëŠ” í•„ìˆ˜ì´ë¯€ë¡œ í•­ìƒ ì¡´ì¬
        # íŒ©í† ë¦¬ê°€ ì¡´ì¬í•˜ë©´ ì •ìƒìœ¼ë¡œ ê°„ì£¼ (íŒ©í† ë¦¬ ë‚´ë¶€ì—ì„œ fallback ì²˜ë¦¬)
        if self.llm_factory:
            logger.debug("health_check: LLM íŒ©í† ë¦¬ ì‚¬ìš©, ì •ìƒ ê°„ì£¼")
            return True
        return False

    # ==================== í•˜ìœ„ í˜¸í™˜ì„±: from_config() íŒ©í† ë¦¬ ë©”ì„œë“œ ====================

    @classmethod
    def from_config(
        cls,
        config: dict[str, Any],
        llm_factory: Any = None,  # í•„ìˆ˜
        circuit_breaker_factory: CircuitBreakerFactory | None = None,
    ) -> "GPT5QueryExpansionEngine":
        """
        config dictì—ì„œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

        Args:
            config: ì „ì²´ ì„¤ì • dict (app/config/config.yaml í˜•ì‹)
            llm_factory: LLM Factory ì¸ìŠ¤í„´ìŠ¤ (í•„ìˆ˜)
            circuit_breaker_factory: DI Containerì˜ CircuitBreaker íŒ©í† ë¦¬

        Returns:
            GPT5QueryExpansionEngine ì¸ìŠ¤í„´ìŠ¤

        Raises:
            ValueError: llm_factoryê°€ Noneì¸ ê²½ìš°

        Example:
            >>> engine = GPT5QueryExpansionEngine.from_config(config, llm_factory=factory)
        """
        # llm_factory í•„ìˆ˜ ê²€ì¦
        if llm_factory is None:
            raise ValueError(
                "llm_factoryëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤. "
                "GPT5QueryExpansionEngine.from_config(config, llm_factory=factory)ë¡œ í˜¸ì¶œí•˜ì„¸ìš”."
            )

        # ì„¤ì • ì¶”ì¶œ (query_expansion.yaml ìš°ì„  ì°¸ì¡°)
        query_expansion_config = config.get("query_expansion", {})
        llm_config = query_expansion_config.get("llm", {})
        multi_query_config = query_expansion_config.get(
            "multi_query", config.get("multi_query", {})
        )

        # Provider ì„¤ì • ì½ê¸° (query_expansion.llm.provider ìš°ì„ , ì—†ìœ¼ë©´ ê¸°ë³¸ê°’: openai)
        # ì„¤ì • íŒŒì¼ êµ¬ì¡°: query_expansion.llm.provider
        provider = llm_config.get("provider", query_expansion_config.get("provider", "openai"))

        # ìƒì„±ì í˜¸ì¶œ
        return cls(
            num_expansions=multi_query_config.get("num_expansions", 5),
            max_tokens=multi_query_config.get("max_tokens", 500),
            temperature=multi_query_config.get("temperature", 0.7),
            cache_size=1000,  # ê³ ì •ê°’ (ë ˆê±°ì‹œ í˜¸í™˜)
            cache_ttl=86400,  # 1ì¼ (ë ˆê±°ì‹œ í˜¸í™˜)
            llm_factory=llm_factory,
            provider=provider,  # ì„¤ì •ì—ì„œ ì½ì€ provider ì „ë‹¬
            circuit_breaker_factory=circuit_breaker_factory,
        )
