"""
EvaluatorFactory - ì„¤ì • ê¸°ë°˜ í‰ê°€ê¸° ìë™ ì„ íƒ íŒ©í† ë¦¬

YAML ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ í‰ê°€ê¸° ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
ê¸°ì¡´ GraphRAGFactory, CacheFactory íŒ¨í„´ê³¼ ë™ì¼í•œ êµ¬ì¡°.

ì‚¬ìš© ì˜ˆì‹œ:
    from app.modules.core.evaluation import EvaluatorFactory

    # YAML ì„¤ì • ê¸°ë°˜ í‰ê°€ê¸° ìƒì„±
    evaluator = EvaluatorFactory.create(config, llm_client=llm)

    # ì§€ì› í‰ê°€ê¸° ì¡°íšŒ
    EvaluatorFactory.get_supported_evaluators()

ì§€ì› í‰ê°€ê¸°:
    - internal: LLM ê¸°ë°˜ ì‹¤ì‹œê°„ ë‚´ë¶€ í‰ê°€ (ë¹ ë¦„, ì €ë¹„ìš©, ê¸°ë³¸ê°’)
    - ragas: Ragas ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ ë°°ì¹˜ í‰ê°€ (ê³µì‹ ë ¥, Phase 2 ì˜ˆì •)

ì˜ì¡´ì„±:
    - app.lib.logger (ë¡œê¹…)
    - app.modules.core.evaluation.interfaces (IEvaluator Protocol)
    - app.modules.core.evaluation.internal_evaluator (InternalEvaluator)
"""
from typing import Any

from app.lib.logger import get_logger

from .interfaces import IEvaluator
from .internal_evaluator import InternalEvaluator
from .ragas_evaluator import RagasEvaluator

logger = get_logger(__name__)


# ì§€ì› í‰ê°€ê¸° ë ˆì§€ìŠ¤íŠ¸ë¦¬
SUPPORTED_EVALUATORS: dict[str, dict[str, Any]] = {
    # ì‹¤ì‹œê°„ ë‚´ë¶€ í‰ê°€ (ë¹ ë¦„, ì €ë¹„ìš©, ê¸°ë³¸ê°’)
    "internal": {
        "type": "realtime",
        "class": "InternalEvaluator",
        "description": "LLM ê¸°ë°˜ ì‹¤ì‹œê°„ ë‚´ë¶€ í‰ê°€ (ë¹ ë¦„, ì €ë¹„ìš©)",
        "default_config": {
            "model": "google/gemini-2.5-flash-lite",
            "timeout": 10.0,
        },
    },
    # ë°°ì¹˜ í‰ê°€ (Ragas ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜)
    "ragas": {
        "type": "batch",
        "class": "RagasEvaluator",
        "description": "Ragas ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ë°˜ ë°°ì¹˜ í‰ê°€ (ê³µì‹ ë ¥)",
        "requires_package": "ragas",
        "metrics": ["faithfulness", "answer_relevancy", "context_precision"],
        "default_config": {
            "metrics": ["faithfulness", "answer_relevancy"],
            "batch_size": 10,
            "llm_model": "gpt-4o",
            "embedding_model": "text-embedding-3-large",
        },
    },
}


class EvaluatorFactory:
    """
    ì„¤ì • ê¸°ë°˜ í‰ê°€ê¸° íŒ©í† ë¦¬

    YAML ì„¤ì • íŒŒì¼ì˜ evaluation ì„¹ì…˜ì„ ì½ì–´ ì ì ˆí•œ í‰ê°€ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    ì„¤ì • ì˜ˆì‹œ (features/evaluation.yaml):
        evaluation:
          enabled: false  # ê¸°ë³¸ ë¹„í™œì„±í™”
          provider: "internal"  # internal, ragas
          internal:
            model: "google/gemini-2.5-flash-lite"
            timeout: 10.0
          ragas:
            metrics: ["faithfulness", "answer_relevancy"]
    """

    @staticmethod
    def create(
        config: dict[str, Any],
        llm_client: Any | None = None,
    ) -> IEvaluator | None:
        """
        ì„¤ì • ê¸°ë°˜ í‰ê°€ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±

        Args:
            config: ì „ì²´ ì„¤ì • ë”•ì…”ë„ˆë¦¬ (evaluation ì„¹ì…˜ í¬í•¨)
            llm_client: LLM í´ë¼ì´ì–¸íŠ¸ (internal í‰ê°€ê¸°ì— í•„ìš”)

        Returns:
            IEvaluator ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•œ í‰ê°€ê¸° ì¸ìŠ¤í„´ìŠ¤
            ë¹„í™œì„±í™” ì‹œ None

        Raises:
            ValueError: ì§€ì›í•˜ì§€ ì•ŠëŠ” í”„ë¡œë°”ì´ë”ì¸ ê²½ìš°
            NotImplementedError: ë¯¸êµ¬í˜„ í”„ë¡œë°”ì´ë”ì¸ ê²½ìš°
        """
        eval_config = config.get("evaluation", {})

        # ë¹„í™œì„±í™” ì²´í¬ (ê¸°ë³¸ê°’: False)
        if not eval_config.get("enabled", False):
            logger.info("â„¹ï¸  Evaluation disabled via config")
            return None

        provider = eval_config.get("provider", "internal")

        logger.info(f"ğŸ”„ EvaluatorFactory: provider={provider}")

        # ì§€ì› ì—¬ë¶€ í™•ì¸
        if provider not in SUPPORTED_EVALUATORS:
            supported = list(SUPPORTED_EVALUATORS.keys())
            raise ValueError(
                f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í‰ê°€ê¸° í”„ë¡œë°”ì´ë”: {provider}. "
                f"ì§€ì› ëª©ë¡: {supported}"
            )

        # í”„ë¡œë°”ì´ë”ë³„ í‰ê°€ê¸° ìƒì„±
        if provider == "internal":
            return EvaluatorFactory._create_internal_evaluator(
                config, eval_config, llm_client
            )
        elif provider == "ragas":
            return EvaluatorFactory._create_ragas_evaluator(config, eval_config)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” í‰ê°€ê¸° í”„ë¡œë°”ì´ë”: {provider}")

    @staticmethod
    def _create_internal_evaluator(
        config: dict[str, Any],
        eval_config: dict[str, Any],
        llm_client: Any | None,
    ) -> InternalEvaluator:
        """
        Internal í‰ê°€ê¸° ìƒì„±

        Args:
            config: ì „ì²´ ì„¤ì •
            eval_config: evaluation ì„¹ì…˜ ì„¤ì •
            llm_client: LLM í´ë¼ì´ì–¸íŠ¸

        Returns:
            InternalEvaluator ì¸ìŠ¤í„´ìŠ¤
        """
        internal_config = eval_config.get("internal", {})
        defaults = SUPPORTED_EVALUATORS["internal"]["default_config"]

        evaluator = InternalEvaluator(
            llm_client=llm_client,
            model=internal_config.get("model", defaults["model"]),
            timeout=internal_config.get("timeout", defaults["timeout"]),
        )

        logger.info(
            f"âœ… InternalEvaluator ìƒì„±: "
            f"model={internal_config.get('model', defaults['model'])}"
        )
        return evaluator

    @staticmethod
    def _create_ragas_evaluator(
        config: dict[str, Any],
        eval_config: dict[str, Any],
    ) -> IEvaluator:
        """
        Ragas í‰ê°€ê¸° ìƒì„±

        Args:
            config: ì „ì²´ ì„¤ì •
            eval_config: evaluation ì„¹ì…˜ ì„¤ì •

        Returns:
            RagasEvaluator ì¸ìŠ¤í„´ìŠ¤
        """
        ragas_config = eval_config.get("ragas", {})
        defaults = SUPPORTED_EVALUATORS["ragas"]["default_config"]

        evaluator = RagasEvaluator(
            metrics=ragas_config.get("metrics", defaults["metrics"]),
            batch_size=ragas_config.get("batch_size", defaults["batch_size"]),
            llm_model=ragas_config.get("llm_model", defaults["llm_model"]),
            embedding_model=ragas_config.get(
                "embedding_model", defaults["embedding_model"]
            ),
        )

        logger.info(
            f"âœ… RagasEvaluator ìƒì„±: "
            f"metrics={ragas_config.get('metrics', defaults['metrics'])}, "
            f"available={evaluator.is_available()}"
        )
        return evaluator

    @staticmethod
    def get_supported_evaluators() -> list[str]:
        """
        ì§€ì›í•˜ëŠ” ëª¨ë“  í‰ê°€ê¸° ì´ë¦„ ë°˜í™˜

        Returns:
            í‰ê°€ê¸° ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        return list(SUPPORTED_EVALUATORS.keys())

    @staticmethod
    def get_evaluator_info(name: str) -> dict[str, Any] | None:
        """
        íŠ¹ì • í‰ê°€ê¸°ì˜ ìƒì„¸ ì •ë³´ ë°˜í™˜

        Args:
            name: í‰ê°€ê¸° ì´ë¦„

        Returns:
            í‰ê°€ê¸° ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        return SUPPORTED_EVALUATORS.get(name)

    @staticmethod
    def list_evaluators_by_type(eval_type: str) -> list[str]:
        """
        íƒ€ì…ë³„ í‰ê°€ê¸° ëª©ë¡ ë°˜í™˜

        Args:
            eval_type: í‰ê°€ê¸° íƒ€ì… (realtime, batch ë“±)

        Returns:
            í•´ë‹¹ íƒ€ì…ì˜ í‰ê°€ê¸° ì´ë¦„ ë¦¬ìŠ¤íŠ¸
        """
        return [
            name
            for name, info in SUPPORTED_EVALUATORS.items()
            if info["type"] == eval_type
        ]
