"""
LLM ê¸°ë°˜ ë‹µë³€ í’ˆì§ˆ í‰ê°€ ëª¨ë“ˆ

LLMì„ í™œìš©í•˜ì—¬ ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆì„ 4ê°€ì§€ ì°¨ì›ì—ì„œ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
Self-RAG ì‹œìŠ¤í…œì—ì„œ ë‹µë³€ ì¬ìƒì„± ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- Gemini LLM ê¸°ë°˜ í’ˆì§ˆ í‰ê°€
- 4ê°€ì§€ í‰ê°€ ì°¨ì›: ê´€ë ¨ì„±, ê·¼ê±°ì„±, ì™„ì „ì„±, í™•ì‹ ë„
- í’ˆì§ˆ ì„ê³„ê°’ ê¸°ë°˜ ì¬ìƒì„± í•„ìš” ì—¬ë¶€ íŒë‹¨
"""

import json
from dataclasses import dataclass
from typing import Any

import structlog
from langchain_google_genai import ChatGoogleGenerativeAI

logger = structlog.get_logger(__name__)


@dataclass
class QualityScore:
    """í’ˆì§ˆ í‰ê°€ ì ìˆ˜"""

    relevance: float  # ê´€ë ¨ì„± (0.0-1.0)
    grounding: float  # ê·¼ê±°ì„± (0.0-1.0)
    completeness: float  # ì™„ì „ì„± (0.0-1.0)
    confidence: float  # í™•ì‹ ë„ (0.0-1.0)
    overall: float  # ì¢…í•© ì ìˆ˜ (0.0-1.0)
    reasoning: str  # í‰ê°€ ê·¼ê±°
    raw_response: dict  # LLM ì›ë³¸ ì‘ë‹µ


class LLMQualityEvaluator:
    """
    LLM ê¸°ë°˜ ë‹µë³€ í’ˆì§ˆ í‰ê°€ê¸°

    Gemini LLMì„ ì‚¬ìš©í•˜ì—¬ ìƒì„±ëœ ë‹µë³€ì˜ í’ˆì§ˆì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.
    Self-RAG ì‹œìŠ¤í…œì—ì„œ ì €í’ˆì§ˆ ë‹µë³€ ì¬ìƒì„± ì—¬ë¶€ë¥¼ ê²°ì •í•˜ëŠ” í•µì‹¬ ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤.
    """

    def __init__(
        self,
        llm_provider: str = "google",
        model_name: str = "gemini-2.0-flash-exp",
        api_key: str | None = None,
        quality_threshold: float = 0.75,
        relevance_weight: float = 0.35,
        grounding_weight: float = 0.30,
        completeness_weight: float = 0.25,
        confidence_weight: float = 0.10,
    ):
        self.quality_threshold = quality_threshold
        self.relevance_weight = relevance_weight
        self.grounding_weight = grounding_weight
        self.completeness_weight = completeness_weight
        self.confidence_weight = confidence_weight
        self.llm = None  # Graceful degradation: LLM ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ None

        # LLM ì´ˆê¸°í™” (Graceful Degradation - MVP Phase 1)
        if llm_provider == "google":
            # API í‚¤ê°€ ì—†ìœ¼ë©´ Self-RAG ë¹„í™œì„±í™”
            if not api_key:
                logger.warning(
                    "self_rag_disabled",
                    reason="Google API í‚¤ê°€ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. MVP Phase 1ì—ì„œëŠ” Self-RAG ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.",
                )
                return

            try:
                self.llm = ChatGoogleGenerativeAI(
                    model=model_name,
                    google_api_key=api_key,  # type: ignore[call-arg]
                    temperature=0.0,  # ì¼ê´€ëœ í‰ê°€ë¥¼ ìœ„í•´ 0
                )
                logger.info(
                    "evaluator_initialized",
                    provider=llm_provider,
                    model=model_name,
                    threshold=quality_threshold,
                )
            except Exception as e:
                # Google ìê²©ì¦ëª… ì˜¤ë¥˜ ë˜ëŠ” ê¸°íƒ€ ì´ˆê¸°í™” ì‹¤íŒ¨
                logger.warning(
                    "evaluator_initialization_failed",
                    provider=llm_provider,
                    error=str(e),
                    reason="Self-RAG í‰ê°€ê¸° ì´ˆê¸°í™” ì‹¤íŒ¨. MVP Phase 1ì—ì„œëŠ” Self-RAG ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.",
                )
                # self.llmì€ None ìƒíƒœë¡œ ìœ ì§€
        else:
            logger.error("unsupported_llm_provider", provider=llm_provider)
            raise ValueError(f"Unsupported LLM provider: {llm_provider}")

    async def evaluate(self, query: str, answer: str, context: list[str]) -> QualityScore:
        """
        ë‹µë³€ í’ˆì§ˆ í‰ê°€

        Args:
            query: ì‚¬ìš©ì ì§ˆë¬¸
            answer: ìƒì„±ëœ ë‹µë³€
            context: ê²€ìƒ‰ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

        Returns:
            QualityScore: í’ˆì§ˆ í‰ê°€ ê²°ê³¼
        """
        # Self-RAG ë¹„í™œì„±í™” ìƒíƒœ í™•ì¸ (Graceful Degradation)
        if self.llm is None:
            logger.debug("self_rag_disabled_skip_evaluation")
            # MVP Phase 1: Self-RAG ì—†ì´ ê¸°ë³¸ ì ìˆ˜ ë°˜í™˜
            return self._default_quality_score()

        # í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_evaluation_prompt(query, answer, context)

        # LLM í‰ê°€ ìˆ˜í–‰
        try:
            response = await self.llm.ainvoke(prompt)
            # response.contentëŠ” str | list íƒ€ì…ì´ë¯€ë¡œ str ë³€í™˜
            content: str = (
                response.content if isinstance(response.content, str) else str(response.content)
            )
            raw_response = self._parse_llm_response(content)

            # ì ìˆ˜ ì¶”ì¶œ
            relevance = raw_response.get("relevance", 0.5)
            grounding = raw_response.get("grounding", 0.5)
            completeness = raw_response.get("completeness", 0.5)
            confidence = raw_response.get("confidence", 0.5)
            reasoning = raw_response.get("reasoning", "")

            # ì¢…í•© ì ìˆ˜ ê³„ì‚°
            overall = (
                relevance * self.relevance_weight
                + grounding * self.grounding_weight
                + completeness * self.completeness_weight
                + confidence * self.confidence_weight
            )

            quality_score = QualityScore(
                relevance=relevance,
                grounding=grounding,
                completeness=completeness,
                confidence=confidence,
                overall=overall,
                reasoning=reasoning,
                raw_response=raw_response,
            )

            logger.info(
                "answer_evaluated",
                overall_score=overall,
                relevance=relevance,
                grounding=grounding,
                completeness=completeness,
                confidence=confidence,
                requires_regeneration=overall < self.quality_threshold,
            )

            return quality_score

        except Exception as e:
            logger.error("evaluation_failed", error=str(e))
            # í‰ê°€ ì‹¤íŒ¨ ì‹œ ì¤‘ë¦½ ì ìˆ˜ ë°˜í™˜
            return self._default_quality_score()

    def requires_regeneration(self, quality: QualityScore) -> bool:
        """ì¬ìƒì„± í•„ìš” ì—¬ë¶€ íŒë‹¨"""
        return quality.overall < self.quality_threshold

    def _build_evaluation_prompt(self, query: str, answer: str, context: list[str]) -> str:
        """í‰ê°€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        context_text = "\n\n".join([f"ë¬¸ì„œ {i+1}:\n{doc}" for i, doc in enumerate(context)])

        return f"""ë‹¹ì‹ ì€ AI ë‹µë³€ì˜ í’ˆì§ˆì„ ê°ê´€ì ìœ¼ë¡œ í‰ê°€í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ ë‹µë³€ì„ JSON í˜•ì‹ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:

ğŸ“‹ í‰ê°€ ê¸°ì¤€:
1. relevance (ê´€ë ¨ì„±): ì§ˆë¬¸ê³¼ ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ê´€ë ¨ì´ ìˆëŠ”ê°€?
   - 1.0: ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•¨
   - 0.5: ë¶€ë¶„ì ìœ¼ë¡œ ê´€ë ¨ ìˆìŒ
   - 0.0: ì§ˆë¬¸ê³¼ ë¬´ê´€í•¨

2. grounding (ê·¼ê±°ì„±): ë‹µë³€ì´ ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì— ê·¼ê±°í•˜ê³  ìˆëŠ”ê°€?
   - 1.0: ëª¨ë“  ì •ë³´ê°€ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‚˜ì˜´
   - 0.5: ì¼ë¶€ ì¶”ì¸¡ì´ í¬í•¨ë¨
   - 0.0: ì»¨í…ìŠ¤íŠ¸ì™€ ë¬´ê´€í•œ ë‹µë³€

3. completeness (ì™„ì „ì„±): ì§ˆë¬¸ì— ì™„ì „íˆ ë‹µë³€í–ˆëŠ”ê°€?
   - 1.0: ì§ˆë¬¸ì˜ ëª¨ë“  ë¶€ë¶„ì— ë‹µë³€í•¨
   - 0.5: ì¼ë¶€ë§Œ ë‹µë³€í•¨
   - 0.0: ë‹µë³€ì´ ë¶ˆì™„ì „í•¨

4. confidence (í™•ì‹ ë„): ë‹µë³€ì˜ í™•ì‹¤ì„± ìˆ˜ì¤€ì€?
   - 1.0: ë§¤ìš° í™•ì‹¤í•œ ë‹µë³€
   - 0.5: ë¶ˆí™•ì‹¤ì„± í¬í•¨
   - 0.0: ë§¤ìš° ë¶ˆí™•ì‹¤í•¨

---

ì§ˆë¬¸:
{query}

ì œê³µëœ ì»¨í…ìŠ¤íŠ¸:
{context_text}

ìƒì„±ëœ ë‹µë³€:
{answer}

---

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "relevance": 0.0-1.0,
    "grounding": 0.0-1.0,
    "completeness": 0.0-1.0,
    "confidence": 0.0-1.0,
    "reasoning": "ê° ì ìˆ˜ì— ëŒ€í•œ ê°„ë‹¨í•œ ê·¼ê±°"
}}"""

    def _parse_llm_response(self, content: str) -> dict:
        """LLM ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ë¸”ë¡ ì¶”ì¶œ (```json ... ``` í˜•ì‹ ì²˜ë¦¬)
            if "```json" in content:
                start = content.find("```json") + 7
                end = content.find("```", start)
                content = content[start:end].strip()
            elif "```" in content:
                start = content.find("```") + 3
                end = content.find("```", start)
                content = content[start:end].strip()

            result: dict[str, Any] = json.loads(content)
            return result
        except Exception as e:
            logger.warning("llm_response_parse_failed", error=str(e), content=content[:200])
            return {
                "relevance": 0.5,
                "grounding": 0.5,
                "completeness": 0.5,
                "confidence": 0.5,
                "reasoning": "í‰ê°€ íŒŒì‹± ì‹¤íŒ¨",
            }

    def _default_quality_score(self) -> QualityScore:
        """ê¸°ë³¸ í’ˆì§ˆ ì ìˆ˜ (í‰ê°€ ì‹¤íŒ¨ ì‹œ)"""
        return QualityScore(
            relevance=0.5,
            grounding=0.5,
            completeness=0.5,
            confidence=0.5,
            overall=0.5,
            reasoning="í‰ê°€ ì‹¤íŒ¨ë¡œ ì¸í•œ ê¸°ë³¸ ì ìˆ˜",
            raw_response={},
        )
