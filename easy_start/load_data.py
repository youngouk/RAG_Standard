#!/usr/bin/env python3
"""
ChromaDB ì „ìš© ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸

Docker ì—†ì´ ChromaDBì— ìƒ˜í”Œ FAQ ë°ì´í„°ë¥¼ ì ì¬í•©ë‹ˆë‹¤.
BM25 ì¸ë±ìŠ¤ë„ í•¨ê»˜ êµ¬ì¶•í•˜ì—¬ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.

ì‚¬ìš©ë²•:
    uv run python easy_start/load_data.py

ì˜ì¡´ì„±:
    - chromadb: ë²¡í„° ìŠ¤í† ì–´
    - sentence-transformers: ë¡œì»¬ ì„ë² ë”©
    - kiwipiepy, rank-bm25: BM25 ì¸ë±ìŠ¤ (ì„ íƒì )
"""

import asyncio
import json
import pickle
import sys
from pathlib import Path
from typing import Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# ìƒìˆ˜
CHROMA_PERSIST_DIR = str(project_root / "easy_start" / ".chroma_data")
BM25_INDEX_PATH = str(project_root / "easy_start" / ".bm25_index.pkl")
COLLECTION_NAME = "documents"
SAMPLE_DATA_PATH = project_root / "quickstart" / "sample_data.json"


def prepare_documents(raw_docs: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """
    sample_data.json ë¬¸ì„œë¥¼ ChromaDB ì ì¬ í˜•ì‹ìœ¼ë¡œ ë³€í™˜

    Args:
        raw_docs: sample_data.jsonì˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸

    Returns:
        ChromaDB í˜¸í™˜ í˜•ì‹ì˜ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        ê° ë¬¸ì„œ: {"id": str, "content": str, "metadata": dict}
    """
    result: list[dict[str, Any]] = []

    for doc in raw_docs:
        # í•„ìˆ˜ í•„ë“œ ê²€ì¦
        doc_id = doc.get("id", "")
        title = doc.get("title", "")
        content = doc.get("content", "")
        if not doc_id or not content:
            continue

        # title + content ë³‘í•© (ê²€ìƒ‰ ìµœì í™”)
        full_content = f"{title}\n\n{content}" if title else content

        metadata: dict[str, Any] = {
            "source_file": title,
            "file_type": doc.get("metadata", {}).get("category", "FAQ"),
            "source": "quickstart_sample",
        }

        # category ì¶”ê°€
        category = doc.get("metadata", {}).get("category", "")
        if category:
            metadata["category"] = category

        result.append({
            "id": doc_id,
            "content": full_content,
            "metadata": metadata,
        })

    return result


def build_bm25_index(docs: list[dict[str, Any]]) -> Any:
    """
    BM25 ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤.

    Args:
        docs: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ (id, content, metadata í¬í•¨)

    Returns:
        BM25Index ì¸ìŠ¤í„´ìŠ¤

    Raises:
        ImportError: kiwipiepy ë˜ëŠ” rank-bm25ê°€ ë¯¸ì„¤ì¹˜ëœ ê²½ìš°
    """
    from app.modules.core.retrieval.bm25_engine import BM25Index, KoreanTokenizer

    # ë¶ˆìš©ì–´ í•„í„° ì—°ë™ (ìˆìœ¼ë©´)
    stopword_filter = None
    try:
        from app.modules.core.retrieval.bm25.stopwords import StopwordFilter
        stopword_filter = StopwordFilter(use_defaults=True, enabled=True)
    except ImportError:
        pass

    tokenizer = KoreanTokenizer(stopword_filter=stopword_filter)
    index = BM25Index(tokenizer=tokenizer)
    index.build(docs)

    return index


async def load_to_chroma(
    docs: list[dict[str, Any]],
    embeddings: list[list[float]],
    persist_dir: str = CHROMA_PERSIST_DIR,
    collection_name: str = COLLECTION_NAME,
) -> int:
    """
    ChromaDBì— ë¬¸ì„œ ì ì¬

    Args:
        docs: ì¤€ë¹„ëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        embeddings: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        persist_dir: ChromaDB ì˜ì† ë””ë ‰í† ë¦¬
        collection_name: ì»¬ë ‰ì…˜ ì´ë¦„

    Returns:
        ì ì¬ëœ ë¬¸ì„œ ìˆ˜
    """
    from app.infrastructure.storage.vector.chroma_store import ChromaVectorStore

    store = ChromaVectorStore(persist_directory=persist_dir)

    # ChromaVectorStore í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    chroma_docs = []
    for doc, vector in zip(docs, embeddings, strict=True):
        chroma_docs.append({
            "id": doc["id"],
            "vector": vector,
            "metadata": {
                **doc["metadata"],
                "content": doc["content"],  # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë‚´ìš© ë°˜í™˜ìš©
            },
        })

    count = await store.add_documents(
        collection=collection_name,
        documents=chroma_docs,
    )

    return count


def save_bm25_index(index: Any, path: str = BM25_INDEX_PATH) -> None:
    """
    BM25 ì¸ë±ìŠ¤ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥

    Kiwi(C í™•ì¥)ëŠ” pickle ë¶ˆê°€ì´ë¯€ë¡œ, ì¬êµ¬ì¶•ì— í•„ìš”í•œ
    ë¬¸ì„œì™€ í† í°í™” ê²°ê³¼ë§Œ ì €ì¥í•©ë‹ˆë‹¤.
    """
    Path(path).parent.mkdir(parents=True, exist_ok=True)
    serializable_data = {
        "documents": index._documents,
        "tokenized_corpus": index._tokenized_corpus,
    }
    with open(path, "wb") as f:
        pickle.dump(serializable_data, f)


def load_bm25_index(path: str = BM25_INDEX_PATH) -> Any:
    """
    ì €ì¥ëœ BM25 ì¸ë±ìŠ¤ ë°ì´í„°ë¡œ BM25Indexë¥¼ ì¬êµ¬ì¶•

    pickleì—ì„œ ë¬¸ì„œ + í† í°í™” ê²°ê³¼ë¥¼ ë¡œë“œí•œ í›„
    BM25Plus ì¸ìŠ¤í„´ìŠ¤ë§Œ ì¬ìƒì„±í•©ë‹ˆë‹¤ (í† í°í™” ê³¼ì • ìƒëµ).
    """
    from rank_bm25 import BM25Plus

    from app.modules.core.retrieval.bm25_engine import BM25Index, KoreanTokenizer

    with open(path, "rb") as f:
        data = pickle.load(f)  # noqa: S301

    # í† í¬ë‚˜ì´ì €ëŠ” ê²€ìƒ‰ ì‹œ ì¿¼ë¦¬ í† í°í™”ì—ë§Œ ì‚¬ìš©
    tokenizer = KoreanTokenizer()
    index = BM25Index(tokenizer=tokenizer)

    # ì €ì¥ëœ ë°ì´í„°ë¡œ ë‚´ë¶€ ìƒíƒœ ë³µì› (ì¬í† í°í™” ì—†ì´)
    index._documents = data["documents"]
    index._tokenized_corpus = data["tokenized_corpus"]
    index._bm25 = BM25Plus(data["tokenized_corpus"])

    return index


async def main() -> None:
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ Docker-Free ë¡œì»¬ í€µìŠ¤íƒ€íŠ¸ - ë°ì´í„° ë¡œë“œ")
    print()

    # 1. ìƒ˜í”Œ ë°ì´í„° ë¡œë“œ
    if not SAMPLE_DATA_PATH.exists():
        print(f"âŒ ìƒ˜í”Œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {SAMPLE_DATA_PATH}")
        sys.exit(1)

    with open(SAMPLE_DATA_PATH, encoding="utf-8") as f:
        data = json.load(f)

    raw_docs = data.get("documents", [])
    print(f"ğŸ“„ {len(raw_docs)}ê°œ ë¬¸ì„œ ë¡œë“œ")

    # 2. ë¬¸ì„œ ì¤€ë¹„
    docs = prepare_documents(raw_docs)

    # 3. ë¡œì»¬ ì„ë² ë”© ìƒì„±
    print("ğŸ¤– ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    print("   (ì²« ì‹¤í–‰ ì‹œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œì— 1-2ë¶„ ì†Œìš”)")

    from app.modules.core.embedding.local_embedder import LocalEmbedder

    embedder = LocalEmbedder(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        output_dimensionality=1024,
        batch_size=32,
        normalize=True,
    )
    print("âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")

    texts = [doc["content"] for doc in docs]
    print("ğŸ”¢ ì„ë² ë”© ìƒì„± ì¤‘...")
    embeddings = embedder.embed_documents(texts)
    print(f"âœ… {len(embeddings)}ê°œ ì„ë² ë”© ìƒì„± ì™„ë£Œ (ì°¨ì›: {len(embeddings[0])})")

    # 4. ChromaDB ì ì¬
    print("ğŸ“¥ ChromaDBì— ë¬¸ì„œ ì ì¬ ì¤‘...")
    count = await load_to_chroma(docs, embeddings)
    print(f"âœ… {count}ê°œ ë¬¸ì„œ ChromaDB ì ì¬ ì™„ë£Œ ({CHROMA_PERSIST_DIR})")

    # 5. BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
    print("ğŸ” BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
    try:
        bm25_index = build_bm25_index(docs)
        save_bm25_index(bm25_index)
        print(f"âœ… BM25 ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ ({BM25_INDEX_PATH})")
    except ImportError:
        print("âš ï¸  BM25 ì˜ì¡´ì„± ë¯¸ì„¤ì¹˜ - Dense ê²€ìƒ‰ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤")
        print("   ì„¤ì¹˜: uv sync --extra bm25")

    print()
    print("ğŸ‰ ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")


if __name__ == "__main__":
    asyncio.run(main())
